%Imagine a visual search experiment in which a number of static images are shown at once, with one oddball image in a sea of distracter images. The goal is to find the location of the oddball image as quickly as possible, subject to an upper bound on the probability of error. Vaidhiyan et al. \cite{vaidhiyan2018learning, vaidhiyan2017learning} modeled the above visual search experiment as a problem of finding an anomalous arm in a multi-armed bandit. Similar experiments, but with static images replaced by dynamic drifting-dots images (movies), were conducted by neuroscientists to see how evidence is accumulated in slow perceptual decision making. In these experiments, the dots executed Brownian motions with a fixed drift at each location. Moreover, the drifts were identical in distracter locations and were different from the drift in the oddball location. A systematic analysis of this visual search, along the lines of that carried out by Vaidhiyan et al., requires an understanding of the so-called restless odd Markov arm problem that forms the main subject of this thesis.
%
%Formally, we 


%Consider a multi-armed bandit with $K\geq 3$ arms in which each arm is a time homogeneous and ergodic discrete time Markov process evolving on a common finite state space. Suppose that one of the arms is anomalous in that its transition probability matrix (TPM) is $P_1$ and that of each non-odd arm is $P_2\neq P_1$. The TPMs $P_1$ and $P_2$ may or may not be known to a learning agent whose goal it is to find the anomalous arm (or odd arm) as quickly as possible, subject to an upper bound on error probability. The learning agent samples the arms sequentially, one at a time, until it is sufficiently confident of its knowledge of the odd arm. While the learning agent observes only one arm at any given time, the unobserved arms may either remain frozen until their next sampling time instant (rested arms) or continue to evolve (restless arms). Prior works have analysed the above problem for the case when each arm yields independent and identically distributed observations. We provide the first known extensions to the case of Markov observations from each arm.
%
%In the first part of this thesis, we analyse the setting of rested arms as a first step towards analysing the more difficult setting of restless arms. Here, we assume that the TPMs $P_1$ and $P_2$ are not known beforehand. We present an asymptotic lower bound on the expected time to find the odd arm, where the asymptotics is as the error probability vanishes. Further, we devise a sequential arm selection policy based on the principle of certainty equivalence and demonstrate that our policy achieves an upper bound that matches with the lower bound asymptotically. A key component in our analysis of the lower and the upper bounds is the identification of the fact that for the Markov process of each arm, the long term fraction of entries into a state is equal to the long term fraction of exits from the state (global balance). Both these quantities are in turn equal to the probability of observing the state under the arm's stationary distribution. 
%
%In the second part of this thesis, we derive the results for the setting of restless arms assuming that the TPMs $P_1$ and $P_2$ are known beforehand. The restless nature of the arms makes it necessary for the learning agent to keep track of the time since each arm was last sampled (arm's delay) and the state of each arm when it was last sampled (arm's last observed state). We introduce the notion of trembling hand. Under this trembling hand model, we derive an asymptotic lower bound on the expected time to find the odd arm, devise a sequential arm selection policy, and show that our policy achieves an upper bound that matches with the lower bound asymptotically as the error probability vanishes. In our analysis, we show that the arm delays and the last observed states form a controlled Markov process (a Markov decision problem (MDP)) which is ergodic under any stationary arm selection policy. Our approach of considering the delays and the last observed states of all the arms jointly offers a global perspective of the arms and serves as a `lift' from the local perspective of dealing with the delay and the last observed state of each arm separately (as suggested by the prior works). 
%In the absence of the trembling hand, we argue that it is not clear if the resulting lower and the upper bounds on the expected time to find the odd arm match.

%In the third and final part of this thesis, we extend the results of the previous part to the case when the TPMs $P_1$ and $P_2$ are not known beforehand and must be learnt along the way. Here too, we provide an asymptotic lower bound on the expected time required to find the odd arm. Because the TPMs are not known beforehand, we estimate the TPMs using maximum likelihood (ML) estimation. The key challenge here is in showing that the ML estimates of the TPMs converge to their true values, i.e., the system is identifiable. We prove identifiability under a continuous selection assumption and a certain regularity assumption on the TPMs. We devise a sequential arm selection policy based on the principle of certainty equivalence and show that it achieves an upper bound that matches with the lower bound asymptotically. Our achievability analysis relies crucially on resolving the identifiability problem. 
%We argue that for simpler policies that estimate the TPMs by repeated sampling of each arm before switching to another arm, it is not clear if the lower bound can be achieved or if the desired error probability can be met.

 
In this thesis, we study the problem of identifying an anomalous arm in a multi-armed bandit as quickly as possible, subject to an upper bound on the error probability. Also known as odd arm identification, this problem falls within the class of optimal stopping problems in decision theory and can be embedded within the framework of active sequential hypothesis testing. Prior works on odd arm identification dealt with independent and identically distributed observations from each arm. We provide the first known extension to the case of Markov observations from each arm. Our analysis and results are in the asymptotic regime of vanishing error probability.

We associate with each arm an ergodic discrete time Markov process that evolves on a common, finite state space. The notion of anomaly is that the transition probability matrix (TPM) of the Markov process of one of the arms (the {\em odd arm}) is some $P_{1}$, and that of each non-odd arm is a different $P_{2}$, $P_{2}\neq P_{1}$. A learning agent whose goal it is to find the odd arm samples the arms sequentially, one at any given time, and observes the state of the Markov process of the sampled arm. The Markov processes of the unobserved arms may either remain frozen at their last observed states until their next sampling instant ({\em rested arms}) or continue to undergo state evolution ({\em restless arms}). The TPMs $P_{1}$ and $P_{2}$ may be known to the learning agent beforehand or unknown. We analyse the following cases: (a) rested arms with TPMs unknown, (b) restless arms with TPMs known, and (c) restless arms with TPMs unknown. For each of these cases, we first present an asymptotic lower bound on the {\color{black} growth rate of the }expected time required to find the odd arm, and subsequently devise a sequential arm selection policy which we show achieves the lower bound and is therefore asymptotically optimal. 

A key ingredient in our analysis of the setting of rested arms is the observation that for the Markov process of each arm, the long term fraction of entries into a state is equal to the long term fraction of exits from the state (global balance). When the arms are restless, it is necessary for the learning agent to keep track of the time since each arm was last sampled (arm's {\em delay}) and the state of each arm when it was last sampled (arm's {\em last observed state}). We show that the arm delays and the last observed states form a controlled Markov process which is ergodic under any stationary arm selection policy that picks each arm with a strictly positive probability. Our approach of considering the delays and the last observed states of all the arms jointly offers a global perspective of the arms and serves as a `lift' from the local perspective of dealing with the delay and the last observed state of each arm separately, one that is suggested by the prior works. Lastly, when the TPMs are unknown and have to be estimated along the way, it is important to ensure that the estimates converge almost surely to their true values asymptotically, i.e., the system is {\em identifiable}. We show identifiability follows from the ergodic theorem in the rested case, and provide sufficient conditions for it in the restless case.












