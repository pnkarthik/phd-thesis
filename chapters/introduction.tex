\chapter{Introduction}
\label{ch:introduction}

%Imagine a visual search experiment in which a number of static images are shown at once, with one oddball image in a sea of distracter images. The goal is to find the location of the oddball image as quickly as possible, subject to an upper bound on the probability of error. Vaidhiyan et al. \cite{vaidhiyan2017learning, vaidhiyan2017learning} modeled the above visual search experiment as a problem of finding an anomalous arm in a multi-armed bandit. Similar experiments, but with static images replaced by dynamic drifting-dots images (movies), were conducted by neuroscientists \cite{krueger2017evidence} to see how evidence is accumulated in slow perceptual decision making. In these experiments, the dots executed Brownian motions with a fixed drift at each location. Moreover, the drifts were identical in distracter locations and were different from the drift in the oddball location. A systematic analysis of this visual search, along the lines of that carried out by Vaidhiyan et al., requires an understanding of the so-called restless odd Markov arm problem that forms the main subject of this thesis.
%
%For the purposes of carrying out mathematical analysis, the visual search experiments described above may be cast as a problem of identifying an anomalous Markov process in a multi-armed bandit in the following way.  

Suppose that a decision entity wishes to select a subset of one or more alternatives among a fixed number of alternatives, each of which yields a random reward from an unknown distribution upon being selected. In the face of uncertainty, the decision entity must select the alternatives sequentially in a way so as to maximise its overall reward accumulated over a finite time horizon of $T$ time instants or an infinite time horizon. Such examples of decision making under uncertainty are popularly modeled using {\em multi-armed bandits} in which an arm is analogous to an altenative. Defining {\em regret} of an arms selection strategy (policy) as the difference between the total expected reward obtained under the policy and the total reward obtained under a policy which knows the reward distributions, the goal of the decision entity is, equivalently, to repeatedly sample the arms so as to minimise the finite or the infinite time horizon regret. Typically, the successive rewards from any given arm are assumed either to be independent and identically distributed (iid) or to have a first order dependence (Markov), and independent of the rewards of the other arms.


%Multi-armed bandits constitute a popular model for decision making under uncertainty. The classical setup of multi-armed bandits consists of a fixed number of alternatives (called {\em arms}) competing for a common resource that can sample a subset of arms at any given time. Each arm, upon being sampled, yields a random reward whose distribution is unknown to a decision entity. The goal of the decision entity is to sample the arms in such a way that the expected sum of rewards accumulated over a finite horizon of $T$ time instants  is maximised. 

While one line of works on multi-armed bandits is based on the theme of maximising rewards (or equivalently minimising regret) as described above, another line of works focuses on the theme of {\em optimal stopping} where the goal is to test for the validity of one or more hypotheses as quickly as possible and stop further sampling of the arms. For instance, the problem of best arm identification \cite{kaufmann2016complexity} in which the goal is to find the index of the arm that yields the largest mean reward as quickly as possible and stop, is an example of an optimal stopping problem in multi-armed bandits. Another instance of an optimal stopping problem in multi-armed bandits, one that constitutes the main topic of this thesis, is the problem of {\em odd arm identification} in which one of the arms in the multi-armed bandit is anomalous, and the goal is to find the index of the anomalous arm (or {\em odd arm}) as quickly as possible. It is interesting to note here that the arm sampling policies that are optimal in the context of reward maximisation (or regret minimisation) may not necessarily be so for optimal stopping problems; we refer the reader to \cite{Bubeck2011} for a discussion on this. 
%Additionally, when studying optimal stopping problems in multi-armed bandits, the notion of rewards may be discarded as the goal is not to maximise rewards. Instead, the state of a sampled arm may be treated as merely an observation from the arm. 

In this thesis, we study of the problem of odd arm identification in the setting when the successive observations from each arm form a Markov process. In this context, anomaly simply means that the transition probability matrix (TPM) of the odd arm is $P_1$, and that of each non-odd arm is $P_2$, where $P_2\neq P_1$. Throughout the thesis, we study the close interplay between the following important quantities that dictate the performance of any arm sampling policy: (a) the time to stoppage, i.e., the time to identify the index of the odd arm, and (b) the error probability at stoppage. For the purpose of carrying out analysis, it is customary to fix one of these quantities and study the behaviour of the other in terms of the quantity fixed. In this thesis, we fix the error probability and characterise the asymptotic behaviour of the time to stoppage as a function of the error probability, where the asymptotics is as the error probability vanishes. 

\section{Motivation}
Our motivation to study the problem of odd arm identification in the setting of Markov observations from the arms comes from the desire to extend, to more general settings, the decision theoretic formulation of a certain visual search experiment conducted by Sripati and Olson \cite{sripati2010global} and analysed in Vaidhiyan et al. \cite{Vaidhiyan2017,vaidhiyan2012active}. In this experiment, human subjects were shown a number of images at once, with one {\em oddball} image in a sea of {\em distracter} images. The goal of the experiment was to understand the relationship between (a) the average time taken by the human subject to identify the oddball image, and (b) the dissimilarity between the oddball and distracter images as perceived by the human subject. Vaidhiyan et al. modelled visual search for locating an oddball image in a sea of distracter images, as quickly as possible, as an odd arm identification problem with Poisson observations. The Poisson observations stemmed from the Poisson point process model for the neuron firings when the human subject focuses on a particular image, the analogue of pulling an arm. They showed that dissimilarity in neural responses to the oddball and the distracter images predicted the time taken by human subjects in detecting the location of the oddball image. The analysis was extended to the case when the parameters of the process were unknown, but had to be learnt during search, in \cite{vaidhiyan2017learning}. 


The oddball and distracter images in the experiments analysed in Vaidhiyan et al. \cite{Vaidhiyan2017, vaidhiyan2012active, vaidhiyan2017learning} and in Sripati and Olson \cite{sripati2010global} were static images. Similar experiments, but with dynamic drifting-dots images (movies), as in Krueger et al. \cite{krueger2017evidence}, were conducted by Vaidhiyan et al. to see how evidence is accumulated in slow perceptual decision making. In these experiments, the dots executed Brownian motions with a fixed drift at each location. Moreover, the drifts were identical in the distracter locations and were different from the drift in the oddball location. Subjects had to identify the oddball (anomalous) movie location as quickly as possible. A proper analysis of this visual search, along the lines of  \cite{Vaidhiyan2017}, \cite{vaidhiyan2012active} and \cite{vaidhiyan2017learning}, requires an understanding of the so-called \emph{restless} odd Markov arm problem where\footnote{The correspondence between the oddball movie experiment and the restless odd Markov arm problem is as follows: an arm corresponds to a movie; an observation from an arm corresponds to the positions of the dots in a movie frame; the Markov relationship between the successive observations from an arm corresponds to the dependence between the positions of the dots in the successive frames of a movie.} each arm yields Markov observations, one of the arms is anomalous, and the states of the unobserved arms continue to change (restless arms). Indeed, in the aforementioned drifting-dots experiment, the positions of the dots (state) will have changed when the subject returns to observe a particular location after a decision to look at another location. An analysis of the restless odd Markov arm problem forms the main subject of this thesis.

 
\section{Prior works on Odd Arm Identification}
The problem of odd arm identification has been studied in the literature for the case of independent and identically distributed (iid) observations from each arm. The works of Vaidhiyan et al. \cite{Vaidhiyan2017, vaidhiyan2012active, vaidhiyan2017learning} study the case of iid, indeed Poisson, observations from each arm. Prabhu et al. \cite{prabhu2017learning} extend the analysis of Vaidhiyan et al. to the case of iid observations belonging to a generic exponential family. The works  \cite{Vaidhiyan2017}, \cite{vaidhiyan2012active}, \cite{vaidhiyan2017learning} and \cite{prabhu2017learning}  can be embedded within the classical works of Chernoff \cite{Chernoff1959} and Albert \cite{albert1961sequential},  and provide a general framework for the analysis of lower bounds on expected number of samples required for identifying the index of the odd arm. In addition, they also provide explicit policies  that achieve these lower bounds in the asymptotic regime as error probability vanishes. We refer the reader to also \cite{kaufmann2016complexity,garivier2016optimal,hemo2016asymptotically,nitinawarat2017,naghshvar2010active,naghshvar2010information,naghshvar2011performance,naghshvar2013active} for other related works on iid observations. While the aforementioned works deal with iid arms, the novelty in this paper is that we consider Markov arms. To the best of our knowledge, we believe that our work is the first to consider Markov arms in the context of odd arm identification.

\section{Problem Setup and Objective}
Formally, our problem setup is as follows. We consider a multi-armed bandit with $K\geq 3$\footnote{This ensures that the notion of an ``odd'' arm makes sense.} arms. We associate with each arm a time homogeneous and ergodic discrete time Markov process evolving on a common, finite state space. We assume that the TPM of the odd arm is $P_{1}$ and that of each non-odd arm is $P_{2}\neq P_{1}$. Given an error probability threshold $\epsilon>0$, a learning agent wishes to identify the index of the odd arm as quickly as possible, subject to its error probability not exceeding $\epsilon$. The learning agent may or may not possess the knowledge of the TPMs of the arms, and samples the arms sequentially, one at any given time, until it is sufficiently confident of which arm is the odd arm. While the learning agent observes only one arm at any given time, the unobserved arms continue to evolve (restless arms). 

Suppose $\pi$ is an arm selection {\em policy} of the learning agent whose time to find the odd arm is $\tau(\pi)$. Given $\epsilon>0$, let $\Pi(\epsilon)$ denote the collection of all policies whose error probability is at most $\epsilon$. Let $C=(h, P_{1}, P_{2})$ denote an arms configuration in which $h$ is the index of the odd arm, $P_{1}$ is the TPM of arm $h$ and $P_{2}$ is the TPM of each non-odd arm $a\neq h$. Writing $E^{\pi}[\cdot|C]$ to denote the expectation computed under the policy $\pi$ and under the arms configuration $C$, an examination of the prior works reveals that $$ \inf\limits_{\pi\in \Pi(\epsilon)} E^{\pi}[\tau(\pi)|C] = O\left(\log \frac{1}{\epsilon}\right); $$ the constant multiplying $\log (1/\epsilon)$ in the above equation is, in general, a function of the arms configuration $C$. Our objective is to characterise
\begin{align}
\lim\limits_{\epsilon\downarrow 0}  \inf\limits_{\pi\in \Pi(\epsilon)} \frac{E^{\pi}[\tau(\pi)|C]}{\log (1/\epsilon)}.
\label{eq:intro_main_problem}
\end{align}
That is, our interest is to capture mathematically the asymptotic growth rate of the expected time required to find the odd arm subject to an upper bound on the error probability, where the asymptotics is as the error probability vanishes.

\section{Our Contributions}
Although our end goal is to study the restless odd Markov arm problem, the continued evolution of the arms in the restless setting presents many challenges in the analysis. Therefore, as a key first step towards an understanding of the restless setting, we analyse the simpler setting of {\em rested} arms in which the unobserved arms do not evolve and remain frozen at their previously observed states. For the settings of rested arms and restless arms, we provide answers to \eqref{eq:intro_main_problem} by first deriving a problem-instance dependent (arms configuration dependent) lower bound for \eqref{eq:intro_main_problem}. We show that the lower bound is of the form $1/\alpha(h, P_{1}, P_{2})$, where the constant $\alpha(h, P_{1}, P_{2})$ captures the hardness of the problem: the closer the TPMs $P_{1}$ and $P_{2}$ are (in an appropriately defined sense), the smaller the value of $\alpha$ is, and therefore the larger the lower bound, thereby implying longer times to find the odd arm. The exact form of $\alpha(h, P_{1}, P_{2})$ depends on whether or not the TPMs $P_{1}$ and $P_{2}$ are known to the learning agent beforehand. Complementing the lower bound results in each of the above cases (rested/restless, TPMs unknown/known), we devise a sequential arm selection policy and demonstrate that the expected time for the policy to find the odd arm satisfies an asymptotic upper bound that is equal to $1/\alpha(h, P_{1}, P_{2})$. Thus, our answer to \eqref{eq:intro_main_problem} is $1/\alpha(h, P_{1}, P_{2})$, where $\alpha(h,P_1,P_2)$ depends on whether the arms are rested/restless, whether the TPMs $P_1$ and $P_2$ are known or unknown, etc. From the symmetry of the problem, it can be deduced that $\alpha(h,P_1,P_2)$ does not depend on $h$, the index of the odd arm. 

\section{Organisation of the Thesis}
This thesis is organised as follows. In Chapter \ref{ch:rested_arms}, we analyse the setting of rested arms when the TPMs $P_1$ and $P_2$ are not known beforehand\footnote{The case of known TPMs in the setting of rested arms is a straighforward extension of the case of iid observations from the arms, and is therefore omitted.}. A key component in our analysis of the lower and the upper bounds for this case is the identification of the fact that for the Markov process of each arm, the long term fraction of entries into a state is equal to the long term fraction of exits from the state (global balance). Both these quantities are in turn equal to the probability of observing the state under the arm's stationary distribution.

In Chapter \ref{ch:restless_with_known_TPMs}, we derive the results for the setting of restless arms assuming that the TPMs $P_1$ and $P_2$ are known beforehand. The restless nature of the arms makes it necessary for the learning agent to keep track of the time since each arm was last sampled (arm's delay) and the state of each arm when it was last sampled (arm's last observed state). We introduce the notion of ``trembling hand'' commonly observed in visual search experiments, in which at any given time the learning agent intends to sample a certain arm but the actual arm sampled differs from the intended arm with a small probability. We show that the arm delays and the last observed states form a controlled Markov process which, under the trembling hand model, is ergodic under any stationary arm selection policy. Our approach of considering the delays and the last observed states of all the arms jointly offers a global perspective of the arms and serves as a `lift' from the local perspective of dealing with the delay and the last observed state of each arm separately (as done in the prior works). In the absence of the trembling hand, we discuss the difficulties associated with showing that the lower and the upper bounds match.

In Chapter \ref{ch:restless_with_unknown_TPMs}, we extend the results of Chapter \ref{ch:restless_with_known_TPMs} to the case when the TPMs $P_1$ and $P_2$ are not known beforehand and must be learnt along the way. Here, we estimate the TPMs using maximum likelihood (ML) estimation. The key challenge here is in showing that the ML estimates of the TPMs converge to their true values, i.e., the system is {\em identifiable}. We prove identifiability under a continuous selection assumption and a certain regularity assumption on the TPMs. Our achievability (upper bound) analysis relies crucially on resolving the identifiability problem. For a simpler policy that estimates the TPMs by repeated sampling of each arm before switching to another arm, we highlight the difficulties  in achieving the lower bound and meeting the desired error probability.

We conclude the thesis in Chapter \ref{ch:conclusions_and_future_work} and propose some future directions for exploration.


%\vspace{0.5cm}
%\textbf{A simplified model and our contributions} 
% . Broadly, we study the problem of odd arm identification in the following settings: 
%\begin{itemize}
%	\item Arms are rested and the TPMs $P_1$ and $P_2$ are not known beforehand.
%	\item Arms are restless and the TPMs $P_1$ and $P_2$ are known beforehand.
%	\item Arms are restless and the TPMs  $P_1$ and $P_2$ are not known beforehand.
%\end{itemize}
%For each of the above settings, we first present an asymptotic lower bound on the expected time to find the location of the odd arm, where the asymptotics is as the error probability vanishes. Subsequently, we show that in each of the above settings, there exists a policy which achieves the lower bound asymptotically. We provide an explicit description of the policy for each setting. We omit the analysis of the case when the arms are rested and the TPMs $P_1$ and $P_2$  are known beforehand as this is subsumed by the case of unknown $P_1$ and $P_2$. Our results  generalise the results of the prior works  \cite{vaidhiyan2017learning, Vaidhiyan2017, prabhu2017learning} to the case of Markov observations.
%
%In our analysis of the setting of rested arms, we bring out the key idea that any two successive selections of an arm result in observing a transition from the state corresponding to the arm's first selection to the state corresponding to the arm's second selection. As a consequence, for each state in the state space, the empirical proportion of times an arm occupies the state prior to a transition is equal, in the long run, to the empirical proportion of times the arm occupies the state after a transition. We then replace these common proportions by the probability of the arm occupying this state under its stationary distribution. Such a replacement by stationary probabilities is possible mainly due to the rested nature of the arms, and may not be possible in more general settings such as when the arms are restless. Identifying invariant quantities, such as the long-term fraction of exits and entries from a state in this case, lies at the heart of our analysis. When the arms are restless, 
%
%
%
%
%
%
%
%
%
