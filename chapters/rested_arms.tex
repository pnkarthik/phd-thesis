\chapter{Rested Arms}
\label{ch:rested_arms}
\section{Preamble}

In this chapter, we analyse the setting of rested arms with TPMs unknown. That is, given a multi-armed bandit with $K\geq 3$ arms and an arms configuration $C=(h, P_{1}, P_{2})$ in which $h$ is the index of the odd arm, the TPM of arm $h$ is $P_{1}$, and the TPM of each of the remaining arms is $P_{2}\neq P_{1}$, we wish to characterise $$ \lim\limits_{\epsilon\downarrow 0}  \inf\limits_{\pi\in \Pi(\epsilon)} \frac{E^{\pi}[\tau(\pi)|C]}{\log (1/\epsilon)} $$ for the setting of rested arms when $P_{1}$ and $P_{2}$ are not known beforehand. Our analysis of the setting of rested arms forms a key first step towards analysing the more difficult setting of restless arms. The rested case has its own interesting features. For example, as we shall see later in this chapter, the asymptotically optimal arm selection strategy does not explicitly depend on the last observed states of the arms. This, at first glance, is surprising.

\subsection{Prior Works on Rested Arms}
{One of the earliest works to consider the setting of rested arms is that of Gittins' \cite{gittins1979bandit} in which it is assumed that each arm yields a random `reward' when selected, and that the successive rewards from any given arm constitute a Markov process. In this reward setting, the central problem is one of maximising the infinite horizon average discounted reward. For this problem, Gittins proposed and demonstrated the optimality of a simple index-based policy that, at each time, involves constructing an index for every arm based on the knowledge of the transition laws of the arms and selecting an arm with the largest index. Agarwal et al. \cite{agrawal1989asymptotically} consider a similar setting as Gittins' in which each arm yields Markov rewards. However, unlike in  \cite{gittins1979bandit}, the authors of \cite{agrawal1989asymptotically} do not assume the knowledge of the transition laws of the arms. Instead, they assume that the transition law of each arm is parameterised by an unknown parameter belonging to a known, finite, parameter space. Define `regret' as the difference between the infinite time horizon expected sum of rewards generated by any policy and that generated by a policy which knows the parameters of the arms. The goal of the authors of \cite{agrawal1989asymptotically} is to design policies whose regret, in the asymptotic limit as time $n\to\infty$, is $o(n^\alpha)$ for every $\alpha>0$. For this problem, the authors of \cite{agrawal1989asymptotically} provide a lower bound in which the long-term regret grows asymptotically as $\log n$ times a multiplicative constant that captures the hardness of the problem. Furthermore, they propose a policy and demonstrate that it achieves the lower bound in the limit as $n\to \infty$.}

{While the aforementioned works deal with reward maximisation, and the associated regret minimisation in the unknown parameters setting, our problem is one of {\em optimal stopping}. We note that the lower bound in \cite{agrawal1989asymptotically}, although quantifying the asymptotic growth rate of regret, does not reflect the quickness of learning the underlying parameters of the arms. That is, the results in \cite{agrawal1989asymptotically} do not shed light on the minimum number of arm selections that are needed, on the average, in order to learn the parameters of the arms up to a desired level of accuracy. We answer this question when one of the arms is anomalous and the asymptotics is as the error probability vanishes. In doing so, we treat the state of any selected arm as merely a Markov observation from the arm and not as a reward, since our objective is one of optimal stopping and not of regret minimisation. We note here that policies which are optimal in the context of the problem studied here may not necessarily be optimal in the context of regret minimisation, and vice-versa; see Bubeck et al. \cite{Bubeck2011} for a discussion on this. Finally, the unknown parameters of our problem are the transition laws of the odd arm and the non-odd arm Markov processes, and the index of the odd arm, thus making our parameter set a continuum, unlike the finite parameter set considered in \cite{agrawal1989asymptotically}.}

Finally, a recent and independent work of Moulos \cite{moulos2019optimal} studies a closely related problem of \emph{best arm identification} in rested multi-armed bandits, where the goal is to find the arm with the largest stationary mean. The results presented in \cite{moulos2019optimal} are in terms of an asymptotic and a non-asymptotic lower bound, where the asymptotics is as the error probability vanishes, and a policy for best arm identification whose asymptotic upper bound is four times larger than the asymptotic lower bound. In this chapter, we present the first known asymptotic lower bound for the problem of odd arm identification, and an asymptotically optimal policy that meets the lower bound. This is in contrast with the gap between the upper bound and the lower bound in \cite{moulos2019optimal} for best arm identification. We anticipate that a policy similar to ours should close the gap between the upper and lower bounds in \cite{moulos2019optimal}.

\subsection{An Overview of Our Contributions}
Below, we highlight our contributions and bring out the challenges that we need to overcome in the analysis of the setting of rested arms.
\begin{enumerate}
    \item \textcolor{black}{In Section \ref{rested_arms_sec:lower_bound}}, we derive an asymptotic lower bound on {\color{black} the growth rate of} the expected number of arm selections required by any policy that the learning agent may use to identify the index of the odd arm. Here, the asymptotics is as the error probability vanishes. Similar to the lower bounds appearing in \cite{prabhu2017learning} and \cite{agrawal1989asymptotically}, our lower bound has a problem-instance (or arms configuration) dependent constant that quantifies the effort required by any policy to identify the true index of the odd arm by guarding itself against identifying the nearest, incorrect alternative. \textcolor{black}{This constant is a function of the transition probability matrices of the odd arm and the non-odd arms.}
    \item We characterise the growth rate of the expected number of arm selections required by any policy as a function of the maximum acceptable error probability, and show that in the regime of vanishingly small error probabilitys, this growth rate is logarithmic in the inverse of the error probability. The analysis of the lower bounds in \cite{vaidhiyan2017learning} and \cite{prabhu2017learning} uses the familiar data processing inequality presented in the work of Kaufmann et al. \cite{kaufmann2016complexity} that is based on Wald's identity \cite{wald1944cumulative} for iid processes. However, the Markov setting in our problem does not permit the use of Wald's identity. Therefore, we derive results for our Markov setting generalising those appearing in \cite{kaufmann2016complexity}, and subsequently use these generalisations to arrive at the lower bound. See Section \ref{rested_arms_sec:lower_bound} for the details.
    \item \textcolor{black}{In the analysis of the lower bound, we bring out the key idea that any two successive selections of an arm result in the learning agent observing a transition from the state corresponding to the arm's first selection to the state corresponding to the arm's second selection. As a consequence, for each state in the state space, the empirical proportion of times an arm occupies the state prior to a transition is equal, in the long run, to the empirical proportion of times the arm occupies the state after a transition. We then replace these common proportions by the probability of the arm occupying this state under its stationary distribution. Such a replacement by stationary probabilities is possible mainly due to the rested nature of the arms, and may not be possible in more general settings such as when the arms are restless.}
    \item \textcolor{black}{In Section \ref{rested_arms_sec:achievability}}, we propose a sequential arm selection scheme that takes as inputs two parameters, one of which may be chosen appropriately to meet the acceptable error probability, while the other may be tuned to ensure that the performance of our scheme comes arbitrarily close to the lower bound, thus making our scheme near-optimal.


    We now contrast the near-optimality of our scheme with the near-optimality of the scheme proposed by Vaidhiyan et al. in \cite{vaidhiyan2017learning}, and highlight a key simplification that our scheme entails. The scheme of Vaidhiyan et al. is built around the important fact that each arm is sampled at a non-trivial, strictly positive and optimal rate that is bounded away from zero, as given by the lower bound, thereby allowing for exploration of the arms in an optimal manner. This stemmed from their specific Poisson observations. However, the lower bound presented in Section \ref{rested_arms_sec:lower_bound} may not have this property
    %    does not allow for us to establish a similar fact
in the context of Markov observations.
    %see \cite[Assumption 4]{prabhu2017learning}. However, the authors of \cite{prabhu2017learning} establish the validity of their assumption only for some of the commonly used distributions belonging to the exponential family, thereby leaving open the validity of the assumption in full generality.
    Therefore, recognising the requirement of sampling the arms at a non-trivial rate for good performance of our scheme, in this chapter, we use the idea of ``forced exploration'' proposed by Albert in \cite{albert1961sequential}. In particular, we propose a simplified way of sampling the arms by considering a mixture of uniform sampling and the optimal sampling given by the lower bound in Section \ref{rested_arms_sec:lower_bound}. We do this by introducing an appropriately tuneable parameter that controls the probability of switching between uniform sampling and optimal sampling, the latter being given by the lower bound. While this ensures that our policy samples each arm with a strictly positive probability at each step, it also gives us the flexibility to select an appropriate value for this parameter so that the upper bound on the performance of our scheme may be made arbitrarily close to our lower bound. We refer the reader to Section \ref{rested_arms_sec:achievability} for the details.
\end{enumerate}

\subsection{Chapter Organisation}
The rest of this chapter is organised as follows. In Section \ref{rested_arms_sec:notations}, we set up the notations that will be used throughout the chapter. In Section \ref{rested_arms_sec:lower_bound}, we present a lower bound on the performance of any policy. In Section \ref{rested_arms_sec:achievability}, we present a sequential arm selection policy and demonstrate its near optimality. We present the main result of this chapter in Section \ref{rested_arms_sec:main_result}, combining the results of Sections \ref{rested_arms_sec:lower_bound} and \ref{rested_arms_sec:achievability}. In Section \ref{rested_arms_sec:simulation_results}, we provide some simulation to support the theoretical development. We present the proofs of all the results in Section \ref{rested_arms_sec:proofs_of_main_results}, and summarise the key points of this chapter in Section \ref{rested_arms_sec:conclusions}.

\section{Notations}
\label{rested_arms_sec:notations}
In this section, we set up the notations that will be used throughout the chapter. Let $K\geq 3$ denote the number of arms, and let $\mathcal{A}=\{1,2,\ldots,K\}$ denote the set of arms. We associate with each arm an irreducible, aperiodic, time homogeneous discrete-time Markov process on a finite state space $\mathcal{S}$, where the Markov process of each arm is independent of the Markov processes of the other arms. We denote by $|\mathcal{S}|$ the cardinality of $\mathcal{S}$. Without loss of generality, we take $\mathcal{S}=\{1,2,\ldots,|\mathcal{S}|\}$. Hereinafter, we use the phrase `Markov process of arm $a$' to refer to the Markov process associated with arm $a\in \mathcal{A}$.

At each discrete time instant, one out of the $K$ arms is selected and its state is observed. We let $A_n$ denote the arm selected at time $n$, and let $\bar{X}_n$ denote the state of arm $A_n$, where $n\in\{0,1,2,\ldots\}$. We treat $A_0$ as the zeroth arm selection and $\bar{X}_0$ as the zeroth observation. Selection of an arm at time $n$ is based on the history $(\bar{X}^{n-1},A^{n-1})$ of past observations and arms selected; here, $\bar{X}^k$ (resp. $A^k$) is a shorthand notation for the sequence $\bar{X}_0,\ldots,\bar{X}_k$ (resp. $A_0,\ldots,A_k$). We shall refer to such a sequence of arm selections and observations as a policy, which we generically denote by $\pi$. For each $a\in\mathcal{A}$, we denote the Markov process of arm $a$ by the collection $(X_k^a)_{k\geq 0}$ of random variables. Further, we denote by $N_a(n)$ the number of times arm $a$ is selected by a policy up to (and including) time $n$, i.e.,
\begin{equation}
N_a(n)=\sum\limits_{t=0}^{n}1_{\{A_t=a\}}.\label{rested_arms_eq:N_a(n)}	
\end{equation}
 Then, for each $n\geq 0$, we have the observation
\begin{equation}
	\bar{X}_n=X_{N_{A_n}(n)-1}^{A_n}.\label{rested_arms_eq:obs_at_time_n}
\end{equation}

We consider a scenario in which the Markov process of one of the arms (hereinafter referred to as the odd arm) follows a probability transition matrix $P_1=(P_1(j|i))_{i,j\in\mathcal{S}}$, while those of rest of the arms follow a probability transition matrix $P_2=(P_2(j|i))_{i,j\in\mathcal{S}}$, where $P_2\neq P_1$; here, $P(j|i)$ denotes the entry in the $i${th} row and $j$th column of the matrix $P$. Further, we let $\mu_1$ and $\mu_2$ denote the unique stationary distributions of $P_1$ and $P_2$ respectively. We denote by $\nu$ the common distribution for the initial state of each Markov process. In other words, for arm $a\in \mathcal{A}$, we have $X_0^a\sim \nu$, and this is the same distribution for all arms. We operate in a setting where the probability transition matrices and their associated stationary distributions are unknown to the learning agent.

For each $a\in\mathcal{A}$ and state $i\in \mathcal{S}$, we denote by $N_a(n,i)$ the number of times up to (and including) time $n$ the Markov process of arm $a$ is observed to \textcolor{black}{occupy state $i$ prior to a transition}, i.e.,
\begin{equation}
	N_a(n,i)=\sum\limits_{m=1}^{N_a(n)-1} 1_{\{X_{m-1}^a=i\}}.\label{rested_arms_eq:N_a(n,i)}
\end{equation}
Similarly, for each $i,j\in \mathcal{S}$, we denote by $N_a(n,i,j)$ the number of times up to (and including) time $n$ the Markov process of arm $a$ is observed to \textcolor{black}{make a transition from state $i$ to state $j$}, i.e.,
\begin{equation}
	N_a(n,i,j)=\sum\limits_{m=1}^{N_a(n)-1} 1_{\{X_{m-1}^a=i,\,X_m^a=j\}}.\label{rested_arms_eq:N_a(n,i,j)}
\end{equation}
Clearly, then, the following hold:
\begin{subequations}
\begin{enumerate}
	\item For each $a\in \mathcal{A}$ and $i\in\mathcal{S}$,
	\begin{equation}
		\sum\limits_{j\in\mathcal{S}}N_a(n,i,j)=N_a(n,i).\label{rested_arms_eq:sum_N_a(n,i,j)}
	\end{equation}
	\item For each $a \in \mathcal{A}$,
	\begin{equation}
		\sum\limits_{i\in\mathcal{S}}N_a(n,i)=N_a(n)-1.\label{rested_arms_eq:N_a_summed_over_i}
	\end{equation}
	\item For each $n$,
	\begin{equation}
		\sum\limits_{a\in\mathcal{A}}N_a(n)=n+1.\label{rested_arms_eq:sum_N_a}
	\end{equation}
\end{enumerate}
\end{subequations}
We note here that the upper index of the summation in \eqref{rested_arms_eq:N_a(n,i)} is $N_a(n)-1$, and not $N_a(n)$, since the last observed transition on arm $a$ would be from the state $X_{N_a(n)-2}^a$ to the state $X_{N_a(n)-1}^a$. This is further reflected by the summation in \eqref{rested_arms_eq:N_a_summed_over_i}.

Fix probability transition matrices $P_1$ and $P_2$, where $P_2\neq P_1$, and let $H_{h}$ denote the hypothesis that $h$ is the index of the odd arm. The probability transition matrix of arm $h$ is $P_1$; all other arms have $P_2$. We refer to the triplet $C=(h,P_1,P_2)$ as a configuration. Our problem is one of detecting the true hypothesis among all possible configurations given by $$\mathcal{C}=\{C=(h,P_1,P_2):h\in\mathcal{A},\,\textcolor{black}{P_1\text{ and }P_2\text{ are transition probability matrices on }\mathcal{S}},\, \,P_2\neq P_1\}$$
when $P_1$ and $P_2$ are unknown. Let $C=(h,P_1,P_2)$ denote the underlying configuration of the arms. For each $a\in\mathcal{A}$, we denote by $(Z_h^a(n))_{n\geq 0}$ the log-likelihood process of arm $a$ under configuration $C$, with $h$ being the true index of the odd arm. Using the notations introduced above, we may then express $Z_h^a(n)$ as
\begin{equation}
	Z_h^a(n)=
\begin{cases}
	0,& N_a(n)=0,\\
	\log \nu(X_0^a),& N_a(n)=1,\\
	\log\nu(X_0^a)+\sum\limits_{m=1}^{N_a(n)-1}\log P_h^a(X_m^a|X_{m-1}^a),& N_a(n)\geq 2,
\end{cases}
\label{rested_arms_eq:Z_h^a(n)}
\end{equation}
where $P_h^a(j|i)$ denotes the conditional probability under hypothesis $H_h$ of observing state $j$ on arm $a$ given that state $i$ was observed on arm $a$ at the previous sampling instant, and is given by
\begin{equation}
	P_h^a(j|i)=\begin{cases}
		P_1(j|i),&a=h,\\
		P_2(j|i),&a\neq h.
	\end{cases}
	\label{rested_arms_eq:P_h^a(j|i)}
\end{equation}
Then, since the Markov processes of all the arms are independent of one another, for a given sequence $(A^n,\bar{X}^n)$ of arm selections and observations under a policy $\pi$ and a configuration $C=(h,P_1,P_2)$, denoting by $(Z_h(n))_{n\geq 0}$ the log-likelihood process under hypothesis $H_h$ of all arm selections and observations up to time $n$, we have
\begin{equation}
Z_h(n)=\sum\limits_{a=1}^{K}Z_h^a(n),\label{rested_arms_eq:log_likelihood_under_hyp_h}
\end{equation}
where $Z_h^a(n)$ is as given in \eqref{rested_arms_eq:Z_h^a(n)}.
%where $\nu_h^a(i)$ denotes the probability of observing arm $a$ in state $i\in\mathcal{S}$ under hypothesis $H_h$, and is given by
%\begin{equation}
%	\nu_h^a(i)=\begin{cases}
%		\nu_1(i),&a=h,\\
%		\nu_2(i),&a\neq h,
%	\end{cases}
%	\label{rested_arms_eq:init_dist_of_arm_a_under_hyp_h}
%\end{equation}
On similar lines, for any two configurations $C=(h,P_1,P_2)$ and $C'=(h',P_1',P_2')$, where $P_2'\neq P_1'$ and $h'\neq h$, for each $a\in\mathcal{A}$, we define the log-likelihood process $(Z_{hh'}^a(n))_{n\geq 0}$ of configuration $C$ with respect to configuration $C'$ for arm $a$ as
\begingroup\allowdisplaybreaks\begin{align}
	Z_{hh'}^a(n)&=Z_h^a(n)-Z_{h'}^a(n)\nonumber\\
	&=\begin{cases}0,& N_a(n)=0,1,\\
	\sum\limits_{m=1}^{N_a(n)-1}\log \dfrac{P_{h}^{a}(X_m^a|X_{m-1}^a)}{P_{h'}^{a}(X_m^a|X_{m-1}^a)},& N_a(n)\geq 2.\label{rested_arms_eq:Z_{hh'}^a(n)}
\end{cases}
\end{align}\endgroup
We note that in the above equation, for $P_h^a$, we should use \eqref{rested_arms_eq:P_h^a(j|i)}, and for $P_{h'}^a$, we shall use, for all $a\in\mathcal{A}$ and $i,j\in\mathcal{S}$,
\begin{equation}
	P_{h'}^a(j|i)=\begin{cases}
		P_1'(j|i),&a=h',\\
		P_2'(j|i),&a\neq h'.
	\end{cases}
	\label{rested_arms_eq:P_{h'}^a(j|i)}
\end{equation}
Finally, we denote by $(Z_{hh'}(n))_{n\geq 0}$ the log-likelihood process of configuration $C$ with respect to $C'$ as
\begin{equation}
	Z_{hh'}(n)=\sum\limits_{a=1}^{K}Z_{hh'}^a(n),\label{rested_arms_eq:log_likelihood_under_hyp_h_and_h'}
\end{equation}
which includes all arm selections and observations.

The observation process $(\bar{X}_n)_{n\geq 0}$ and the arm selection process $(A_n)_{n\geq 0}$ are assumed to be defined on a common probability space $(\Omega,\mathcal{F},P)$. We define the filtration $(\mathcal{F}_n)_{n\geq 0}$ as
\begin{equation}
%	\mathcal{F}_n=\begin{cases}
%		\{\phi,\Omega\},&n=0,\\
%		\sigma(A^{n-1},Z^{n-1}),&n\geq 1.
%	\end{cases}
	\mathcal{F}_n = \sigma(A^n,\bar{X}^n),\quad n\geq 0.
	\label{rested_arms_eq:filtration}
\end{equation}
We use the convention that the zeroth arm selection $A_0$ is measurable with respect to the sigma algebra $\{\phi,\Omega\}$, whereas for all $n\geq 1$, the $n$th arm selection $A_n$ is $\mathcal{F}_{n-1}$-measurable.
For any stopping time $\tau$ with respect to the filtration in \eqref{rested_arms_eq:filtration}, we denote by $\mathcal{F}_{\tau}$ the $\sigma$-algebra
\begin{equation}
	\mathcal{F}_{\tau}=\{E\in\mathcal{F}:E\cap \{\tau=n\}\in\mathcal{F}_{n}\text{ for all }n\geq 0\}.\label{rested_arms_eq:stopping_time_sigma_alg}
\end{equation}

Our focus will be on policies $\pi$ that identify the index of the odd arm by sequentially sampling the arms, one at every time instant, and learning from the arms selected and observations obtained in the past. Specifically, at any given time, a policy $\pi$ prescribes one of the following alternatives:
\begin{enumerate}
\item Select an arm, based on the history of past observations and arms selected, according to a fixed distribution $\lambda$ independent of the underlying configuration of the arms, i.e., for each $n\geq 1$,
\begin{equation}
P(A_{n}=a|A^{n-1},\bar{X}^{n-1})=\lambda(a).	\label{rested_arms_eq:lambda(a)}
\end{equation}
\item Stop selecting arms, and declare the index $I(\pi)$ as the odd arm.	
\end{enumerate}
 Given a maximum acceptable error probability $\epsilon>0$, we denote by $\Pi(\epsilon)$ the family of all policies whose probability of error at stoppage for any underlying configuration of the arms is at most $\epsilon$. That is,
\begingroup\allowdisplaybreaks\begin{align}
	\Pi(\epsilon)=\bigg\lbrace\pi:P^\pi(I(\pi)\neq h|C)\leq \epsilon~\forall~ C=(h,P_1,P_2),\text{ where }h\in\mathcal{A}\text{ and }P_2\neq P_1\bigg\rbrace.\label{rested_arms_eq:Pi(epsilon)}
\end{align}\endgroup
%We write $C=(h,P_1,P_2)$ to denote a configuration triplet in which $h\in\mathcal{A}$ denotes the index of the odd arm, $P_1$ denotes the probability transition matrix of the Markov process of the odd arm, and $P_2$ denotes the probability transition matrix of each of the non-odd arm Markov processes.

For a policy $\pi$, we denote its stopping time by $\tau(\pi)$.
%For the remainder of this section, we fix $C=(h,P_1,P_2)$ to the underlying configuration, and let $C'=(h',P_1',P_2')$ denote any alternative configuration, where $h'\neq h$.
 Further, we write $E^\pi[\cdot|C]$ and $P^\pi(\cdot|C)$ to denote expectations and probabilities given that the underlying configuration of the arms is $C$. In this chapter, we characterise the behaviour of $E^\pi[\tau(\pi)|C]$ for any policy $\pi\in\Pi(\epsilon)$, as $\epsilon$ approaches zero. We re-emphasise that $\pi$ cannot depend on the knowledge of $P_1$ or $P_2$, but could attempt to learn these along the way.

 \begin{remark}
%	It is important to note that for any fixed transition probability matrices $P_1$, $P_2$, with $P_1\neq P_2$, the set of policies whose probability of error at stoppage is within $\epsilon$ may be larger than the set $\Pi(\epsilon)$. However, our interest is in policies that work for all $P_1\neq P_2$ since our setting is one in which the probability transition matrices are not known to the learner. However, it is not clear a priori that $\Pi(\epsilon)$ is nonempty.
%\textcolor{red}{For a fixed configuration $C=(h,P_1,P_2)$, with $P_1\neq P_2$, let $\Pi(\epsilon|C)$ denote the set  of all policies whose probability of error at stoppage is within $\epsilon$ when the underlying configuration of the arms is $C$. We note that this set may be larger for certain specific instances of $P_1$ and $P_2$  than for others. Since our setting is one in which the underlying configuration of the arms is not known to the learner, we wish to construct policies that work for any underlying configuration $C$. Such policies are precisely those given in \eqref{rested_arms_eq:Pi(epsilon)}. That is,}
%\begin{equation}
%\textcolor{red}{\Pi(\epsilon)=\bigcap\limits_{C}\Pi(\epsilon|C).}\label{rested_arms_eq:Pi(epsilon)_as_intersection}
%\end{equation}
%\textcolor{red}{However, it is not a priori clear whether the set in \eqref{rested_arms_eq:Pi(epsilon)_as_intersection} is nonempty. It is nonempty for the case of iid rewards from arms, as was established in \cite{vaidhiyan2017learning} and \cite{prabhu2017learning}. In this chapter, we show that the above set is nonempty even for the setting of rested and Markov arms.}
Fix an odd arm index $h$, and consider the simpler case when $P_1$, $P_2$ are known, $P_2\neq P_1$. Let $\Pi(\epsilon|P_1,P_2)$ denote the set of all policies whose probability of error at stoppage is within $\epsilon$. From the definition of $\Pi(\epsilon)$ in \eqref{rested_arms_eq:Pi(epsilon)}, it follows that
\begin{equation}
\Pi(\epsilon)=\bigcap\limits_{P_1,P_2:P_2\neq P_1} \Pi(\epsilon|P_1,P_2). \label{rested_arms_eq:Pi(epsilon)_as_intersection}
\end{equation}
That is, policies in $\Pi(\epsilon)$ work for any $P_1, P_2$, with $P_2\neq P_1$. It is not a priori clear whether the set $\Pi(\epsilon)$ is nonempty. That it is nonempty for the case of iid observations was established in \cite{Chernoff1959}. In this chapter, we show that $\Pi(\epsilon)$ is nonempty even for the setting of rested and Markov arms.
\end{remark}

\begin{remark}
	The distribution $\lambda$ appearing in \eqref{rested_arms_eq:lambda(a)} may, in general, be a function of time index $n$.
\end{remark}

In the next section, we provide a configuration dependent lower bound on $E^\pi[\tau(\pi)|C]$ for any policy $\pi\in\Pi(\epsilon)$. In Section \ref{rested_arms_sec:achievability}, we propose a sequential arm selection policy that achieves the lower bound asymptotically as the probability of error vanishes. We present the proofs in Section \ref{rested_arms_sec:proofs_of_main_results}.









\section{Converse: Lower Bound}\label{rested_arms_sec:lower_bound}
For any two transition probability matrices $P$ and $Q$ of dimension $|\mathcal{S}|\times |\mathcal{S}|$, and a probability distribution $\mu$ on $\mathcal{S}$, define $D(P\|Q|\mu)$ as
\begin{equation}
 	D(P\|Q|\mu)\coloneqq\sum\limits_{i\in\mathcal{S}}\mu(i)\sum\limits_{j\in\mathcal{S}}P(j|i)\log\frac{P(j|i)}{Q(j|i)},\label{rested_arms_eq:D(P_1||P|mu_1}
 \end{equation}
 with the convention $0\log 0=0\log\frac{0}{0}=0$. \textcolor{black}{The quantity in \eqref{rested_arms_eq:D(P_1||P|mu_1} is known as \emph{conditional informational divergence}, and the notation used above for representing the same is standard in the literature. See, for instance, Csisz\'{a}r and K\"{o}rner \cite[Eq. (2.4)]{csiszar2011information}.} The following proposition gives an asymptotic lower bound on {\color{black} the growth rate of} the expected stopping time of any policy $\pi\in\Pi(\epsilon)$ as $\epsilon\downarrow 0$.
\begin{prop}\label{rested_arms_prop:lower_bound}
 	Under the arms configuration $C=(h,P_1,P_2)$,
 	\begin{equation}
 		\lim\limits_{\epsilon\downarrow 0}\inf\limits_{\pi\in\Pi(\epsilon)}\frac{E^\pi[\tau(\pi)|C]}{\log ({1}/{\epsilon})}\geq \frac{1}{D^*(h,P_1,P_2)},\label{rested_arms_eq:lower_bound}
 	\end{equation}
 	where $D^*(h,P_1,P_2)$ is an arms configuration-dependent constant given by
 	\begingroup\allowdisplaybreaks\begin{align}
 		&D^*(h,P_1,P_2)=\max\limits_{0\leq\lambda_1\leq 1}\left\lbrace\lambda_1\,D(P_1\|P|\mu_1)+(1-\lambda_1)\frac{(K-2)}{(K-1)}D(P_2\|P|\mu_2)\right\rbrace.\label{rested_arms_eq:D^*(h,P_1,P_2)_simpl_final}
 \end{align}\endgroup
 In \eqref{rested_arms_eq:D^*(h,P_1,P_2)_simpl_final}, $P$ is a probability transition matrix whose $(i, j)$th entry is given by
 \begingroup\allowdisplaybreaks\begin{align}
 	P(j|i)=\frac{\lambda_1\mu_1(i)P_1(j|i)+(1-\lambda_1)\frac{(K-2)}{(K-1)}\mu_2(i)P_2(j|i)}{\lambda_1\mu_1(i)+(1-\lambda_1)\frac{(K-2)}{(K-1)}\mu_2(i)}.\label{rested_arms_eq:P(j|i)}
 \end{align}\endgroup
\end{prop}
The proof of Proposition \ref{rested_arms_prop:lower_bound} broadly follows the outline of the proof of the lower bound in \cite{kaufmann2016complexity}, with necessary modifications for the setting of Markov observations. Below, we outline some of the key steps in the proof. For an arbitrary choice of error probability $\epsilon>0$, we first show that for any policy $\pi\in\Pi(\epsilon)$,  the expected value of the total sum of log-likelihoods up to the stopping time $\tau(\pi)$ can be lower bounded by the binary relative entropy function
\begin{equation}
	d(\epsilon,1-\epsilon)\coloneqq\epsilon\log\frac{\epsilon}{1-\epsilon}+(1-\epsilon)\log\frac{1-\epsilon}{\epsilon}.\label{rested_arms_eq:d(epsilon,1-epsilon)}
\end{equation}

Next, we express the expected sum of log-likelihoods up to the stopping time $\tau(\pi)$ in terms of the expected value of the stopping time. It is in obtaining such an expression that works such as \cite{kaufmann2016complexity}, \cite{vaidhiyan2017learning} and \cite{prabhu2017learning} that are based on iid observations use Wald's identity, which greatly simplifies their analysis of the lower bound. The Markov observations of our setting does not permit the use of Wald's identity. Therefore, we first obtain a generalisation of \cite[Lemma 18]{kaufmann2016complexity}, a change of measure based argument, for the setting of Markov observations, and subsequently use this generalisation to obtain the desired relation.

%\textcolor{red}{We then show that under the setting of rested arms (i.e., when unobserved arms do not undergo state transitions, but remain frozen at their previously observed states), for any $a\in\mathcal{A}$, the empirical frequency of observing arm $a$ exit out of state $i\in\mathcal{S}$ equals, in the long run, the empirical frequency of observing arm $a$ enter into state $i$. We then replace this latter frequency by the probability of observing arm $a$ in state $i$ under its stationary distribution. This leads to the terms $\mu_1$ and $\mu_2$, which are the unique stationary distributions of the odd arm and the non-odd arms respectively, appear in the expression \eqref{rested_arms_eq:D^*(h,P_1,P_2)_simpl_final}. We emphasise that this step in the analysis is possible due to the rested nature of the arms, and is not true in more general settings such as when the unobserved arms continue to evolve.}
We then show that for any arm $a\in\mathcal{A}$, the long run frequency of observing the arm \textcolor{black}{occupying state $i\in \mathcal{S}$ prior to a transition} is equal to that of arm $a$ \textcolor{black}{occupying the state $i$ after a transition}, and note that this common frequency is the stationary probability of observing the arm in the state $i$. This explains the appearance of the unique stationary distributions $\mu_1$ and $\mu_2$ of the odd arm and the non-odd arms respectively in the expression \eqref{rested_arms_eq:D^*(h,P_1,P_2)_simpl_final}. We wish to emphasise that this step in the proof is possible due to the rested nature of the arms.

{Finally, combining the above steps and using $d(\epsilon,1-\epsilon)/\log \frac{1}{\epsilon} \to 1$ as $\epsilon\downarrow 0$, we arrive at the lower bound in \eqref{rested_arms_eq:lower_bound}. The details may be found in Section \ref{rested_arms_appndx:proof_of_lower_bound}.}

\begin{remark}
The right-hand side of \eqref{rested_arms_eq:D^*(h,P_1,P_2)_simpl_final} is a function only of the transition probability matrices $P_1$ and $P_2$, and does not depend on $h$, the index of the odd arm. This is due to symmetry in the structure of arms, and we deduce that $D^*(h,P_1,P_2)$ does not depend on $h$. However, we include $h$ for the sake of consistency with the notation $C=(h,P_1,P_2)$ used to denote arm configurations. Further, it reminds us that $D^*$ may depend on all the parameters of the underlying configuration in more general composite hypothesis testing settings.
\end{remark}

 Going further, we let $\lambda^*\in[0,1]$ denote the value of $\lambda$ that achieves the maximum in \eqref{rested_arms_eq:D^*(h,P_1,P_2)_simpl_final}. We then define $\lambda_{opt}(h,P_1,P_2)=(\lambda_{opt}(h,P_1,P_2)(a))_{a\in\mathcal{A}}$ as the probability distribution on $\mathcal{A}$ given by
 \begin{equation}
 	\lambda_{opt}(h,P_1,P_2)(a)\coloneqq\begin{cases}
 		\lambda^*,&a=h,\\
 		\frac{1-\lambda^*}{K-1},&a\neq h.
 	\end{cases}\label{rested_arms_eq:lambda^*(h,P_1,P_2)}
 \end{equation}

 In the next section, we construct a policy that, at each time step, chooses arms with probabilities that match with those in \eqref{rested_arms_eq:lambda^*(h,P_1,P_2)} in the long run, in an attempt to reach the lower bound. While it is not a priori clear that this yields an asymptotically optimal policy, we show that this is indeed the case.

% \textcolor{red}{In Section \ref{rested_arms_sec:proofs_of_main_results}, we show that \eqref{rested_arms_eq:lambda^*(h,P_1,P_2)} represents the probabilities with which an optimal policy selects arms at each time step. In the next section, we construct a policy that, at each time step, chooses arms with probabilities that match with those in \eqref{rested_arms_eq:lambda^*(h,P_1,P_2)} in the long run, thereby making the policy asymptotically optimal.}


% In the next section, we propose a sequential arm selection policy and demonstrate that it achieves the lower bound in \eqref{rested_arms_eq:lower_bound} asymptotically as the probability of error vanishes. Further, we show the asymptotic arm selection frequencies of our proposed policy can be made arbitrarily close to the optimal arm selection frequencies given by \eqref{rested_arms_eq:lambda^*(h,P_1,P_2)}.

\section{Achievability}\label{rested_arms_sec:achievability}
In this section, we propose a scheme that achieves the lower bound of Section \ref{rested_arms_sec:lower_bound} asymptotically as the probability of error vanishes. Our policy is a modification of the policy proposed by Prabhu et al. \cite{prabhu2017learning} for the case of $K$ iid processes. We denote our policy by $\pi^\star(L,\delta)$, where $L>1$ and $\delta\in(0,1)$ are the parameters of the policy.

Our policy is based on a modification of the classical generalised likelihood ratio (GLR) test in which we replace the maximum that appears in the numerator of the classical GLR test statistic by an average computed with respect to a carefully constructed artificial prior over the space $\mathcal{P}(\mathcal{S})$ of all probability distributions on $\mathcal{S}$. We describe the modified GLR test statistic in the next section.

\subsection{The Modified GLR Test Statistic}
We revisit \eqref{rested_arms_eq:log_likelihood_under_hyp_h}, and suppose that each arm is selected once in the first $K$ time slots. Note that this does not affect the asymptotic performance. Then, under configuration $C=(h,P_1,P_2)$, the log-likelihood process $Z_h(n)$ may be expressed for any $n\geq K$ as
\begingroup\allowdisplaybreaks\begin{align}
	Z_h(n)=\sum\limits_{a=1}^{K}\log\nu(X_0^a)+\sum\limits_{i,j\in\mathcal{S}}N_h(n,i,j)\log P_1(j|i)
	+\sum\limits_{i,j\in\mathcal{S}}\left(\sum\limits_{a\neq h}N_a(n,i,j)\right)\log P_2(j|i),\label{rested_arms_eq:Z_{hh'}(N)_alt_expr}
\end{align}\endgroup
from which the likelihood process under $C$, denoted by $f(A^n,\bar{X}^n|C)$, may be written as
\begingroup\allowdisplaybreaks\begin{align}
	f(A^n,\bar{X}^n|C)=\prod\limits_{a=1}^K \nu(X_0^a)\prod\limits_{i,j\in\mathcal{S}}(P_1(j|i))^{N_h(n,i,j)}
	\cdot \prod\limits_{i,j\in\mathcal{S}}(P_2(j|i))^{\sum\limits_{a\neq h}N_a(n,i,j)}.\label{rested_arms_eq:likelihod_under_hyp_h}
\end{align}\endgroup
%Since \eqref{rested_arms_eq:Z_{hh'}(N)_alt_expr} denotes the log-likelihood process under configuration $C=(h,P_1,P_2)$, we may express the likelihood process under $C$, denoted by $f(A^n,\bar{X}^n|C)$, as
%\begingroup\allowdisplaybreaks\begin{align}
%	&f(A^n,\bar{X}^n|C)
%	&=\prod\limits_{a=1}^K \nu(X_0^a)\prod\limits_{i,j\in\mathcal{S}}\left((P_1(j|i))^{N_h(n,i,j)}(P_2(j|i))^{\sum\limits_{a\neq h}N_a(n,i,j)}\right).\label{rested_arms_eq:likelihod_under_hyp_h}
%\end{align}\endgroup

We now introduce an artificial prior on the space of all transition probability matrices for the state space $\mathcal{S}$. \textcolor{black}{Our choice of the prior is motivated by the requirement of having an appropriate conjugate prior for the likelihood in \eqref{rested_arms_eq:likelihod_under_hyp_h}. We therefore construct the Dirichlet distribution-based prior, noting that it meets our requirement.} Let $\text{Dir}(1,\ldots,1)$ denote the Dirichlet distribution with $|\mathcal{S}|$ parameters $\alpha_1,\ldots,\alpha_{|\mathcal{S}|}$, where $\alpha_j=1$ for all $j\in\mathcal{S}$. Then, denoting by $\mathscr{P}(\mathcal{S})$ the space of all transition probability matrices of size $|\mathcal{S}|\times |\mathcal{S}|$, we specify a prior on $\mathscr{P}(\mathcal{S})$ using the above Dirichlet distribution as follows: for any $P=(P(j|i))_{i,j\in\mathcal{S}}\in \mathscr{P}(\mathcal{S})$, $P(\cdot|i)$ is chosen according to the above Dirichlet distribution, independently of $P(\cdot|j)$ for all $j\neq i$. Further, for any two matrices $P,Q\in\mathscr{P}(\mathcal{S})$, the rows of $P$ are independent of those of $Q$. Then, it follows that under this prior, the joint density at ($P_1$, $P_2$) for $P_1,P_2\in\mathscr{P}(\mathcal{S})$  is
\begingroup\allowdisplaybreaks\begin{align}
	\mathscr{D}(P_1,P_2)&\coloneqq\prod\limits_{i\in\mathcal{S}}\frac{\prod\limits_{j\in\mathcal{S}}(P_1(j|i))^{\alpha_j-1}}{B(1\ldots,1)}\prod\limits_{i\in\mathcal{S}}\frac{\prod\limits_{j\in\mathcal{S}}(P_2(j|i))^{\alpha_j-1}}{B(1\ldots,1)}\nonumber\\
	&=\frac{1}{B(1,\ldots,1)^{2|\mathcal{S}|}}\label{rested_arms_eq:Dirichlet_prior},
\end{align}\endgroup
where $B(1,\ldots,1)$ denotes the normalisation factor for the distribution $\text{Dir}(1,\ldots,1)$, and the second line above follows by substituting $\alpha_j=1$, $j\in\mathcal{S}$.
%by considering a Dirichlet prior with fixed parameters $(\alpha_j)_{j\in\mathcal{S}}$ on the space $\mathcal{P}(\mathcal{S})$ of all probability distributions on $\mathcal{S}$; here, $\alpha_j>0$ for all $j\in\mathcal{S}$. We set $\alpha_j=1$ for all $j\in \mathcal{S}$, and denote the resulting Dirichlet distribution by $\text{Dir}(1,\ldots,1)$. This choice of prior is motivated by the fact that the posterior distribution under this prior belongs to the family of Dirichlet distributions on $\mathcal{S}$, when the likelihood of arm selections and observations is given by \eqref{rested_arms_eq:likelihod_under_hyp_h}. We denote by $\mathscr{D}(P_1,P_2|C)$ the probability, under the aforementioned prior, of jointly picking each row of matrix $P_1$ independently of its remaining rows and independently of all rows of matrix $P_2$ (similar condition for picking each row of matrix $P_2$), i.e.,
%\begingroup\allowdisplaybreaks\begin{align}
%	&\mathscr{D}(P_1,P_2|C)
%	&=\frac{1}{(B((\alpha_j)_{j\in\mathcal{S}}))^{2|\mathcal{S}|}}\prod\limits_{i\in\mathcal{S}}\prod\limits_{j\in\mathcal{S}}(P_1(j|i)P_2(j|i))^{\alpha_j-1}\label{rested_arms_eq:Dirichlet_prior},
%\end{align}\endgroup
%where $(B((\alpha_j)_{j\in\mathcal{S}}))^{2|\mathcal{S}|}$ is the normalisation factor that makes \eqref{rested_arms_eq:Dirichlet_prior} a joint probability distribution.
%To simplify our calculations, we set $\alpha_j=1$ for all $j\in\mathcal{S}$. This corresponds to picking each row of $P_1$ (resp. $P_2$) independently and uniformly at random from the $(|\mathcal{S}|-1)$-dimensional probability simplex.

By a minor abuse of notation, we denote by $f(A^n,\bar{X}^n|H_h)$ the average of the likelihood in \eqref{rested_arms_eq:likelihod_under_hyp_h} computed with respect to the prior in \eqref{rested_arms_eq:Dirichlet_prior}. From the property that the Dirichlet distribution is the appropriate conjugate prior for the observation process,
\begingroup\allowdisplaybreaks\begin{align}
	&f(A^n,\bar{X}^n|H_h)=\prod\limits_{a=1}^{K}\nu(X_0^a)\prod\limits_{i\in\mathcal{S}}\frac{B((N_h(n,i,j)+1)_{j\in\mathcal{S}})}{B(1,\ldots,1)}
	\prod\limits_{i\in\mathcal{S}}\frac{B\left(\left(\sum\limits_{a\neq h}N_a(n,i,j)+1\right)_{j\in\mathcal{S}}\right)}{B(1,\ldots,1)},\label{rested_arms_eq:f(Z^n,A^n|H_h)}
\end{align}\endgroup
where in the above expression, $B((N_h(n,i,j)+1)_{j\in\mathcal{S}})$ denotes the normalisation factor for a Dirichlet distribution with parameters $(N_h(n,i,j)+1)_{j\in\mathcal{S}}$. It can be shown that  $f(A^n,\bar{X}^n|H_h)$ is also the expected value of the likelihood in \eqref{rested_arms_eq:likelihod_under_hyp_h} computed with respect to the prior in \eqref{rested_arms_eq:Dirichlet_prior}, i.e.,
\begingroup\allowdisplaybreaks\begin{align}
	f(A^n,\bar{X}^n|H_h)
	=\prod\limits_{a=1}^{K}\nu(X_0^a)\prod\limits_{i\in\mathcal{S}}E\left[\prod\limits_{j\in\mathcal{S}}X_{ij}^{N_h(n,i,j)}\cdot Y_{ij}^{\sum\limits_{a\neq h}N_a(n,i,j)}\right]
	%	\nonumber
%	\\
%	&=\prod\limits_{a=1}^{K}\nu(X_0^a)\prod\limits_{i\in\mathcal{S}}\left(\frac{B((N_h(n,i,j)+1)_{j\in\mathcal{S}})}{B(1,\ldots,1)}\right)
%	&\hspace{3cm}\prod\limits_{i\in\mathcal{S}}\left(\frac{B((\sum\limits_{a\neq h}N_a(n,i,j)+1)_{j\in\mathcal{S}})}{B(1,\ldots,1)}\right),
\end{align}\endgroup
where in the above set of equations, the random vectors $(X_{ij})_{i,j\in\mathcal{S}}$ and $(Y_{ij})_{i,j\in\mathcal{S}}$ are independent with independent components, and jointly distributed according to \eqref{rested_arms_eq:Dirichlet_prior}, and the expectation is also with respect to this joint density.

Let $\hat{P}^n_{h,1}$ and $\hat{P}^n_{h,2}$ denote the maximum likelihood estimates of probability transition matrices $P_1$ and $P_2$ respectively under the hypothesis $H_h$. Taking partial derivatives of the right-hand side of  \eqref{rested_arms_eq:likelihod_under_hyp_h} with respect to $P_1(j|i)$ and $P_2(j|i)$ for each $i,j\in\mathcal{S}$, and setting each of these derivatives to zero, we get
\begingroup\allowdisplaybreaks\begin{align}
	\hat{P}_{h,1}^n(j|i)
	%=\arg\max\limits_{C=(h,\cdot,\cdot)}f(A^n,\bar{X}^n|C)
	=\frac{N_h(n,i,j)}{N_h(n,i)},\quad
	\hat{P}_{h,2}^n(j|i)
	%&=\arg\max\limits_{C=(h,\cdot,\cdot)}f(A^n,\bar{X}^n|C)
	=\frac{\sum\limits_{a\neq h}N_a(n,i,j)}{\sum\limits_{a\neq h}N_a(n,i)}.\label{rested_arms_eq:ML_est_under_hyp_h}
\end{align}\endgroup

%let $\hat{P}_{h,1}^n$ and $\hat{P}_{h,2}^n$ denote the maximum likelihood estimates of $P_1$ and $P_2$ based on the sequence $(A^n,\bar{X}^n)$ under configuration $C$. That is, for all $i,j\in\mathcal{S}$,
%\begingroup\allowdisplaybreaks\begin{align}
%	\hat{P}_{h,1}^n(j|i)
%	%=\arg\max\limits_{C=(h,\cdot,\cdot)}f(A^n,\bar{X}^n|C)
%	&=\frac{N_h(n,i,j)}{N_h(n,i)},
%	\hat{P}_{h,2}^n(j|i)
%	%&=\arg\max\limits_{C=(h,\cdot,\cdot)}f(A^n,\bar{X}^n|C)
%	&=\frac{\sum\limits_{a\neq h}N_a(n,i,j)}{\sum\limits_{a\neq h}N_a(n,i)}.\label{rested_arms_eq:ML_est_under_hyp_h}
%\end{align}\endgroup
Plugging the estimates in \eqref{rested_arms_eq:ML_est_under_hyp_h}  back into \eqref{rested_arms_eq:likelihod_under_hyp_h}, we get the maximum likelihood of all observations and actions under hypothesis $H_h$:
\begingroup\allowdisplaybreaks\begin{align}
	\hat{f}(A^n,\bar{X}^n|H_h)&\coloneqq\max\limits_{C=(h,\cdot,\cdot)}f(A^n,\bar{X}^n|C)\nonumber\\
	&=\prod\limits_{a=1}^K \nu(X_0^a)\prod\limits_{i,j\in\mathcal{S}}\bigg\lbrace\left(\frac{N_h(n,i,j)}{N_h(n,i)}\right)^{N_h(n,i,j)}
	\left(\frac{\sum\limits_{a\neq h}N_a(n,i,j)}{\sum\limits_{a\neq h}N_a(n,i)}\right)^{\sum\limits_{a\neq h}N_a(n,i,j)}\bigg\rbrace.\label{rested_arms_eq:ml_likelihod_under_hyp_h}
\end{align}\endgroup

We now define our modified GLR statistic. Let $H_h$ and $H_{h'}$ be any two hypotheses, with $h'\neq h$. Let $\pi$ be a policy whose sequence of arm selections and observations up to time  $n$ is $(A^n,\bar{X}^n)$. Then, the modified GLR statistic of $H_h$ with respect to $H_{h'}$ up to time $n$ is denoted by $M_{hh'}(n)$ and is defined as
\begingroup\allowdisplaybreaks\begin{align}
	M_{hh'}(n)
	%&=\log\frac{f(A^n,\bar{X}^n|C)}{\hat{f}(A^n,\bar{X}^n|C')}
	&=\log\frac{f(A^n,\bar{X}^n|H_h)}{\hat{f}(A^n,\bar{X}^n|H_{h'})}\nonumber\\
	&=T_1+T_2(n)+T_3(n)+T_4(n)+T_5(n),\label{rested_arms_eq:M_{hh'}(n)}
\end{align}\endgroup
where the terms appearing in \eqref{rested_arms_eq:M_{hh'}(n)} are as follows.
\begin{enumerate}
	\item The term $T_1$ is given by
	\begin{equation}
		T_1=2|\mathcal{S}|\log\left(\frac{1}{B(1,\ldots,1)}\right).\label{rested_arms_eq:t_1}
	\end{equation}
	\item The term $T_2(n)$ is given by
    \begin{equation}
    	T_2(n)=\sum\limits_{i\in\mathcal{S}}\log B((N_h(n,i,j)+1)_{j\in\mathcal{S}}).\label{rested_arms_eq:t_2(n)}
    \end{equation}
	\item The term $T_3(n)$ is given by
	\begin{equation}
		T_3(n)=\sum\limits_{i\in\mathcal{S}}\log B\left(\left(\sum\limits_{a\neq h}N_a(n,i,j)+1\right)_{j\in\mathcal{S}}\right).\label{rested_arms_eq:t_3(n)}
	\end{equation}
	\item The term $T_4(n)$ is given by
	\begin{equation}
		T_4(n)=-\sum\limits_{i,j\in\mathcal{S}}N_{h'}(n,i,j)\log\frac{N_{h'}(n,i,j)}{N_{h'}(n,i)}.\label{rested_arms_eq:t_4(n)}
	\end{equation}
	\item The term $T_5(n)$ is given by
	\begingroup\allowdisplaybreaks\begin{align}
		T_5(n)=-\sum\limits_{i,j\in\mathcal{S}}\sum\limits_{a\neq h'}N_{a}(n,i,j)\log\frac{\sum\limits_{a\neq h'}N_{a}(n,i,j)}{\sum\limits_{a\neq h'}N_{a}(n,i)}.\label{rested_arms_eq:t_5(n)}
	\end{align}\endgroup
\end{enumerate}

Note that $\nu$, the distribution of the initial state of any arm, is irrelevant since it appears in both \eqref{rested_arms_eq:f(Z^n,A^n|H_h)} and \eqref{rested_arms_eq:ml_likelihod_under_hyp_h}, and thus cancels out in writing \eqref{rested_arms_eq:M_{hh'}(n)}. 

\begin{remark}
	We wish to mention here that the expression on the right-hand side of \eqref{rested_arms_eq:likelihod_under_hyp_h} for $f(A^n,\bar{X}^n|C)$ represents the likelihood of all observations up to time $n$ ``conditioned on'' the actions $A^n$ up to time $n$. In other words, a more precise expression for $f(A^n,\bar{X}^n|C)$ is
	\begingroup\allowdisplaybreaks\begin{align}
		f(A^n,\bar{X}^n|C)=\bigg[\prod\limits_{t=0}^n P_h(A_t|A^{t-1},\bar{X}^{t-1})\bigg]\prod\limits_{a=1}^K \nu(X_0^a)\prod\limits_{i,j\in\mathcal{S}}(P_1(j|i))^{N_h(n,i,j)}
	\cdot \prod\limits_{i,j\in\mathcal{S}}(P_2(j|i))^{\sum\limits_{a\neq h}N_a(n,i,j)},\label{rested_arms_eq:likelihod_under_hyp_h_precise}
	\end{align}\endgroup
	where $P_h(A_t|A^{t-1},\bar{X}^{t-1})$ represents the probability of selecting the arm $A_t$ at time $t$ when the true hypothesis is $H_h$ (i.e., when $h$ is the index of the odd arm), with the convention that at time $t=0$, this term represents $P_h(A_0)$. Note that for any policy, this must be independent of the true hypothesis $H_h$, and
is thus the same for any two hypotheses $H_h$ and $H_{h'}$.
%	However, this probability is independent of the odd arm index since the policy $\pi^\star(L,\delta)$ decides to select arm $A_t$ at time $t$ according to the probability distribution $\lambda_{opt}(h^*(t),\hat{P}^t_{h^*(t),1},\hat{P}^t_{h^*(t),2})$ which is independent of the odd arm index. Therefore, this probability is the same for any two hypotheses $H_h$ and $H_{h'}$, where $h'\neq h$.
As a consequence of this, the first term within square brackets on the right-hand side of \eqref{rested_arms_eq:likelihod_under_hyp_h_precise} appears in both the numerator and the denominator terms of the modified GLR statistic of \eqref{rested_arms_eq:M_{hh'}(n)}, and  thus cancels out. Hence, we omit writing this term in the expressions of \eqref{rested_arms_eq:likelihod_under_hyp_h}, \eqref{rested_arms_eq:f(Z^n,A^n|H_h)} and \eqref{rested_arms_eq:ml_likelihod_under_hyp_h}.
%	 However, we take this term into account in our analysis of the probability of error of policy $\pi^\star(L,\delta)$ in Section \ref{rested_arms_appndx:pi_{LRMB}(L,delta)_belongs_to_Pi(epsilon)}.
\end{remark}

\subsection{The Policy $\pi^\star(L,\delta)$}
With the above ingredients in place, we now describe our policy based on the modified GLR test statistic of \eqref{rested_arms_eq:M_{hh'}(n)}. Let
\begin{equation}
	M_h(n)\coloneqq \min\limits_{h'\neq h}M_{hh'}(n)\label{rested_arms_eq:M_h(n)}
\end{equation}
denote the modified GLR of hypothesis $H_h$, $h\in\mathcal{A}$, with respect to its nearest alternative.
\newpage
\vspace{.1in}
\hrule
\vspace{.1in}
\noindent \textbf{\underline{\emph{Policy }$\pi^{\star}(L,\delta)$}}:
\vspace{.1in}\\
Fix parameters $L> 1$ and $\delta\in(0,1)$. Let $(B_n)_{n\geq 1}$ be a sequence of iid Bernoulli($\delta)$ random variables such that $B_{n+1}$ is independent of the sequence $(A^n,\bar{X}^n)$ for all $n\in\{0,1,2,\ldots\}$.
Let $A_0=1$, $A_1=2$ and so on until $A_{K-1}=K$.  For $n\geq K-1$, follow the below mentioned steps until stoppage.
\begin{enumerate}
	\item Compute $\theta(n)\in \arg\max\limits_{h\in\mathcal{A}}M_h(n)$. Resolve ties, if any, uniformly at random.
	\item If $M_{\theta(n)}(n)<\log((K-1)L$, then sample the next arm $A_{n+1}$ based on the history $(A^n,\bar{X}^n)$ as per the following rule:
	    \begin{enumerate}
            \item If $B_{n+1}=1$, sample $A_{n+1}$ uniformly at random.
            \item If $B_{n+1}=0$, sample $A_{n+1}$ according to $\lambda_{opt}(\theta(n),\hat{P}^n_{\theta(n),1},\hat{P}^n_{\theta(n),2})$.
	    \end{enumerate}
	    Update $n \leftarrow n+1$ and go back to step 1.	
	\item If $M_{\theta(n)}(n)\geq\log((K-1)L)$, then stop and declare $\theta(n)$ as the odd arm index.
\end{enumerate}
\vspace{.1in}
\hrule
\vspace{0.1in}
In the above policy, $\theta(n)$ provides the best estimate  of the odd arm at time $n$. If the modified GLR test statistic of arm $\theta(n)$ is sufficiently larger than that of its nearest incorrect alternative ($\geq \log((K-1)L)$), then this indicates that the policy is confident that $\theta(n)$ is the odd arm. At this stage, the policy stops taking further samples and declares $\theta(n)$ as the index of the odd arm. If not, the policy continues to obtain further samples.

{We refer to the rule in item (2) above as \emph{forced exploration} with parameter $\delta$. A similar rule also appears in \cite{albert1961sequential}. Based on the description in items 2(a) and 2(b) above, it follows that for each $a\in\mathcal{A}$,}
\begingroup\allowdisplaybreaks\begin{align}
	P(A_{n+1}=a|A^n,\bar{X}^n)
	&=\frac{\delta}{K}+(1-\delta)\,\lambda_{opt}(\theta(n),\hat{P}^n_{\theta(n),1},\hat{P}^n_{\theta(n),2})(a)\nonumber\\
	&\geq \frac{\delta}{K}>0.\label{rested_arms_eq:P(A_{n+1}=a|sigma(A^n,X^n)}
\end{align}\endgroup
As we will see, the strictly positive lower bound in \eqref{rested_arms_eq:P(A_{n+1}=a|sigma(A^n,X^n)} will ensure that the policy selects each arm at a non-trivial frequency so as to allow for sufficient exploration of all the arms. Also, we will show that the parameters $L$ and $\delta$ may be selected so that our policy achieves a desired target error probability, while also ensuring that the normalised expected stopping time of the policy is arbitrarily close to the lower bound in \eqref{rested_arms_eq:lower_bound}.

{
\begin{remark}
%	In order to find the distribution $\lambda_{opt}(\theta(n),\hat{P}^n_{\theta(n),1},\hat{P}^n_{\theta(n),2})$ in step (2(b)) of the policy, we first write an expression for $D^*(\theta(n),\hat{P}^n_{\theta(n),1},\hat{P}^n_{\theta(n),2})$, similar in form to \eqref{rested_arms_eq:D^*(h,P_1,P_2)_simpl_final}, by replacing the probability transition matrices $P_1$ and $P_2$ in \eqref{rested_arms_eq:D^*(h,P_1,P_2)_simpl_final} and \eqref{rested_arms_eq:P(j|i)} with their corresponding ML estimates $\hat{P}^n_{\theta(n),1}$ and $\hat{P}^n_{\theta(n),2}$ respectively. Then, we let $\lambda^*(n)$ denote the value of $\lambda\in[0,1]$ that yields the maximum value is similar in form to that in \eqref{rested_arms_eq:lambda^*(h,P_1,P_2)}, with $\lambda^*$ in \eqref{rested_arms_eq:lambda^*(h,P_1,P_2)} replaced by the value of $\lambda\in[0,1]$ that maximises .
Evaluating the distribution $\lambda_{opt}(\theta(n),\hat{P}^n_{\theta(n),1},\hat{P}^n_{\theta(n),2})$ in step 2(a) of the policy involves solving the maximisation problem in \eqref{rested_arms_eq:D^*(h,P_1,P_2)_simpl_final} with the probability transition matrices $P_1$ and $P_2$ replaced by their corresponding ML estimates $\hat{P}^n_{\theta(n),1}$ and $\hat{P}^n_{\theta(n),2}$ respectively at each time $n\geq K-1$ until stoppage. In the event when any of the rows of the estimated matrices has all its entries as zero, we substitute the corresponding zero row by a row with a single `1' in one of the $|\mathcal{S}|$ positions picked uniformly at random. As we shall demonstrate shortly, the ML estimates converge to their respective true values as more observations are accumulated. Therefore, such a substitution operation (or any modification thereof that replaces the all-zero rows by an arbitrary probability vector) needs to be carried out only for finitely many time slots, and does not affect the asymptotic performance of the policy.
\end{remark}}

\subsection{Performance of $\pi^\star(L,\delta)$}
In this subsection, we show that the expected number of samples required by policy $\pi^\star(L,\delta)$ to find the index of the odd arm can be made arbitrarily close to that in \eqref{rested_arms_eq:lower_bound} in the regime of vanishing error probabilities. We show that this can be achieved by choosing the parameters $L$ and $\delta$ carefully. We organise this subsection as follows:
\begin{enumerate}
%    \item We first show that when the underlying configuration of the arms is $C=(h,P_1,P_2)$, the maximum likelihood estimates $\hat{P}^n_{h,1}$ and $\hat{P}^n_{h,2}$ converge \text{almost surely} to the respective true values $P_1$ and $P_2$.
	\item First, we show that when the odd arm index is $h$, the modified GLR test statistic $M_h(n)$ has a strictly positive drift under our policy. Subsequently, we show that our policy stops in finite time \text{almost surely}.
	\item For any fixed target error probability $\epsilon>0$, we show that by setting $L=1/\epsilon$, our policy belongs to the family $\Pi(\epsilon)$, i.e., its probability of error at stoppage is within $\epsilon$.
	\item We obtain an upper bound on the expected stopping time of our policy, and demonstrate that this upper bound may be made arbitrarily close to the lower bound in \eqref{rested_arms_eq:lower_bound} by choosing an appropriate value of $\delta\in (0,1)$.
\end{enumerate}

\subsubsection{Strictly Positive Drift of the Modified GLR Test Statistic}\label{rested_arms_subsubsec:positive_drift}
Consider a version of the policy $\pi^\star(L, \delta)$ that never stops, i.e., a version that never checks for the condition in item (3) in the description of the policy. Call this version of the policy the {\em non-stopping version of $\pi^\star(L, \delta)$}. The main result on the strictly positive drift of the modified GLR test statistic is as described in the following proposition.
\begin{prop}\label{rested_arms_prop:positive_drift_of_M_{hh'}(n)}
	Fix parameters $L>1$ and $\delta\in(0,1)$. For all $h, h'\in \mathcal{A}$ such that $h'\neq h$, under the non-stopping version of $\pi^\star(L, \delta)$,
	\begin{equation}
		\liminf\limits_{n\to\infty}\frac{M_{hh'}(n)}{n}>0\quad \text{\text{almost surely}}.\label{rested_arms_eq:positive_drift_of_M_{hh'}(n)}
	\end{equation}
\end{prop}
%\begin{proof}
%	See appendix \ref{rested_arms_appndx:proof_of_strictly_positive_drift_of_M_{hh'}(n)}.
%\end{proof}
The proof of Proposition \ref{rested_arms_prop:positive_drift_of_M_{hh'}(n)} is based on the key idea that forced exploration with parameter $\delta\in(0,1)$ (items 2(a) and 2(b) of policy $\pi^\star(L,\delta)$) results in sampling each arm with a strictly positive rate that grows linearly. It is in showing an analogue of Proposition \ref{rested_arms_prop:positive_drift_of_M_{hh'}(n)} for iid Poisson observations that the authors of \cite{vaidhiyan2017learning} use their result of \cite[Proposition 3]{vaidhiyan2017learning} on guaranteed exploration at a strictly positive rate. Since it is not clear if the analogue of \cite[Proposition 3]{vaidhiyan2017learning} holds in general, we use the idea in \cite{albert1961sequential} of forced exploration. We present the details in Section \ref{rested_arms_appndx:proof_of_strictly_positive_drift_of_M_{hh'}(n)}. We refer the reader to \cite{garivier2016optimal} on how to make do with forced exploration at a sublinear rate.


As an immediate consequence of the above proposition, we note that \text{almost surely},
\begingroup\allowdisplaybreaks\begin{align}
	\liminf\limits_{n\to\infty}M_h(n)=\liminf\limits_{n\to\infty}\min\limits_{h'\neq h}M_{hh'}(n)>0.\label{rested_arms_eq:positive_drift_of_M_h(n)}
\end{align}\endgroup
The result in \eqref{rested_arms_eq:positive_drift_of_M_h(n)} has the following implication. For $h, h'\in \mathcal{A}$ such that $h'\neq h$, let $\textsf{GLR}_{hh'}(n)$ denote the classical GLR test statistic of hypothesis $\mathcal{H}_h$ with respect to $\mathcal{H}_{h'}$ at time $n$. Clearly, $M_{hh'}(n) \leq \textsf{GLR}_{hh'}(n)$ \text{almost surely} for all $n$. Further, $\textsf{GLR}_{hh'}(n)=-\textsf{GLR}_{h'h}(n)$. We then have
\begingroup\allowdisplaybreaks\begin{align}
	\limsup\limits_{n\to\infty}M_{h'}(n)&=\limsup\limits_{n\to\infty}\min\limits_{a\neq h'}M_{h'a}(n)\nonumber\\
	&\leq \limsup\limits_{n\to\infty}M_{h'h}(n)\nonumber\\
	&\leq \limsup\limits_{n\to\infty}\textsf{GLR}_{h'h}(n)\nonumber\\
	&=\limsup\limits_{n\to\infty}-\textsf{GLR}_{hh'}(n)\nonumber\\
	&\leq -\liminf\limits_{n\to\infty}M_{hh'}(n)\nonumber\\
	&\leq -\liminf\limits_{n\to\infty}M_{h}(n)\nonumber\\
	&<0\quad \text{\text{almost surely}}.\label{rested_arms_eq:limsup_M_{h'}(n)_less_than_0}
\end{align}\endgroup
From the above set of inequalities, it follows that under policy $\pi^\star(L,\delta)$, \text{almost surely},
 \begin{equation}
 	\theta(n)=h\quad \forall~n\text{ sufficiently large}.\label{rested_arms_eq:h^*(n)_equal_to_h_almost_surely}
 \end{equation}

Let $\pi^\star_h(L,\delta)$ denote a version of the policy $\pi^\star(L, \delta)$ that stops only at declaration $h$. It then follows that the stopping times of policies $\pi^\star(L,\delta)$ and $\pi^\star_h(L,\delta)$ are \text{almost surely} related as $\tau(\pi^\star_h(L,\delta))\geq \tau(\pi^\star(L,\delta))$, as a consequence of which we have the following set of almost sure inequalities:
\begingroup\allowdisplaybreaks\begin{align}
	\tau(\pi^\star(L,\delta))
	&\leq \tau(\pi^\star_h(L,\delta))\nonumber\\
	&=\inf\{n\geq 1:M_h(n)\geq \log((K-1)L)\}\nonumber\\
	&\leq \inf\bigg\lbrace n\geq 1:M_{hh'}(n')\geq \log((K-1)L)\text{ for all }n'\geq n\text{ and for all }h'\neq h\bigg\rbrace\nonumber\\
	&<\infty,\label{rested_arms_eq:stopping_time_finite_almost_surely}
\end{align}\endgroup
where the last line follows as a consequence of Proposition \ref{rested_arms_prop:positive_drift_of_M_{hh'}(n)}. This establishes that the policy $\pi^\star(L,\delta)$ stops in finite time \text{almost surely}.

\subsubsection{Error Probability of Policy $\pi^\star(L,\delta)$}
We now show that given a target error probability $\epsilon>0$, by setting $L=1/\epsilon$, the policy $\pi^\star(L,\delta)$ belongs to the family $\Pi(\epsilon)$ for all $\delta\in (0,1)$. This is formalised in the proposition below.
\begin{prop}\label{rested_arms_prop:pi_{LRMB}(L,delta)_belongs_to_Pi(epsilon)}
	Fix $\epsilon>0$. Then, for $L=1/\epsilon$, we have $\pi^\star(L,\delta)\in\Pi(\epsilon)$ for all $\delta\in(0,1)$.
\end{prop}
{The proof uses Proposition \ref{rested_arms_prop:positive_drift_of_M_{hh'}(n)} and the fact that policy $\pi^{\star}(L,\delta)$ stops in finite time \text{almost surely}. Further, the average in the numerator of the modified GLR test statistic, in place of the maximum in the classical GLR test statistic, plays a role. For the details, see Section \ref{rested_arms_appndx:pi_{LRMB}(L,delta)_belongs_to_Pi(epsilon)}.}

%\begin{proof}
%As remarked earlier, we note that policy $\pi^\star(L,\delta)$ commits error if one of the following events is true:
%\begin{enumerate}
%	\item The policy never stops in finite time.
%	\item The policy stops in finite time and declares $h'\neq h$ as the true index of the odd arm.
%%	\item There exists $n$ such that $h^*(n)=h'\neq h$ and $M_{\theta(n)}\geq \log((K-1)L)$, at which point the policy stops at time $n$ and declares $h'$ as the true index of the odd arm.
%\end{enumerate}
%The event in item $1$ above has zero probability as a consequence of \eqref{rested_arms_eq:stopping_time_finite_almost_surely}.
%%Also, in the event in item $2$ above implies that $M_{h'}(n)\geq \log((K-1)L)$.
%Thus, the probability of error of policy $\pi=\pi^\star(L,\delta)$, which we denote by $P^\pi_e$, may be evaluated as follows: suppose $C=(h,P_1,P_2)$ is the underlying configuration of the arms. Then,
%\begingroup\allowdisplaybreaks\begin{align}
%	&P^\pi_e =P^\pi(I(\pi)\neq h|C)
%	&=P^\pi\bigg(\exists~ n\text{ and }~h'\neq h\text{ such that }
%	&\hspace{3cm}I(\pi)=h'\text{ and } \tau(\pi)=n\bigg\vert C\bigg).\label{rested_arms_eq:P_e_partial_1}
%\end{align}\endgroup
%We now let
%\begingroup\allowdisplaybreaks\begin{align}
%	\mathcal{R}_{h'}(n)=\{\omega:\tau(\pi)(\omega)=n,\,I(\pi)(\omega)=h'\}\label{rested_arms_eq:R_{h'}(n)}
%\end{align}\endgroup
%denote the set of all sample paths for which the policy stops at time $n$ and declares $h'$ as the true index of the odd arm. Clearly, the collection $\{\mathcal{R}_{h'}(n):h'\neq h,\,n\geq 0\}$ is a collection of mutually disjoint sets. Therefore, we have
%\begingroup\allowdisplaybreaks\begin{align}
%&P^\pi_e=P^\pi\left(\bigcup\limits_{h'\neq h}\,\bigcup\limits_{n=0}^{\infty}\mathcal{R}_{h'}(n)\bigg\vert C\right)
%&= \sum\limits_{h'\neq h}\sum\limits_{n=0}^{\infty}P^\pi(\tau(\pi)=n,I(\pi)=h'|C)
%&= \sum\limits_{h'\neq h}\sum\limits_{n=0}^{\infty}~\int\limits_{\mathcal{R}_{h'}(n)}\,dP^\pi(\omega|C)
%&=\sum\limits_{h'\neq h}\sum\limits_{n=0}^{\infty}~\int\limits_{\mathcal{R}_{h'}(n)}f(\bar{X}^n(\omega),A^n(\omega)|C)\,d(\bar{X}^n(\omega),A^n(\omega))
%&\stackrel{(a)}{\leq} \sum\limits_{h'\neq h}\sum\limits_{n=0}^{\infty}~\int\limits_{\mathcal{R}_{h'}(n)}\hat{f}(\bar{X}^n(\omega),A^n(\omega)|H_h)\,d(\bar{X}^n(\omega),A^n(\omega))
%&=\sum\limits_{h'\neq h}\sum\limits_{n=0}^{\infty}~\bigg\lbrace\int\limits_{\mathcal{R}_{h'}(n)}e^{-M_{h'h}(n)}~{f}(\bar{X}^n(\omega),A^n(\omega)|H_{h'})
%&\hspace{6cm} d(\bar{X}^n(\omega),A^n(\omega))\bigg\rbrace
%&\leq \sum\limits_{h'\neq h}\sum\limits_{n=0}^{\infty}~\bigg\lbrace\int\limits_{\mathcal{R}_{h'}(n)}\frac{1}{(K-1)L}~{f}(\bar{X}^n(\omega),A^n(\omega)|H_{h'})
%&\hspace{6cm} d(\bar{X}^n(\omega),A^n(\omega))\bigg\rbrace
%&=\sum\limits_{h'\neq h}\frac{1}{(K-1)L}~P^\pi\left(\bigcup\limits_{n=0}^{\infty}\mathcal{R}_{h'}(n)\bigg|H_{h'}\right)
%&{\leq}~ \frac{1}{L},
%\end{align}\endgroup
%where $(a)$ above follows by the definition of $\hat{f}$ in \eqref{rested_arms_eq:ml_likelihod_under_hyp_h}. Setting $L=1/\epsilon$ gives $P_e^{\pi}\leq \epsilon$, thus proving that $\pi=\pi^\star(L,\delta)\in\Pi(\epsilon)$. This completes the proof of the proposition.
%\end{proof}

\subsubsection{Upper Bound on the Expected Stopping Time of Policy $\pi^\star(L,\delta)$}
We conclude this section by presenting an upper bound on the expected stopping time of the policy $\pi^\star(L,\delta)$. We show that this upper bound may be made arbitrarily close to the lower bound in \eqref{rested_arms_eq:lower_bound} by tuning $\delta$ appropriately.

As a first step, we show that under the non-stopping version of the policy $\pi^\star(L,\delta)$, when $C=(h,P_1,P_2)$ is the underlying arms configuration, the modified GLR process has an asymptotic drift that is close to $D^*(h,P_1,P_2)$ that appears in the lower bound \eqref{rested_arms_eq:lower_bound}.

%This is given in the following proposition, the proof of which is relegated to the appendix.
%\begin{prop}\label{rested_arms_prop:lim_M_h(n)/n_correct_drift}
%	Let $C=(h,P_1,P_2)$ denote the underlying configuration of the arms. Then, under the non-stopping version of policy $\pi^\star(L,\delta)$, we have
%	\begin{equation}
%		\lim\limits_{n\to\infty}\frac{M_{hh'}(n)}{n}=L_\delta^*(h,P_1,P_2),\label{rested_arms_eq:lim_M_{h}(n)/n_almost_correct_drift}
%	\end{equation}
%	where the quantity $L_\delta^*(h,P_1,P_2)$ is given by
%	\begingroup\allowdisplaybreaks\begin{align}
%		&L_\delta^*(h,P_1,P_2)
% 		&=\bigg(\lambda_\delta^*(h)\,D(P_1||P_\delta|\mu_1)
% 		&\hspace{1cm}+(1-\lambda_\delta^*(h))\left(\frac{K-2}{K-1}\right)D(P_2||P_\delta|\mu_2)\bigg),\label{rested_arms_eq:L_delta^*(h,P_1,P_2)}
%    \end{align}\endgroup
%    with $\lambda_\delta^*(h)\in[0,1]$ and the probability transition matrix $P_\delta$ specified as follows:
%    \begin{equation}
%     	\lambda_\delta^*(h)=\frac{\delta}{K}+(1-\delta)\lambda_{opt}(h),\label{rested_arms_eq:lambda_delta^*(h)}
%     	\end{equation}
%     	and for each $i,j\in\mathcal{S}$,
%     	\begingroup\allowdisplaybreaks\begin{align}
%     	P_\delta(j|i)=\frac{N_\delta(j|i)}{D_\delta(j|i)},\label{rested_arms_eq:P_delta(j|i)}
%     \end{align}\endgroup
%     where the numerator and denominator terms $N_\delta(j|i)$ and $D_\delta(j|i)$ in \eqref{rested_arms_eq:P_delta(j|i)} are given by
%     \begingroup\allowdisplaybreaks\begin{align}
%     	N_\delta(j|i)&=\lambda_\delta^*(h)\mu_1(i)P_1(j|i)
%     	&+(1-\lambda_\delta^*(h))\left(\frac{K-2}{K-1}\right)\mu_2(i)P_2(j|i),
%     	D_\delta(j|i)&=\lambda_\delta^*(h)\mu_1(i)+(1-\lambda_\delta^*(h))\left(\frac{K-2}{K-1}\right)\mu_2(i).\label{rested_arms_eq:D_delta(j|i)}
%     \end{align}\endgroup
%In \eqref{rested_arms_eq:lambda_delta^*(h)}, $\lambda_{opt}(h)$ denotes the value of $\lambda(h)\in[0,1]$ that maximises the right-hand side of \eqref{rested_arms_eq:D^*(h,P_1,P_2)_simpl_final}.
%\end{prop}
\begin{prop}\label{rested_arms_prop:lim_M_h(n)/n_correct_drift}
	Fix  parameters $L\geq 1$ and $\delta\in(0,1)$. Then, for all $h, h'\in \mathcal{A}$ such that $h'\neq h$, under the non-stopping version of $\pi^{\star}(L,\delta)$ and under the arms configuration $C=(h, P_1, P_2)$,
	\begin{equation}
		\lim\limits_{n\to\infty}\frac{M_{hh'}(n)}{n}=D_\delta^*(h,P_1,P_2) \quad \text{\text{almost surely}},
		\label{rested_arms_eq:lim_M_{h}(n)/n_almost_correct_drift}
	\end{equation}
	where the quantity $D_\delta^*(h,P_1,P_2)$ is given by
\begingroup\allowdisplaybreaks\begin{align}
  D_\delta^*(h,P_1,P_2)=\lambda_\delta^*\,D(P_1||P_\delta|\mu_1)
		+(1-\lambda_\delta^*)\frac{(K-2)}{(K-1)}D(P_2||P_\delta|\mu_2),\label{rested_arms_eq:L_delta^*(h,P_1,P_2)}
    \end{align}\endgroup
    with $\lambda_\delta^*=\frac{\delta}{K}+(1-\delta)\lambda^*$
    and for each $i,j\in\mathcal{S}$, $P_\delta(j|i)$ is as in \eqref{rested_arms_eq:P(j|i)} with $\lambda_1$ replaced by $\lambda_\delta^*$.
\end{prop}
%\begin{proof}
%See appendix \ref{rested_arms_appndx:proof_of_prop_lim_M_h(n)/n_correct_drift}.	
%\end{proof}
We note that the policy $\pi^*(L,\delta)$ works with only the estimated transition probability matrices $\hat{P}^n_{\theta(n),1}$ and $\hat{P}^n_{\theta(n),2}$. To show \eqref{rested_arms_eq:lim_M_{h}(n)/n_almost_correct_drift}, we must therefore ensure that the estimates approach the true values and a property akin to continuity holds, that is, taking actions based on $\hat{P}^n_{\theta(n),1}$ and $\hat{P}^n_{\theta(n),2}$, which are only approximately close to $P_1$ and $P_2$, adds only $o(1)$ to the drift $D_\delta^*(h,P_1,P_2)$. This is the notion of {\em certainty equivalence} in control theory. The details of the proof may be found in Section \ref{rested_arms_appndx:proof_of_prop_lim_M_h(n)/n_correct_drift}.

%As our next step, we show that the stopping time of policy $\pi^\star(L,\delta)$ goes to infinity with vanishing error probability. We then use this to extend the result of Proposition \ref{rested_arms_prop:lim_M_h(n)/n_correct_drift} to policy $\pi^\star(L,\delta)$.
%\begin{lemma}\label{rested_arms_lemma:stopping_time_of_policy_goes_to_infinity}
%	Let $C=(h,P_1,P_2)$ denote the underlying configuration of the arms. Then, under policy $\pi^\star(L,\delta)$, we have
%	\begin{equation}
%		\liminf\limits_{L\to\infty}\tau(\pi^\star(L,\delta))=\infty\text{ \text{almost surely}}\label{rested_arms_eq:stopping_time_of_policy_goes_to_infinity}
%	\end{equation}
%\end{lemma}
%\begin{proof}
%See appendix \ref{rested_arms_appndx:stopping_time_of_policy_goes_to_infinity}.	
%\end{proof}
%We then have the following result.
%\begin{lemma}
%	Let $C=(h,P_1,P_2)$ denote the underlying configuration of the arms. Then, under policy $\pi^\star(L,\delta)$, we have
%	\begin{equation}
%		\lim\limits_{L\to\infty}\frac{M_{hh'}(\tau(\pi^\star(L,\delta)))}{\tau(\pi^\star(L,\delta))}=L_\delta^*(h,P_1,P_2).\label{rested_arms_eq:M_h(N(pi))/N(pi)_has_almost_correct_drift}
%	\end{equation}
%\end{lemma}
%\begin{proof}
%The proof follows as a consequence of Proposition \ref{rested_arms_prop:lim_M_h(n)/n_correct_drift} and Lemma \ref{rested_arms_lemma:stopping_time_of_policy_goes_to_infinity}.
%\end{proof}
We now state the main result of this section.
\begin{prop}\label{rested_arms_prop:upper_bound}
	Fix $\delta\in(0,1)$. Under the policy $\pi=\pi^\star(L,\delta)$ and under the arms configuration $C=(h, P_1, P_2)$,
	\begin{equation}
		\limsup\limits_{L\to\infty}\frac{E^\pi[\tau(\pi)|C]}{\log L}\leq \frac{1}{D_\delta^*(h,P_1,P_2)}.\label{rested_arms_eq:upper_bound}
	\end{equation}
\end{prop}
%\begin{proof}
%See appendix \ref{rested_arms_appndx:proof_of_upper_bound}.
%\end{proof}
{The proof uses Proposition \ref{rested_arms_prop:lim_M_h(n)/n_correct_drift} and involves showing that (a) the stopping time $\tau(\pi^\star(L, \delta))$ satisfies an asymptotic almost sure upper bound that matches with the right-hand side of \eqref{rested_arms_eq:upper_bound}, and (b) the family $\{\tau(\pi^\star(L,\delta))/\log L:L> 1\}$ is uniformly integrable. The almost sure convergence together with uniform integrability yields the relation \eqref{rested_arms_eq:upper_bound}. The details may be found in Section \ref{rested_arms_appndx:proof_of_upper_bound}.}

It is clear that $D^*_\delta(h,P_1,P_2)$ is a continuous function of $\delta$, with the property that
\begin{equation}
	\lim\limits_{\delta\downarrow 0}D_\delta^*(h,P_1,P_2)=D^*(h,P_1,P_2),\label{rested_arms_eq:L_delta^*(h,P_1,P_2)_converges_to_D^*(h,P_1,P_2)_as_delta_goes_to_0}
\end{equation}
where $D^*(h,P_1,P_2)$ on the right-hand side of \eqref{rested_arms_eq:L_delta^*(h,P_1,P_2)_converges_to_D^*(h,P_1,P_2)_as_delta_goes_to_0} is the same the constant that appears in the lower bound of \eqref{rested_arms_eq:lower_bound}. Thus, we note that $\delta$ may be tuned to make $D_\delta^*(h,P_1,P_2)$ as close as desired to $D^*(h,P_1,P_2)$, hence establishing the near-optimality of the policy $\pi^\star(L,\delta)$.

\section{The Main Result}\label{rested_arms_sec:main_result}
We now present the main result of this chapter, combining the lower and the upper bounds stated in Section \ref{rested_arms_sec:lower_bound} and Section \ref{rested_arms_sec:achievability} respectively.
\begin{theorem}
	Consider $K\geq 3$ independent Markov processes on a common finite state space that are irreducible, aperiodic and time homogeneous. Suppose that $C=(h,P_1,P_2)$ is the underlying configuration of the arms, where $h$ is the odd arm index, and $P_2\neq P_1$. Let $(\epsilon_n)_{n\geq 1}$ denote a sequence of error probability values with the property that $\epsilon_n\to 0$ as $n\to\infty$. Then, for each $n$ and $\delta\in(0,1)$, the policy $\pi^\star(L_n,\delta)$ with $L_n=1/\epsilon_n$ belongs to the family $\Pi(\epsilon_n)$. Furthermore, we have
	\begingroup\allowdisplaybreaks\begin{align}
		\liminf\limits_{n\to\infty}\inf\limits_{\pi\in\Pi(\epsilon_n)}\frac{E[\tau(\pi)|C]}{\log L_n}
		=\lim\limits_{\delta\downarrow 0}\lim\limits_{n\to\infty}\frac{E[\tau(\pi^\star(L_n,\delta))|C]}{\log L_n}=\frac{1}{D^*(h,P_1,P_2)}.
	\end{align}\endgroup
\end{theorem}
\begin{proof}
From Proposition \ref{rested_arms_prop:lower_bound}, it follows that the expected stopping time of any policy $\pi\in\Pi(\epsilon_n)$ grows as $\frac{\log L_n}{D^*(h,P_1,P_2)}$ for large values of $n$. Also, from Proposition \ref{rested_arms_prop:pi_{LRMB}(L,delta)_belongs_to_Pi(epsilon)}, policy $\pi^\star(L_n,\delta)$ belongs to the family $\Pi(\epsilon_n)$ and, from Proposition \ref{rested_arms_prop:upper_bound}, achieves an asymptotic growth of at most $(\log L_n)/D_\delta^*(h,P_1,P_2)$. Since $\lim\limits_{\delta\downarrow 0}D_\delta^*(h,P_1,P_2)=D^*(h,P_1,P_2)$, we may approach the lower bound by choosing an arbitrarily small value of $\delta$. This establishes the theorem.
\end{proof}

{While those familiar with such stopping problems may easily guess the form of $D^*(h,P_1,P_2)$, the proof is not a straighforward extension of the iid case. To re-emphasise the challenges posed by the setting of Markov observations, Wald's identity is not available for the converse and a generalisation is needed, while a forced exploration approach provides achievability.}


%\section{Numerical Results}\label{rested_arms_sec:numerical_results}
%In this section, we verify the performance of policy $pi_{LRMB}(L,\delta)$ numerically through simulations. We fix $K=8$, and consider the Markov chain of each arm to evolve on the state space $\mathcal{S}=\{0,1\}$, with the probability transition matrices $P_1$ and $P_2$ given by
%\begin{equation}
%	P_1 = \begin{bmatrix}
%		0.5&0.5\\
%		0.5&0.5
%	\end{bmatrix},\quad P_2=\begin{bmatrix}
%		0.1&0.9\\
%		0.9&0.1
%	\end{bmatrix}.\label{rested_arms_eq:P_1_and_P_2_matrices_simulations}
%\end{equation}
%Clearly, then, we have $\mu_1 = [0.5,~0.5]^T=\mu_2$. Further, we fix $h=1$ to be the index of the odd arm. Plugging these into the formula for $D^*(h,P_1,P_2)$ in \eqref{rested_arms_eq:D^*(h,P_1,P_2)}, we get
%\begin{equation}
%	\lambda_{opt}(h)=,\quad D^*(h,P_1,P_2)=.\label{rested_arms_eq:lambda^*_D^*(h,P_1,P_2)_simulations}
%\end{equation}
%
%Fig. \ref{rested_arms_fig:variation_of_upper_bound_with_delta} shows a plot of the variation of the term $L_\delta^*(h,P_1,P_2)$ given by \eqref{rested_arms_eq:L_delta^*(h,P_1,P_2)} as a function of $\delta$, when $\delta$ is varied in steps of $10^{-3}$. Also included in the same plot is the value of $D^*(h,P_1,P_2)$. According to \eqref{rested_arms_eq:P(A_{n+1}=a|A^n,\bar{X}^n)}, we note that larger values of $\delta$ result in a higher probability of choosing the arms uniformly at random, whereas the optimal probabilities with which the arms are to be selected is as per \eqref{rested_arms_eq:lambda^*(h,P_1,P_2)}. Therefore, this results in larger deviations of $L_\delta^*(h,P_1,P_2)$ from $D^*(h,P_1,P_2)$, as evident in Fig. \ref{rested_arms_fig:variation_of_upper_bound_with_delta} for $\delta\approx 1$. However, for values of $\delta$ near $0$, as per \eqref{rested_arms_eq:P(A_{n+1}=a|A^n,\bar{X}^n)}, the policy selects the arms according to $\lambda_{opt}(\theta(n),\hat{P}^n_{\theta(n),1},\hat{P}^n_{\theta(n),2})$ with high probability. From the convergence in \eqref{rested_arms_eq:lambda^*_converges_to_true_lambda^*_values}, this implies that the policy selects the arms with probabilities that are close to those given by $\lambda_{opt}(h,P_1,P_2)$, thereby resulting in $L_\delta^*(h,P_1,P_2)$ being close to $D^*(h,P_1,P_2)$ for values of $\delta$ close to 0.
%\begin{figure}[h]
%	\begin{center}
%		\includegraphics[width=\linewidth]{EPS/variation_of_L_delta}
%		\caption{Variation of $L_\delta^*(h,P_1,P_2)$ with $\delta$.}
%		\label{rested_arms_fig:variation_of_upper_bound_with_delta}
%	\end{center}
%\end{figure}
%
%In Fig. [reference for figure 2], we compare the performance of policy $\pi^\star(L,\delta)$ with that of a policy that knows the probability transition matrices $P_1$ and $P_2$.
%
%\section{Conclusions}\label{rested_arms_sec:conclusions}

\section{Simulation Results}\label{rested_arms_sec:simulation_results}
\begin{figure}
	\begin{center}
		\includegraphics[width=11cm,height=7cm]{images/Plots8ArmsAndLmax20.eps}
	\end{center}
	\caption{Plots of average stopping time of policy $\pi^{\star}(L,\delta)$, as function of $\log L$, for $\delta=0.01,0.1,0.25$.}
	\label{rested_arms_fig:plots}
\end{figure}
Fix $K=8$ and $C=(h,P_1,P_2)$, with $h=1$ and $$ P_1=\begin{bmatrix}
	0.5&0.5\\0.5&0.5
\end{bmatrix},\quad  P_2=\begin{bmatrix}
	0.1&0.9\\0.9&0.1
\end{bmatrix}.$$
Fig. \ref{rested_arms_fig:plots} depicts the average stopping time of policy $\pi^{\star}(L,\delta)$ as a function of $\log L$, averaged over $100$ rounds of iterations, for $\delta=0.01,0.1,0.25$. For the aforementioned values of $P_1$ and $P_2$, numerical evaluation yields $D^*(h,P_1,P_2) \simeq 0.094$, thus resulting in a lower bound of $1/D^*(h,P_1,P_2) \simeq 10.635$. Since \eqref{rested_arms_eq:lower_bound} is a statement about the slope of the growth rate of average stopping time of policy $\pi^{\star}(L,\delta)$ as a function of $\log L$, the top 3 plots in the figure respect the lower bound in \eqref{rested_arms_eq:lower_bound}, with the slopes in these plots only marginally higher than that given by the lower bound. Theory predicts that as $\delta \downarrow 0$ and $L\to\infty$, the slopes will approach the lower bound. Also included in the figure are the plots of (a) the lower bound for the case when $P_1$ and $P_2$ are known, and (b) a policy similar to that of $\pi^{\star}(L,\delta)$ that uses the knowledge of $P_1$ and $P_2$ to identify the index of the odd arm. Such a policy clearly takes lesser time than $\pi^\star(L,\delta)$ to identify the index of the odd arm. The figure shows that the performance of this policy also matches in slope to that given by its lower bound for large values of $L$.

\section{Proofs}\label{rested_arms_sec:proofs_of_main_results}
\subsection{Proof of Proposition \ref{rested_arms_prop:lower_bound}}\label{rested_arms_appndx:proof_of_lower_bound}
We first present below three lemmas that will be used in proving the proposition. The first of these, given below, is an analogue of the change of measure argument of Kaufmann et al. \cite[Lemma 18]{kaufmann2016complexity} for the case of Markov observations from each arm. Recall that
\begin{equation*}
	\mathcal{F}_\tau=\{E\in\mathcal{F}:E\cap \{\tau=n\}\in\mathcal{F}_n\text{ for all }n\geq 0 \},	
\end{equation*}
where for each $n$, $\mathcal{F}_n$ is as defined in \eqref{rested_arms_eq:filtration}. Further, for any $h'\neq h$, define $Z_{hh'}(\tau)\coloneqq Z_{h}(\tau)-Z_{h'}(\tau)$, where $Z_{h}(\tau)=\sum_{a=1}^{K}Z_{h}^a(\tau)$.

\begin{lemma}\label{rested_arms_lemma:ChangeOfMeasure_htoh'}
	Fix $\epsilon>0$ and probability transition matrices $P_1$ and $P_2$, and let $\tau$ be the stopping time of a policy $\pi\in\Pi(\epsilon)$. Then, for any event $E\in \mathcal{F}_{\tau}$ and configuration triplets $C=(h,P_1,P_2)$ and $C'=(h',P_1',P_2')$, with $h'\neq h$, we have
	\begin{equation}
	P^\pi(E|C')=E^\pi[\mathbb{I}_{E}\,\,\exp(-Z_{hh'}(\tau))|C].\label{rested_arms_eq:statement_of_lemma_ChangeOfMeasure_htoh'}
	\end{equation}
\end{lemma}

\begin{proof}
The proof follows the outline in \cite{kaufmann2016complexity}, with crucial modifications needed for the Markov problem at hand. We use the shorthand notations $E_{h}[\cdot]$ and $E_{h'}[\cdot]$ to denote respectively the quantities $E^\pi[\cdot|C]$ and $E^\pi[\cdot|C']$; similarly, $P_{h}(\cdot)$ and $P_{h'}(\cdot)$ denote the respective probabilities. We begin by showing that for all $n\geq 0$, the following statement is true: for any measurable function $g:\mathcal{A}^{n+1}\times\mathcal{S}^{n+1}\to \mathbb{R}$, we have
\begin{equation}
E_{h'}[g(A^n,\bar{X}^{n})]=E_{h}[g(A^n,\bar{X}^{n})\,\exp(-Z_{hh'}(n))].\label{rested_arms_eq:for_all_g_mble_lemma}
\end{equation}
Assuming that the above statement is true, for any $E\in \mathcal{F}_{\tau}$, we have
\begingroup\allowdisplaybreaks\begin{align}
P_{h'}(E)&=E_{h'}[\mathbb{I}_{E}]\nonumber\\
&\stackrel{(a)}{=}\sum\limits_{n=0}^{\infty} E_{h'}[\mathbb{I}_{E}\mathbb{I}_{\{\tau=n\}}]\nonumber\\
&\stackrel{(b)}{=}\sum\limits_{n=0}^{\infty} E_{h}[\mathbb{I}_{E}\mathbb{I}_{\{\tau=n\}}\,\exp(-Z_{hh'}(n))]\nonumber\\
&=E_{h}[\mathbb{I}_{E}\,\,\exp(-Z_{hh'}(\tau))],\label{rested_arms_eq:proof_lem_1}
\end{align}\endgroup
hence proving the lemma. In the above set of equations, $(a)$ is due to monotone convergence theorem, and $(b)$ follows from the application of \eqref{rested_arms_eq:for_all_g_mble_lemma} to the function $g(A^n,\bar{X}^{n})=\mathbb{I}_{E}\cdot\mathbb{I}_{\{\tau=n\}}$ by noting that $E\in \mathcal{F}_{\tau}$, and therefore $E\cap\{\tau=n\}\in \mathcal{F}_{n}$ for all $n$.

We now proceed to prove \eqref{rested_arms_eq:for_all_g_mble_lemma} by induction on $n$.
%Noting that for each $a\in\mathcal{A}$, $X_0^a\sim \nu$ under hypotheses $H_h$ and $H_{h'}$, we have
%\begingroup\allowdisplaybreaks\begin{align}
%	E_{h'}[g(X_0^a)]&=\sum\limits_{i\in\mathcal{S}} g(i)\nu(i)=E_{h}[g(X_0^a)].\label{rested_arms_eq:induction_n=0_case}
%\end{align}\endgroup
From \eqref{rested_arms_eq:log_likelihood_under_hyp_h_and_h'} and  \eqref{rested_arms_eq:Z_{hh'}^a(n)}, it follows that $Z_{hh'}(0)=0$. Then, for any measurable function $g:\mathcal{A}^{n+1}\times \mathcal{S}^{n+1}\to\mathbb{R}$, the proof of \eqref{rested_arms_eq:for_all_g_mble_lemma} for $n=0$ follows from the following set of equations:
\begingroup\allowdisplaybreaks\begin{align}
	E_{h'}[g(A_0,\bar{X}_0)]
	&=\sum\limits_{a=1}^{K}\sum\limits_{i\in\mathcal{S}}P_{h'}(A_0=a)\cdot P_{h'}(\bar{X}_0=i|A_0=a)\cdot g(a,i)\nonumber\\
	&=\sum\limits_{a=1}^{K}\sum\limits_{i\in\mathcal{S}}P_{h'}(A_0=a)\cdot P_{h'}(X_0^a=i)\cdot g(a,i)\nonumber\\
	&=\sum\limits_{a=1}^{K}\sum\limits_{i\in\mathcal{S}}P_{h'}(A_0=a)\cdot \nu(i)\cdot g(a,i)\nonumber\\
	&\stackrel{(a)}{=}\sum\limits_{a=1}^{K}\sum\limits_{i\in\mathcal{S}}P_{h}(A_0=a)\cdot P_h(X_0^a=i)\cdot g(a,i)\nonumber\\
	&=E_h[g(A_0,\bar{X}_0)]\nonumber\\
	&=E_h[g(A_0,\bar{X}_0)\exp(-Z_{hh'}(0))],
\end{align}\endgroup
where in writing $(a)$, we use
\begin{itemize}
\item the fact that $P_h(A_0=a)=P_{h'}(A_0=a)$ since the manner in which $A_0$ is selected is not a function of either $h$ or $h'$. For instance, we may assume that each of the arms is picked once in the first $K$ time instants, and note that this does not affect the asymptotic performance of the policy. In such a case, $P_h(A_0=1)=1=P_{h'}(A_0=1)$.
\item the fact that $X_0^a\sim \nu$ under hypotheses $H_h$ and $H_{h'}$.
\end{itemize}
 %(such an assumption is not very restrictive since the manner of selecting $A_0$ does not affect the asymptotic analysis that is of main interest to us).
%$$P_{h'}(X_0^a=i|A_0=a)=P_h(X_0^a=i|A_0=a)=\nu(i)$$ for each $i\in\mathcal{S}$. Therefore, we have $E_h[g(Z_0)]=E_{h'}[g(Z_0)]$.	

We now assume that \eqref{rested_arms_eq:for_all_g_mble_lemma} holds for some positive integer $n$, and show that it also holds for $n+1$. By the law of iterated expectations, we have
\begingroup\allowdisplaybreaks\begin{align}
&E_{h'}[g(A^{n+1},\bar{X}^{n+1})]=E_{h'}\left[E_{h'}\left[g(A^{n+1},\bar{X}^{n+1})|A^n,\bar{X}^n\right]\right].\label{rested_arms_eq:induc_hyp_appl}
\end{align}\endgroup
Since the inner conditional expectation term on the right-hand side of \eqref{rested_arms_eq:induc_hyp_appl} is a measurable function of $(A^n,\bar{X}^n)$, using the induction hypothesis, we get
\begingroup\allowdisplaybreaks\begin{align}
& E_{h'}[g(A^{n+1},\bar{X}^{n+1})]\nonumber\\
& =E_{h}\left[E_{h'}\left[g(A^{n+1},\bar{X}^{n+1})|A^n,\bar{X}^n\right]\,\exp(-Z_{hh'}(n))\right]\nonumber\\
&=\sum\limits_{a^n\in\mathcal{A}^n}\sum\limits_{\bar{x}^n\in \mathcal{S}^{n+1}}P_h(A^n=a^n,\bar{X}^n=\bar{x}^n)\cdot \exp(-z_{hh'}(n))\cdot E_{h'}[g(A^{n+1},\bar{X}^{n+1})|A^n=a^n,\bar{X}^n=\bar{x}^n]\label{rested_arms_eq:lem_1_induc_partial_1},
\end{align}\endgroup
where $z_{hh'}(n)$ denotes the value of $Z_{hh'}(n)$ when $A^n=a^n$ and $\bar{X}^n=\bar{x}^n$. Then, we have
\begingroup\allowdisplaybreaks\begin{align}
&E_{h'}[g(A^{n+1},\bar{X}^{n+1})|A^n=a^n,\bar{X}^n=\bar{x}^n]\nonumber\\
&=\sum\limits_{a'=1}^{K}\sum\limits_{j\in\mathcal{S}}g(a^n,a',\bar{x}^n,j)\cdot P_{h'}(A_{n+1}=a'|A^n=a^n,\bar{X}^n=\bar{x}^n)
\cdot P_{h'}^{a'}(X_{N_{a'}(n)}^{a'}=j|X_{N_{a'}(n)-1}^{a'})\nonumber\\
&=\sum\limits_{a'=1}^{K}\sum\limits_{j\in\mathcal{S}}g(a^n,a',\bar{x}^n,j)\cdot P_{h}(A_{n+1}=a'|A^n=a^n,\bar{X}^n=\bar{x}^n)
\cdot P_{h'}^{a'}(X_{N_{a'}(n)}^{a'}=j|X_{N_{a'}(n)-1}^{a'}),\label{rested_arms_eq:lem_1_induc_partial_2}
\end{align}\endgroup
where in writing the last line above, we use the fact that the probability of selecting an arm at any time, based on the history of past arm selections and observations, is independent of the underlying configuration of the arms, and is thus the same under hypotheses $H_h$ and $H_{h'}$. We now write \eqref{rested_arms_eq:lem_1_induc_partial_2} as
\begingroup\allowdisplaybreaks\begin{align}
&E_{h'}[g(A^{n+1},\bar{X}^{n+1})|A^n=a^n,\bar{X}^n=\bar{x}^n]\nonumber\\
&=\sum\limits_{a'=1}^{K}\sum\limits_{j\in\mathcal{S}}\bigg\lbrace g(a^n,a',\bar{x}^n,j)\cdot P_{h}(A_{n+1}=a'|A^n=a^n,\bar{X}^n=\bar{x}^n)\nonumber\\
&\hspace{4cm}
\cdot\frac{P_{h'}^{a'}(X_{N_{a'}(n)-1}^{a'}=j|X_{N_{a'}(n)-1}^{a'})}{P_{h}^{a'}(X_{N_{a'}(n)}^{a'}=j|X_{N_{a'}(n)-1}^{a'})}\cdot P_{h}^{a'}(X_{N_{a'}(n)}^{a'}=j|X_{N_{a'}(n)-1}^{a'})\bigg\rbrace.\label{rested_arms_eq:lem_1_induc_partial_3}
\end{align}\endgroup
Plugging back \eqref{rested_arms_eq:lem_1_induc_partial_3} in \eqref{rested_arms_eq:lem_1_induc_partial_1}, and using
\begin{equation}
	z_{hh'}(n+1)=z_{hh'}(n)+\log\frac{P_{h}^{a'}(X_{N_{a'}(n)}^{a'}=j|X_{N_{a'}(n)-1}^{a'})}{P_{h'}^{a'}(X_{N_{a'}(n)}^a=j|X_{N_{a'}(n)-1}^{a'})},
\end{equation}
we get
\begingroup\allowdisplaybreaks\begin{align}
&E_{h'}[g(A^{n+1},\bar{X}^{n+1})]\nonumber\\
&=\sum\limits_{a^n\in\mathcal{A}^n}\sum\limits_{\bar{x}^n\in \mathcal{S}^{n+1}}\sum\limits_{a'=1}^{K}\sum\limits_{j\in\mathcal{S}}\bigg\lbrace g(a^n,a',\bar{x}^n,j)\cdot \exp(-z_{hh'}(n+1))\nonumber\\
&\hspace{3cm}\cdot P_h(A^n=a^n,\bar{X}^n=\bar{x}^n)
\cdot P_{h}(A_{n+1}=a',\bar{X}_{n+1}=j|A^n=a^n,\bar{X}^n=\bar{x}^n)\bigg\rbrace\nonumber\\
&=E_h[g(A^{n+1},\bar{X}^{n+1})\exp(-Z_{hh'}(n+1))],
\end{align}\endgroup
hence proving \eqref{rested_arms_eq:statement_of_lemma_ChangeOfMeasure_htoh'} .
%Now,
%\begingroup\allowdisplaybreaks\begin{align}
%&E_{h'}[g(A^{n+1},\bar{X}^{n+1}|\mathcal{F}_{n},A_{n+1}=a]\exp(-Z_{hh'}(n))
%&E_{h'}[g(A^n,a,\bar{X}^{n},X_{N_a(n)}^{a})|\mathcal{F}_{n},A_{n+1}=a]\cdot \exp(-Z_{hh'}(n))
%&=\sum\limits_{j\in S} g(A^n,a,\bar{X}^{n},j)\cdot \frac{P_{h'}^{a}(X_{N_a(n)}^{a}=j|X_{N_a(n)-1}^{a})}{P_{h}^{a}(X_{N_a(n)}^{a}=j|X_{N_a(n)-1}^{a})}
%&\hspace{2cm}\cdot P_{h}^{a}(X_{N_a(n)}^{a}=j|X_{N_a(n)-1}^{a})\cdot \exp(-Z_{hh'}(n))
%&=\sum\limits_{j\in\mathcal{S}}g(A^n,a,\bar{X}^{n},j)\cdot P_{h}^{a}(X_{N_a(n)}^{a}=j|X_{N_a(n)-1}^{a})
%&\hspace{4cm}\cdot \exp(-Z_{hh'}(n+1))\label{rested_arms_eq:lem_1_induc_partial_2}
%\end{align}\endgroup
%Plugging in \eqref{rested_arms_eq:lem_1_induc_partial_2} into \eqref{rested_arms_eq:lem_1_induc_partial_1}, we get
%\begingroup\allowdisplaybreaks\begin{align}
%&E_{h'}[g(Z^{n+1})]
%&\stackrel{(a)}{=}E_{h}\bigg[\sum\limits_{a=1}^{K}\sum\limits_{j\in S}\mathbb{I}_{\{A_{n+1}=a\}}g(Z^{n},j)
%&\hspace{1cm}P_{h}^{a}(X_{N_a(n)}^a=j|X_{N_a(n)-1}^a)\exp(-Z_{hh'}^a(n+1,j))\bigg\vert\mathcal{F}_n\bigg]
%&=E_{h}\bigg[E_h[g(Z^{n+1})\,\exp\left(-Z_{hh'}(n+1)\right)|\mathcal{F}_{n}]\bigg]
%&=E_h[g(Z^{n+1})\,\exp\left(-Z_{hh'}(n+1)\right)],\label{rested_arms_eq:lem_1_induc_partial_3}
%\end{align}\endgroup
%where in $(a)$ above,
%\begingroup\allowdisplaybreaks\begin{align}
%&Z_{hh'}^a(n+1,j)
%&\coloneqq\sum\limits_{a=1}^{K}\sum\limits_{m=1}^{N_{a}(n)-1}\log\left(\frac{P_{h}^{a}(X_m^{a}|X_{m-1}^{a})}{P_{h'}^{a}(X_{m}^{a}|X_{m-1}^{a})}\right)
%
%&\hspace{2cm}+\log\left(\frac{P_{h}^{a}(X_{N_a(n)}^a=j|X_{N_a(n)-1}^a)}{P_{h'}^{a}(X_{N_a(n)}^a=j|X_{N_a(n)-1}^a)}\right).
%\end{align}\endgroup
\end{proof}

The second lemma below relates the expected number of $i$ to $j$ transitions $E^\pi[N_a(\tau,i,j)|C]$ observed on the Markov process of arm $a$ to $E^\pi[N_a(\tau,i)|C]$, the expected number of exits out of state $i$ observed on the Markov process of arm $a$.

\begin{lemma}\label{rested_arms_lemma:RelBtwNijAndNi}
	Fix $\epsilon>0$, a policy $\pi\in\Pi(\epsilon)$, and a configuration $C=(h,P_1,P_2)$. For each $i,j\in \mathcal{S}$ and $a\in \mathcal{A}$, we have
	\begin{equation}
	E^\pi[N_a(\tau,i,j)|C]=E^\pi[N_a(\tau,i)|C]\cdot P_{h}^a(j|i),\label{rested_arms_eq:RelBtwNijAndNi}
	\end{equation}
	where $P_h^a(j|i)$ is as given in \eqref{rested_arms_eq:P_h^a(j|i)}.
	\qed
\end{lemma}

\begin{proof}
We use the shorthand notation $E_h[\cdot]$ to denote $E^\pi[\cdot|C]$.
We demonstrate that for each $i,j\in \mathcal{S}$ and $a\in \mathcal{A}$,
\begingroup\allowdisplaybreaks\begin{align}
{E}_{h}[{E}_{h}[N_a(\tau,i,j)|X_0^a]|N_a(\tau)]
=E_{h}[E_{h}[N_a(\tau,i)|X_0^a]|N_a(\tau)]\cdot P_h^a(j|i).\label{rested_arms_eq:RelBtwNijAndNiWithIteratedExpec}
\end{align}\endgroup
Towards this, we note that
\begingroup\allowdisplaybreaks\begin{align}
E_{h}[E_{h}[N_a(\tau,i,j)|X_0^a]|N_a(\tau)]
=E_{h}\left[\sum\limits_{m=1}^{N_a(\tau)-1}E_{h}[\mathbb{I}_{\{X_{m-1}^a=i,\,X_m^a=j\}}|X_0^a]\bigg\vert N_a(\tau)\right].\label{rested_arms_eq:lower_bound_partial_5_rested_arms}
\end{align}\endgroup
We now simplify the inner conditional expectation term in \eqref{rested_arms_eq:lower_bound_partial_5_rested_arms} by considering the cases $m=1$ and $m\geq 2$ separately.
\begin{enumerate}
	\item Case $m=1$:
	In this case, we get
	\begingroup\allowdisplaybreaks\begin{align}
	E_{h}[\mathbb{I}_{\{X_{0}^a=i,\,X_1^a=j\}}|X_0^a]
	&=\mathbb{I}_{\{X_{0}^a=i\}}\cdot E_{h}[\mathbb{I}_{\{X_1^a=j\}}|X_0^a]\nonumber\\
	&=\mathbb{I}_{\{X_{0}^a=i\}}\cdot P_h^a(X_1^a=j|X_0^a=i)\nonumber\\
	&=\mathbb{I}_{\{X_{0}^a=i\}}\cdot P_h^a(j|i).\label{rested_arms_eq:Casem=1}
	\end{align}\endgroup
	\item Case $m\geq 2$: Here, we get
	\begingroup\allowdisplaybreaks\begin{align}
	E_{h}[\mathbb{I}_{\{X_{m-1}^a=i,\,X_m^a=j\}}|X_0^a=k]
	&=P_h^a(X_{m-1}^a=i,\,X_m^a=j|X_0^a=k)\nonumber\\
	&\stackrel{(a)}{=}P_h^a(X_{m-1}^a=i|X_0^a=k)\cdot P_h^a(X_1^a=j|X_0^a=i)\nonumber\\
	&=E_{h}[\mathbb{I}_{\{X_{m-1}^{a}=i\}}|X_0^a=k]\cdot P_h^a(j|i),
	\end{align}\endgroup
	from which it follows that $E_{h}[\mathbb{I}_{\{X_{m-1}^a=i,\,X_m^a=j\}}|X_0^a]=E_{h}[\mathbb{I}_{\{X_{m-1}^{a}=i\}}|X_0^a]\cdot P_h^a(j|i)$. In the above set of equations, $(a)$ follows from the fact that the Markov process of arm $a$ is time homogeneous.
\end{enumerate}
From the aforementioned cases, it follows that the relation
\begingroup\allowdisplaybreaks\begin{align}
E_{h}[\mathbb{I}_{\{X_{m-1}^a=i,\,X_m^a=j\}}|X_0^a]=E_{h}[\mathbb{I}_{\{X_{m-1}^{a}=i\}}|X_0^a]\cdot P_h^a(j|i)\label{rested_arms_eq:ConditioningOnX_0^a}
\end{align}\endgroup
holds for all $m\geq 1$. Substituting \eqref{rested_arms_eq:ConditioningOnX_0^a} in \eqref{rested_arms_eq:lower_bound_partial_5_rested_arms} and simplifying, we arrive at \eqref{rested_arms_eq:RelBtwNijAndNiWithIteratedExpec}. The lemma then follows by applying expectation $E_{h}[\cdot]$ to both sides of \eqref{rested_arms_eq:RelBtwNijAndNiWithIteratedExpec}.	
\end{proof}

The third lemma presented below will be used to simplify a minimisation term later in the proof of the proposition.
 \begin{lemma}
 \label{rested_arms_lemma:convex_comb_of_KL_div}
% 	Denote by $\mathcal{P}(\mathcal{S})$ the set of all probability distributions on the set $\mathcal{S}$, and let $\nu_1$ and $\nu_2$ be any two distinct elements of $\mathcal{P}(\mathcal{S})$. Then, 
	For all $w_1,w_2\in[0,1]$ such that $w_1+w_2=1$ and $\nu_1, \nu_2\in \mathcal{P}(\mathcal{S})$, 
 	\begingroup\allowdisplaybreaks\begin{align}
 		\min\limits_{\psi\in\mathcal{P}(\mathcal{S})}\left[w_1 D(\nu_1||\psi)+w_2 D(\nu_2||\psi)\right]
 		=w_1 D(\nu_1||\nu^*)+w_2 D(\nu_2||\nu^*),\label{rested_arms_eq:convex_comb_of_KL_div}
 	\end{align}\endgroup
 	where $\nu^*\in\mathcal{P}(\mathcal{S})$ is given by $\nu^*=w_1\nu_1+w_2\nu_2$.
 \end{lemma}
 \begin{proof}
 This is well known with $\nu^*$ viewed as a root of ``information centre'' and the right-hand side of \eqref{rested_arms_eq:convex_comb_of_KL_div} viewed as a mutual information. Here is the proof for completeness.

  Let $\nu^*$ be as defined in the statement of the lemma. For any $\psi\in\mathcal{P}(\mathcal{S})$, we have
{\color{black}
 \begingroup\allowdisplaybreaks\begin{align}
 w_1 D(\nu_1||\psi)+w_2 D(\nu_2||\psi)
% &=w_1E_{\nu_1}\left[\log\frac{d\nu_1}{d\psi}\right]+w_2E_{\nu_2}\left[\log\frac{d\nu_2}{d\psi}\right]\nonumber\\
% &=w_1E_{\nu_1}\left[\log\frac{d\nu_1}{d\nu^*}\right]+w_2E_{\nu_2}\left[\log\frac{d\nu_2}{d\nu^*}\right]+w_1E_{\nu_1}\left[\log\frac{d\nu^*}{d\psi}\right]+w_2E_{\nu_2}\left[\log\frac{d\nu^*}{d\psi}\right]\nonumber\\
% &=w_1E_{\nu_1}\left[\log\frac{d\nu_1}{d\nu^*}\right]+w_2E_{\nu_2}\left[\log\frac{d\nu_2}{d\nu^*}\right]+E_{\nu^*}\left[\log\frac{d\nu^*}{d\psi}\right]\nonumber\\
 &=w_1D(\nu_1\|\nu^*)+w_2 D(\nu_2\| \nu^*)+D(\nu^*||\psi)\nonumber\\
 &\geq D(\nu_1\|\nu^*)+w_2 D(\nu_2\| \nu^*),\label{rested_arms_eq:conv_comb_of_KL_div_proof}
 \end{align}\endgroup}
 with equality in the last line above if and only if $\psi=\nu^*$. This completes the proof of the lemma.	
 \end{proof}

\begin{proof}[Proof of Proposition \ref{rested_arms_prop:lower_bound}]
Fix an arbitrary $\epsilon>0$, and let $\pi\in\Pi(\epsilon)$ be a policy whose stopping is $\tau=\tau(\pi)$. Without loss of generality, we assume that $E^\pi[\tau(\pi)|C] < \infty$, for otherwise the inequality \eqref{rested_arms_eq:lower_bound} holds trivially. We organise the proof of the proposition into various sections. In the first of these sections presented below, we lower bound the expected value of $Z_{hh'}(\tau)$ in terms of the error probability $\epsilon$. This uses the above Lemma \ref{rested_arms_lemma:ChangeOfMeasure_htoh'}, Lemma \ref{rested_arms_lemma:RelBtwNijAndNi} and the result of \cite[Lemma 19]{kaufmann2016complexity}.

\subsubsection{A Lower Bound on The Expected Value of $Z_{hh'}(\tau)$}
Let $\pi\in\Pi(\epsilon)$, with stopping time is $\tau=\tau(\pi)$. For any $h'\neq h$, let $Z_{hh'}(\tau)$ be as defined in the statement of Lemma \ref{rested_arms_lemma:ChangeOfMeasure_htoh'}. Then, Lemma \ref{rested_arms_lemma:ChangeOfMeasure_htoh'} in conjunction with \cite[Lemma 19]{kaufmann2016complexity} yields the following: conditioned on the underlying configuration $C=(h,P_1,P_2)$, for any alternative configuration $C'=(h',P_1',P_2')$, where $h'\neq h$, under the assumption that $E^\pi[\tau|C]<\infty$, we have
\begin{equation}
E^\pi[Z_{hh'}(\tau)|C]\geq \sup\limits_{E\in \mathcal{F}_{\tau}}d(P^\pi(E|C),P^\pi(E|C')),\label{rested_arms_eq:analog_of_lemma_19_Kaufmann}
\end{equation}
 where \[d(p,q)\coloneqq p\log\left(\frac{p}{q}\right)+(1-p)\log\left(\frac{1-p}{1-q}\right)\]
denotes the binary KL divergence, with the convention that $d(0,0)=0=d(1,1)$. We now note the following points:
\begin{enumerate}
    \item For each alternative configuration $C'$, by taking $E=\{I(\pi)=h\}$ and recognising that $\pi\in\Pi(\epsilon)$, we have $P^\pi(E|C)>1-\epsilon$ and $P^\pi(E|C')\leq \epsilon$. Using this, along with the fact that the mapping $x\mapsto d(x,y)$ is monotone increasing for $x<y$ and the mapping $y\mapsto d(x,y)$ is monotone decreasing for any fixed $x$, we obtain
	\begingroup\allowdisplaybreaks\begin{align}
		d(P^\pi(E|C),P^\pi(E|C'))&\geq d(1-\epsilon,P^\pi(E|C'))\nonumber\\
		&\geq d(1-\epsilon,\epsilon).
	\end{align}\endgroup
	
	\item We may minimise both sides of \eqref{rested_arms_eq:analog_of_lemma_19_Kaufmann} over all alternative configurations $C'$ to obtain
	\begingroup\allowdisplaybreaks\begin{align}
		\min\limits_{C'=(h',P_1',P_2')}E^\pi[Z_{hh'}(\tau)|C]
		\geq \min\limits_{C'=(h',P_1',P_2')}\,\sup\limits_{E\in \mathcal{F}_{\tau}}d(P^\pi(E|C),P^\pi(E|C')).\label{rested_arms_eq:analog_of_lemma_19_Kaufmann_min_over_all_C'}
	\end{align}\endgroup
\end{enumerate}

Combining the points noted above, and using $d(1-\epsilon,\epsilon)=d(\epsilon,1-\epsilon)$, we obtain
\begin{equation}
\min\limits_{C'=(h',P_1',P_2')}{E}^\pi[Z_{hh'}(\tau)|C]\geq d(\epsilon,1-\epsilon).\label{rested_arms_eq:lower_bound_partial_1}
\end{equation}

\subsubsection{A Relation Between $E^\pi[Z_{hh'}(\tau)|C]$ and $E^\pi[\tau|C]$}
 As our next step, we obtain an upper bound for $E^\pi[Z_{hh'}(\tau)|C]$ in terms of $E^\pi[\tau|C]$. Towards this, we have
\begingroup\allowdisplaybreaks\begin{align}
{E}^\pi[Z_{hh'}(\tau)|C]
%\sum\limits_{a=1}^{K}{E}^\pi\bigg[\log\left(\frac{P_h^{a}(X_{0}^{a})}{P_{h'}^{a}(X_{0}^{a})}\right)\bigg\vert C\bigg]
=\sum\limits_{a=1}^{K}{E}^\pi\bigg[\sum\limits_{m=1}^{N_{a}(\tau)-1}\log\left(\frac{P_{h}^{a}(X_{m}^{a}|X_{m-1}^{a})}{P_{h'}^{a}(X_{m}^{a}|X_{m-1}^{a})}\right)\bigg\vert C\bigg],\label{rested_arms_eq:lower_bound_partial_2_rested}
\end{align}\endgroup
where we take inner summation term to be zero whenever $N_a(\tau)<2$.
Focus on the expectation term in \eqref{rested_arms_eq:lower_bound_partial_2_rested}.
%and note that for each $a\in \mathcal{A}$.
This term may be written as
\begingroup\allowdisplaybreaks\begin{align}
E^\pi\bigg[\sum\limits_{m=1}^{N_{a}(\tau)-1}\log\left(\frac{P_{h}^{a}(X_{m}^{a}|X_{m-1}^{a})}{P_{h'}^{a}(X_{m}^{a}|X_{m-1}^{a})}\right)\bigg\vert C\bigg]
%            &=E_{h}\bigg[\sum\limits_{m=1}^{N_{a}(\tau)-1}\sum\limits_{i,j\in S}\mathbb{I}_{\{X_{m-1}^a=i,\,X_m^a=j\}} \log\left(\frac{P_{h}^{a}(X_{m}^{a}=j|X_{m-1}^{a}=i)}{P_{h'}^{a}(X_{m}^{a}=j|X_{m-1}^{a}=i)}\right)\bigg]
&\stackrel{(a)}{=}E^\pi\bigg[\sum\limits_{m=1}^{N_{a}(\tau)-1}\sum\limits_{i,j\in S}\mathbb{I}_{\{X_{m-1}^a=i,\,X_m^a=j\}}\log\left(\frac{P_{h}^{a}(j|i)}{P_{h'}^{a}(j|i)}\right)\bigg\vert C\bigg]\nonumber\\
&=\sum\limits_{i,j\in S}E^\pi[N_a(\tau,i,j)|C]\,f^a_{hh'}(j|i),\label{rested_arms_eq:lower_bound_partial_3_rested_bandit}
\end{align}\endgroup
where $(a)$ above follows from the fact that the Markov process of arm $a$ is time homogeneous, and $f_{hh'}^a(j|i)\coloneqq\log\left(\frac{P_{h}^{a}(j|i)}{P_{h'}^{a}(j|i)}\right)$. Using the result of Lemma \ref{rested_arms_lemma:RelBtwNijAndNi} in \eqref{rested_arms_eq:lower_bound_partial_3_rested_bandit}, we get
\begingroup\allowdisplaybreaks\begin{align}
E^\pi[Z_{hh'}(\tau)|C]
&=\sum\limits_{a=1}^{K}\,\sum\limits_{i,j\in S}{E}^\pi[N_a(\tau,i)|C]\cdot P_{h}^a(j|i)\cdot f^a_{hh'}(j|i)\nonumber\\
&=\sum\limits_{a=1}^{K}\,\sum\limits_{i\in S}{E}[N_a(\tau,i)|C]\,D(P_h^a(\cdot|i)||P_{h'}^a(\cdot|i)),\label{rested_arms_eq:lower_bound_partial_6_rested_arms}
\end{align}\endgroup
where $D(P_h^a(\cdot|i)||P_{h'}^a(\cdot|i))=\sum\limits_{j\in S}P_{h}^a(j|i)f^a_{hh'}(j|i)$ denotes the KL divergence between the probability distributions $P_h^a(\cdot|i)$ and $P_{h'}^a(\cdot|i)$. We now express \eqref{rested_arms_eq:lower_bound_partial_6_rested_arms} by introducing some additional terms as below:
\begingroup\allowdisplaybreaks\begin{align}
&E^\pi[Z_{hh'}(\tau)|C]\nonumber\\
&=(E^\pi[\tau+1|C]-K)
\bigg(\sum\limits_{a=1}^{K}\bigg[\frac{E^\pi[N_a(\tau)|C]-1}{E^\pi[\tau+1|C]-K}\bigg]\sum\limits_{i\in \mathcal{S}}\bigg[\frac{E^\pi[N_a(\tau,i)|C]}{E^\pi[N_a(\tau)|C]-1}\bigg]
D(P_h^a(\cdot|i)||P_{h'}^a(\cdot|i))\bigg)\nonumber\\
&=(E^\pi[\tau+1|C]-K)\bigg(\sum\limits_{a=1}^{K}\bigg[\frac{E^\pi[N_a(\tau)|C]-1}{E^\pi[\tau+1|C]-K}\bigg]
\sum\limits_{i\in \mathcal{S}}p_h^a(i)\cdot D(P_h^a(\cdot|i)||P_{h'}^a(\cdot|i))\bigg),\label{rested_arms_eq:lower_bound_partial_7_rested_arms}
\end{align}\endgroup
where $p_h^a(i)\coloneqq\frac{E^\pi[N_a(\tau,i)|C]}{E^\pi[N_a(\tau)|C]-1}$ represents the average (computed with respect to $E^\pi[\cdot|C])$ fraction of times a transition {out} of state $i$ is observed on the Markov process of arm $a$.

\subsubsection{Asymptotics of Vanishing Error Probability}
Since $\sum\limits_{i\in \mathcal{S}}p_h^a(i)=1$, the inner summation term over $i$ in \eqref{rested_arms_eq:lower_bound_partial_7_rested_arms} represents the average of the numbers $(D(P_h^a(\cdot|i)||P_{h'}^a(\cdot|i)))_{i\in \mathcal{S}}$ with respect to $(p_h^a(i))_{i\in \mathcal{S}}$. Suppose that at some time, arm $a$ is selected, and it \textcolor{black}{makes a transition from state $i$ to state $j$}, for some $i,j\in \mathcal{S}$. Then, the next time arm $a$ is selected, it \textcolor{black}{makes a transition from state $j$ to state $k$} for some $k\in \mathcal{S}$. For $a\in \mathcal{A}$ and $i\in \mathcal{S}$, let
\begin{equation}
N^a(\tau,i)\coloneqq\sum\limits_{m=2}^{N_a(\tau)}\mathbb{I}_{\{X_{m-1}^a=i\}}\label{rested_arms_eq:NoOfEntriesIntoStatei}
\end{equation}
denote the number of times arm $a$ \textcolor{black}{is observed to occupy state $i$ after a transition}. In conjunction with \eqref{rested_arms_eq:N_a(n,i)}, it is easy to see that for each $i\in \mathcal{S}$, we have
\begin{equation}
N_a(\tau,i)=N^a(\tau,i)-\mathbb{I}_{\{X_{N_a(\tau)-1}^a=i\}}+\mathbb{I}_{\{X_0^a=i\}},\label{rested_arms_eq:RelBtwEntryAndExitNumbersForStatei}
\end{equation}
which implies that $N^a(\tau,i)-1\leq N_a(\tau,i)\leq N^a(\tau,i)+1$ \text{almost surely} Thus, we notice that for the Markov process of each arm, for each $i\in\mathcal{S}$, \textcolor{black}{the number of times the arm is observed to occupy state $i$ prior to a transition is at most one more than the number of times it is observed to occupy state $i$ after a transition}.  We then have
\begin{equation}
\frac{E^\pi[N^a(\tau,i)|C]-1}{E^\pi[N_a(\tau)|C]-1}\leq p_h^a(i)\leq \frac{E^\pi[N^a(\tau,i)|C]+1}{E^\pi[N_a(\tau)|C]-1}.\label{rested_arms_eq:p_h^a_in_limit}
\end{equation}

Using \eqref{rested_arms_eq:p_h^a_in_limit} in \eqref{rested_arms_eq:lower_bound_partial_7_rested_arms}, we arrive at the form
\begingroup\allowdisplaybreaks\begin{align}
	u-\Delta\leq E^\pi[Z_{hh'}(\tau)]
	\leq u+\Delta,\label{rested_arms_eq:lower_bound_partial_8}
\end{align}\endgroup
where the terms $u$ and $\Delta$ are as below:
\begingroup\allowdisplaybreaks\begin{align}
u&=(E^\pi[\tau+1|C]-K)
\bigg(\sum\limits_{a=1}^{K}\bigg[\frac{E^\pi[N_a(\tau)|C]-1}{E^\pi[\tau+1|C]-K}\bigg]
\sum\limits_{i\in \mathcal{S}}\bigg[\frac{E^\pi[N^a(\tau,i)|C]}{E^\pi[N_a(\tau)|C]-1}\bigg] D(P_h^a(\cdot|i)||P_{h'}^a(\cdot|i))\bigg),\nonumber\\
\Delta &=\sum\limits_{a=1}^{K}\sum\limits_{i\in\mathcal{S}}D(P_h^a(\cdot|i)||P_{h'}^a(\cdot|i))
=\sum\limits_{i\in\mathcal{S}}D(P_1(\cdot|i)||P_2'(\cdot|i))+\sum\limits_{i\in\mathcal{S}}D(P_2(\cdot|i)||P_1'(\cdot|i))
\nonumber\\
&\hspace{9cm}+\sum\limits_{a\neq h}\sum\limits_{i\in\mathcal{S}}D(P_2(\cdot|i)||P_2'(\cdot|i)).
\end{align}\endgroup

We shall soon show that the regime of vanishing error probabilities, i.e., $\epsilon\downarrow 0$, necessarily means that for each $a\in\mathcal{A}$, $E^\pi[N_a(\tau)|C]\to \infty$, which in turn implies that $E^\pi[\tau|C]\to\infty$. In this asymptotic regime, for each $a\in\mathcal{A}$, the limiting probabilities of \textcolor{black}{arm $a$ occupying a state $i\in \mathcal{S}$ prior to and after a transition are equal, and invariant to the one step transitions on arm $a$}. Since the Markov process of arm $a$ is irreducible and positive recurrent, its probability transition matrix admits a unique stationary distribution. Therefore, by the Ergodic theorem, the aforementioned probabilities must converge to those given by the stationary distribution associated with arm $a$. We shall denote this stationary distribution by $\mu_h^a(\cdot)$ under configuration $C=(h,P_1,P_2)$, given by
\begin{equation}
	\mu_h^a(i)=\begin{cases}
		\mu_1(i),&a=h,\\
		\mu_2(i),&a\neq h.
	\end{cases}\label{rested_arms_eq:mu_h^a}
\end{equation}
Then, as $\epsilon\downarrow 0$, we have that both the lower and upper bounds in \eqref{rested_arms_eq:p_h^a_in_limit} converge to $\mu_h^a(i)$ . We shall soon exploit this fact below to arrive at the lower bound. Going further, we denote by $(q_h^a(i))_{i\in\mathcal{S}}$ the probability distribution given by
\begin{equation}
q_h^a(i)=\frac{E^\pi[N^a(\tau,i)|C]}{E^\pi[N_a(\tau)|C]-1},\quad i\in\mathcal{S}.\label{rested_arms_eq:q_h^a}	
\end{equation}

Using the upper bound in \eqref{rested_arms_eq:lower_bound_partial_8} in combination with \eqref{rested_arms_eq:lower_bound_partial_1}, we have the following chain of inequalities:
\begingroup\allowdisplaybreaks\begin{align}
d(\epsilon,1-\epsilon)
&\leq\min\limits_{C'=(h',P_1',P_2')}\,{E}^\pi[Z_{hh'}(\tau)|C]\nonumber\\
&\leq \min\limits_{C'=(h',P_1',P_2')}(u+\Delta)\nonumber\\
&\leq \min\limits_{C'=(h',P_1',P_2')} u+\min\limits_{C'=(h',P_1',P_2')} \Delta.\label{rested_arms_eq:lower_bound_partial_9_rested_arms}
\end{align}\endgroup

The first term in \eqref{rested_arms_eq:lower_bound_partial_9_rested_arms} may be upper bounded as follows:
{\color{black}
\begingroup\allowdisplaybreaks\begin{align}
&\min\limits_{C'=(h',P_1',P_2')}u \nonumber\\
&	
%&=(E^\pi[\tau+1|C]-K)\,\bigg\lbrace\min\limits_{C'=(h',P_1',P_2')}\bigg(\sum\limits_{a=1}^{K}\bigg[\frac{E^\pi[N_a(\tau)|C]-1}{E^\pi[\tau+1|C]-K}\bigg]
%\sum\limits_{i\in \mathcal{S}}\bigg[\frac{E^\pi[N^a(\tau,i)|C]}{E^\pi[N_a(\tau)|C]-1}\bigg] D(P_h^a(\cdot|i)||P_{h'}^a(\cdot|i))\bigg)\bigg\rbrace\nonumber\\
= (E^\pi[\tau+1|C]-K)\,\bigg\lbrace
\min\limits_{C'=(h',P_1',P_2')}\bigg(\sum\limits_{a=1}^{K}\bigg[\frac{E^\pi[N_a(\tau)|C]-1}{E^\pi[\tau+1|C]-K}\bigg]\sum\limits_{i\in\mathcal{S}}q_h^a(i)\, D(P_h^a(\cdot|i)||P_{h'}^a(\cdot|i))\bigg)\bigg\rbrace\nonumber\\
&\stackrel{(a)}{=} (E^\pi[\tau+1|C]-K)\,\bigg\lbrace
\min\limits_{C'=(h',P_1',P_2')}\bigg(\sum\limits_{a=1}^{K}\bigg[\frac{E^\pi[N_a(\tau)|C]-1}{E^\pi[\tau+1|C]-K}\bigg]
 D(P_h^a(\cdot|\cdot)||P_{h'}^a(\cdot|\cdot)|q_h^a)\bigg)\bigg\rbrace\nonumber\\
&\stackrel{(b)}{\leq} (E^\pi[\tau+1|C]-K)\,\bigg\lbrace\max\limits_{\lambda\in\mathcal{P}(\mathcal{A})}\min\limits_{C'=(h',P_1',P_2')}\bigg(\sum\limits_{a=1}^{K}\lambda(a) D(P_h^a(\cdot|\cdot)||P_{h'}^a(\cdot|\cdot)|q_h^a)\bigg)\bigg\rbrace,\label{rested_arms_eq:proof_of_prop_1_temp_1}
\end{align}\endgroup}
where, in $(a)$ above,
\begin{align*}
D(P_h^a(\cdot|\cdot)||P_{h'}^a(\cdot|\cdot)|q_h^a)\coloneqq \sum\limits_{i\in S}q_h^a(i)\cdot D(P_h^a(\cdot|i)||P_{h'}^a(\cdot|i)),
\end{align*}
while $(b)$ follows by noting that maximising over the set $\mathcal{P}(\mathcal{A})$ of all probability distributions on the set of arms $\mathcal{A}$ only increases the right-hand side. The second term in \eqref{rested_arms_eq:lower_bound_partial_9_rested_arms} may be simplified as
\begingroup\allowdisplaybreaks\begin{align}
	&\min\limits_{C'=(h',P_1',P_2')}\Delta\nonumber\\
	&=\min\limits_{P_1',P_2':P_1'\neq P_2'}\bigg\lbrace \sum\limits_{i\in\mathcal{S}}D(P_1(\cdot|i)||P_2'(\cdot|i))+\sum\limits_{i\in\mathcal{S}}D(P_2(\cdot|i)||P_1'(\cdot|i))
+\sum\limits_{a\neq h}\sum\limits_{i\in\mathcal{S}}D(P_2(\cdot|i)||P_2'(\cdot|i))\bigg\rbrace\nonumber\\
&\stackrel{(a)}{=}\min\limits_{P_2'}\bigg\lbrace \sum\limits_{i\in\mathcal{S}}D(P_1(\cdot|i)||P_2'(\cdot|i))+\sum\limits_{a\neq h}\sum\limits_{i\in\mathcal{S}}D(P_2(\cdot|i)||P_2'(\cdot|i))\bigg\rbrace\nonumber\\
&=\min\bigg\lbrace\sum\limits_{i\in\mathcal{S}}D(P_1(\cdot|i)||P_2(\cdot|i)),~(K-1)\sum\limits_{i\in\mathcal{S}}D(P_2(\cdot|i)||P_1(\cdot|i))\bigg\rbrace,\label{rested_arms_eq:lower_bound_partial_10_rested_arms}
\end{align}\endgroup
where $(a)$ above follows by noting that $P_1'$ appears only in the term $D(P_2(\cdot|i)||P_1'(\cdot|i))$, and that for the choice $P_1'=P_2$, we get $D(P_2(\cdot|i)||P_1'(\cdot|i))=0$ for all $i\in\mathcal{S}$. For ease of notation, we shall denote the quantity in \eqref{rested_arms_eq:lower_bound_partial_10_rested_arms} by $\Delta'$, which we note is a constant.

Combining \eqref{rested_arms_eq:proof_of_prop_1_temp_1} with \eqref{rested_arms_eq:lower_bound_partial_9_rested_arms}, we get the following relation after rearrangement:
\begingroup\allowdisplaybreaks\begin{align}
	d(\epsilon,1-\epsilon)
	\leq \Delta' + (E^\pi[\tau+1|C]-K)\,\bigg\lbrace\max\limits_{\lambda\in\mathcal{P}(\mathcal{A})}\min\limits_{C'=(h',P_1',P_2')}\bigg[\sum\limits_{a=1}^{K}\lambda(a) D(P_h^a(\cdot|\cdot)||P_{h'}^a(\cdot|\cdot)|q_h^a)\bigg]\bigg\rbrace.\label{rested_arms_eq:proof_of_prop_1_temp_2}
\end{align}\endgroup

Since \eqref{rested_arms_eq:proof_of_prop_1_temp_2} is valid for any arbitrary choice of $\epsilon>0$ and for all $\pi\in\Pi(\epsilon)$, letting $\epsilon\downarrow 0$ and using $d(\epsilon,1-\epsilon)/\log \frac{1}{\epsilon} \to 1$ as $\epsilon\downarrow 0$, along with the fact that $q_h^a(i)\to \mu_h^a(i)$ for all $i\in\mathcal{S}$ in the regime of vanishing error probabilities, we get
 \begin{equation}
\lim\limits_{\epsilon\downarrow 0}\inf\limits_{\pi\in\Pi(\epsilon)}\frac{E^\pi[\tau(\pi)|C]}{\log\frac{1}{\epsilon}}\geq \frac{1}{D^*(h,P_1,P_2)},
\end{equation}
 	where the quantity $D^*(h,P_1,P_2)$ depends on the underlying configuration of the arms, and is given by
 	\begingroup\allowdisplaybreaks\begin{align}
 D^*(h,P_1,P_2)
=\max\limits_{\lambda\in\mathcal{P}(\mathcal{A})}\,\min\limits_{C'=(h',P_1',P_2')}\bigg(\sum\limits_{a=1}^{K}\lambda(a)\,D(P_h^a(\cdot|\cdot)||P_{h'}^a(\cdot|\cdot)|\mu_h^a)\bigg).\label{rested_arms_eq:D^*(h,P_1,P_2)}
 	\end{align}\endgroup
 	
We now show that the quantities in \eqref{rested_arms_eq:D^*(h,P_1,P_2)} and \eqref{rested_arms_eq:D^*(h,P_1,P_2)_simpl_final} are the same.
 	
\subsubsection{The Final Steps}
\label{rested_arms_subsec:final_steps}
 Using \eqref{rested_arms_eq:P_h^a(j|i)} and \eqref{rested_arms_eq:mu_h^a}, and using the shorthand notation $D(P_h^a||P_{h'}^a|\mu_h^a)$ to denote the KL divergence term inside the summation in \eqref{rested_arms_eq:D^*(h,P_1,P_2)}, we get
  \begingroup\allowdisplaybreaks\begin{align}
 		&D^*(h,P_1,P_2)\nonumber\\
 		&=\max\limits_{\lambda\in\mathcal{P}(\mathcal{A})}\,\min\limits_{h'\neq h,\,P_1',\,P_2'}\bigg(\lambda(h)\,D(P_1||P_2'|\mu_1)
 		+\lambda(h')\,D(P_2||P_1'|\mu_2)+(1-\lambda(h)-\lambda(h'))D(P_2||P_2'|\mu_2)\bigg).\label{rested_arms_eq:D^*(h,P_1,P_2)_simpl_1}
 \end{align}\endgroup
 Since $P_1'$ appears only in the second term on right-hand side of the above expression, the minimum over all $P_1'$ of the quantity $D(P_2||P_1'|\mu_2)$ is equal to zero, which is attained for $P_1'=P_2$. Thus, we have

 \begingroup\allowdisplaybreaks\begin{align}
 		D^*(h,P_1,P_2)
 		=\max\limits_{\lambda\in\mathcal{P}(\mathcal{A})}\,\min\limits_{h'\neq h,\,P_2'}\bigg(\lambda(h)\,D(P_1||P_2'|\mu_1)
 		+(1-\lambda(h)-\lambda(h'))D(P_2||P_2'|\mu_2)\bigg).\label{rested_arms_eq:D^*(h,P_1,P_2)_simpl_2}
 \end{align}\endgroup
 We now note that
 \begingroup\allowdisplaybreaks\begin{align}
 	\min\limits_{h'\neq h}(1-\lambda(h)-\lambda(h'))&=1-\lambda(h)-\max\limits_{h'\neq h}\lambda(h')\nonumber\\
 	&\stackrel{(a)}{\leq} 1-\lambda(h)-\frac{1-\lambda(h)}{K-1}\nonumber\\
 	&=(1-\lambda(h))\frac{\left(K-2\right)}{(K-1)},\label{rested_arms_eq:min_over_h'_simplified}
 \end{align}\endgroup
 where $(a)$ above follows by lower bounding the maximum of a set of numbers by their arithmetic mean. We then have
 \begingroup\allowdisplaybreaks\begin{align}
 		D^*(h,P_1,P_2)
 		=\max\limits_{0\leq\lambda(h)\leq 1}\,\min\limits_{P_2'}\bigg(\lambda(h)\,D(P_1||P_2'|\mu_1)
 		+(1-\lambda(h))\frac{(K-2)}{(K-1)}D(P_2||P_2'|\mu_2)\bigg).\label{rested_arms_eq:D^*(h,P_1,P_2)_simpl_3}
 \end{align}\endgroup
 Using Lemma \ref{rested_arms_lemma:convex_comb_of_KL_div} in \eqref{rested_arms_eq:D^*(h,P_1,P_2)_simpl_3}, and recognising that the hand side of \eqref{rested_arms_eq:D^*(h,P_1,P_2)_simpl_3} is not a function of $h$, we write
 \begingroup\allowdisplaybreaks\begin{align}
 		D^*(h,P_1,P_2)
 		=\max\limits_{0\leq\lambda_1\leq 1}\bigg(\lambda_1\,D(P_1||P|\mu_1)+(1-\lambda_1)\frac{(K-2)}{(K-1)}D(P_2||P|\mu_2)\bigg),\label{rested_arms_eq:D^*(h,P_1,P_2)_simpl_final_4}
 \end{align}\endgroup
 where $P$ is a probability transition matrix whose entry in the $i$th row and $j$th column is given by
 \begin{equation}
 	P(j|i)=\frac{\lambda_1\mu_1(i)P_1(j|i)+(1-\lambda_1)\frac{(K-2)}{(K-1)}\mu_2(i)P_2(j|i)}{\lambda_1\mu_1(i)+(1-\lambda_1)\frac{(K-2)}{(K-1)}\mu_2(i)}.\label{rested_arms_eq:P_matrix_entries}
 \end{equation}
Noting that the right-hand sides of \eqref{rested_arms_eq:D^*(h,P_1,P_2)_simpl_final_4} and \eqref{rested_arms_eq:D^*(h,P_1,P_2)_simpl_final} are identical, this completes the proof of the proposition.
\end{proof}

\subsection{Proof of Proposition \ref{rested_arms_prop:positive_drift_of_M_{hh'}(n)}}\label{rested_arms_appndx:proof_of_strictly_positive_drift_of_M_{hh'}(n)}
Let $C=(h,P_1,P_2)$ be the underlying configuration of the arms. We first show in the following lemma that under the non-stopping version of policy $\pi^{\star}(L,\delta)$, the maximum likelihood estimates $\hat{P}^n_{h,1}$ and $\hat{P}^n_{h,2}$ converge to their respective true values $P_1$ and $P_2$ \text{almost surely}.
\begin{lemma}\label{rested_arms_lemma:convergence_of_ML_estimates}
	Under the non-stopping version of the policy $\pi^{\star}(L,\delta)$ and under the arms configuration $C=(h,P_1,P_2)$, the following convergences hold \text{almost surely} as $n\to\infty$ for all $i,j\in\mathcal{S}$:
	\begingroup\allowdisplaybreaks\begin{align}
		\frac{N_a(n,i,j)}{N_a(n,i)}\longrightarrow\begin{cases}
			P_1(j|i),&a=h,\\
			P_2(j|i),&a\neq h,
		\end{cases},\qquad
		\frac{\sum\limits_{a\neq h}N_a(n,i,j)}{\sum\limits_{a\neq h}N_a(n,i)}\longrightarrow P_2(j|i).\label{rested_arms_eq:convergence_of_ml_estimates}
	\end{align}\endgroup
\end{lemma}

\begin{proof}
Fix $i,j\in\mathcal{S}$ and $a\in\mathcal{A}$. Let $S_a(n)$ denote the quantity
\begin{equation}
	S_a(n)=\sum\limits_{t=0}^{n-1}\left(\mathbb{I}_{\{A_{t+1}=a\}}-P(A_{t+1}=a|A^t,\bar{X}^t)\right),\label{rested_arms_eq:S_a(n)}
\end{equation}	
where $P(A_{t+1}=a|A^t,\bar{X}^t)$ is given by
	\begingroup\allowdisplaybreaks\begin{align}
	P(A_{t+1}=a|A^t,\bar{X}^t)
	=\frac{\delta}{K}+(1-\delta)\,\lambda^*(h^*(t),\hat{P}^t_{h^*(t),1},\hat{P}^t_{h^*(t),2})(a).\label{rested_arms_eq:P(A_{n+1}=a|Z^n,A^n)}
\end{align}\endgroup

\textcolor{black}{Letting $d^a_{t+1}=\mathbb{I}_{\{A_{t+1}=a\}}-P(A_{t+1}=a|A^t,\bar{X}^t)$, we note that $P(|d_{t+1}|\leq 2|A^t,\bar{X}^t)=1$ for all $t\geq 0$, implying that $\{d_{t}\}_{t\geq 0}$ is bounded uniformly \text{almost surely}. Since $\{d_{t+1}\}_{t\geq 0}$ is a martingale difference sequence, it follows from \cite[Th. 1.2A]{Victor1999} that for every $\epsilon>0$, there exists $c_\epsilon>0$ such that $P(\frac{S_a(n)}{n}>\epsilon)\leq e^{-nc_\epsilon}$. From this, it follows that $S_a(n)/n\to 0$ \text{almost surely}.} This implies that the following is true \text{almost surely} for sufficiently large values of $n$:
\begingroup\allowdisplaybreaks\begin{align}
	\frac{\delta}{2K}<\frac{N_a(n)-1}{n}<1+\frac{\delta}{2K}.\label{rested_arms_eq:N_a(n)_lies_between_two_quantities}
\end{align}\endgroup
Thus, we have $\liminf\limits_{n\to\infty}\frac{N_a(n)}{n}>\frac{\delta}{2K}>0$ \text{almost surely}. By the ergodic theorem, it then follows that as $n\to\infty$, the following convergences hold \text{almost surely}:
\begingroup\allowdisplaybreaks\begin{align}
    \frac{N_a(n,i)}{N_a(n)}\longrightarrow \mu_h^a(i),\quad
	\frac{N_a(n,i,j)/N_a(n)}{N_a(n,i)/N_a(n)}\longrightarrow P_h^a(j|i)\label{rested_arms_eq:first_part_convergence};
\end{align}\endgroup
here, $\mu_h^a(i)$ and $P_h^a(j|i)$ are as defined in \eqref{rested_arms_eq:mu_h^a} and \eqref{rested_arms_eq:P_h^a(j|i)} respectively.
This establishes the convergence in the first line of \eqref{rested_arms_eq:convergence_of_ml_estimates} under the assumption that $C=(h,P_1,P_2)$ is the underlying configuration of the arms.

We then note that \text{almost surely},
\begingroup\allowdisplaybreaks\begin{align}
	\frac{\sum\limits_{a\neq h}N_a(n,i,j)}{\sum\limits_{a\neq h}N_a(n,i)}&=\frac{\sum\limits_{a\neq h}\frac{N_a(n,i,j)}{N_h^a(n,i)}\frac{N_h^a(n,i)}{N_h^a(n)}\frac{N_h^a(n)}{n}}{\sum\limits_{a\neq h}\frac{N_a(n,i)}{N_h^a(n)}\frac{N_h^a(n)}{n}}\nonumber\\
	&\stackrel{n\to\infty}{\longrightarrow} P_2(j|i),
\end{align}\endgroup
where the convergence in the last line above follows from \eqref{rested_arms_eq:first_part_convergence} by noting that for $a\neq h$, when $C=(h,P_1,P_2)$ is the underlying configuration of the arms, $\mu_h^a(i)=\mu_2(i)$ and $P_h^a(j|i)=P_2(j|i)$. This establishes the convergence in the second line of \eqref{rested_arms_eq:convergence_of_ml_estimates}, thus completing the proof of the lemma.
\end{proof}

\begin{proof}[Proof of Proposition \ref{rested_arms_prop:positive_drift_of_M_{hh'}(n)}]
We now use Lemma \ref{rested_arms_lemma:convergence_of_ML_estimates} to show that \eqref{rested_arms_eq:positive_drift_of_M_{hh'}(n)} holds for any $h'\neq h$. Towards this, we show that the quantity on the right-hand side of \eqref{rested_arms_eq:M_{hh'}(n)} is strictly positive.

For any choice of $\epsilon'>0$, we have the following:
\begin{enumerate}
	\item Since $T_1$ is a constant that does not grow with $n$, we have
	\begin{equation}
		\lim\limits_{n\to\infty}\frac{T_1}{n}=0,\label{rested_arms_eq:liminf_t_1(n)/n_final}
	\end{equation}
	and therefore it follows that there exists a positive integer $M_1=M_1(\epsilon')$ such that $T_1/n\geq -\epsilon'$ for all $n\geq M_1$.
	\item From \eqref{rested_arms_eq:t_2(n)}, we have
    \begingroup\allowdisplaybreaks\begin{align}
    	\frac{T_2(n)}{n}=\frac{1}{n}\sum\limits_{i\in\mathcal{S}}\log B((N_h(n,i,j)+1)_{j\in\mathcal{S}}).\label{rested_arms_eq:liminf_t_2(n)/n_1}
    \end{align}\endgroup
    Fix $i\in\mathcal{S}$. Then, we have
    \begingroup\allowdisplaybreaks\begin{align}
    	\log B((N_h(n,i,j)+1)_{j\in\mathcal{S}})=\log E\left[\prod\limits_{j\in\mathcal{S}}X_{ij}^{N_h(n,i,j)}\right],\label{rested_arms_eq:liminf_t_2(n)/n_2}
    \end{align}\endgroup
    where the random vector $(X_{ij})_{j\in\mathcal{S}}$ follows Dirichlet distribution with parameters $\alpha_j=1$ for all $j\in\mathcal{S}$. We now write \eqref{rested_arms_eq:liminf_t_2(n)/n_2} as follows:
    \begingroup\allowdisplaybreaks\begin{align}
    	\frac{1}{N_h(n)}\log B((N_h(n,i,j)+1)_{j\in\mathcal{S}})
    	=\frac{1}{N_h(n)}\log E\left[\exp\left(N_h(n)\sum\limits_{j\in\mathcal{S}} \frac{N_h(n,i,j)}{N_h(n)}\log X_{ij}\right)\right].\label{rested_arms_eq:liminf_t_2(n)/n_3}
    \end{align}\endgroup
    When $C=(h,P_1,P_2)$ is the underlying configuration of the arms, from Lemma \ref{rested_arms_lemma:convergence_of_ML_estimates}, we have that $N_h(n,i,j)/N_h(n)$ converges \text{almost surely} as $n\to\infty$ to $\mu_1(i)P_1(j|i)$. Thus, there exists a positive integer $M_{21}=M_{21}(\epsilon')$ such that for all $n\geq M_{21}$, we have
    \begingroup\allowdisplaybreaks\begin{align}
    	&\frac{1}{N_h(n)}\log B((N_h(n,i,j)+1)_{j\in\mathcal{S}})\nonumber\\
    	&\geq \frac{1}{N_h(n)}\log E\left[\exp\left(N_h(n)\sum\limits_{j\in\mathcal{S}} (\mu_1(i)P_1(j|i)+\epsilon')\log X_{ij}\right)\right].\label{rested_arms_eq:liminf_t_2(n)/n_4}
    \end{align}\endgroup
    Noting that $N_h(n)$ converges \text{almost surely} to $+\infty$ as $n\to\infty$, by Varadhan's integral lemma \cite[Theorem 4.3.1]{AmirDembo2009}, there exists a positive integer $M_{22}=M_{22}(\epsilon')$ such that for all $n\geq M_2=\max\{M_{21},M_{22}\}$, we have
    \begingroup\allowdisplaybreaks\begin{align}
    	&\frac{1}{N_h(n)}\log B((N_h(n,i,j)+1)_{j\in\mathcal{S}})
    	\stackrel{(a)}{\geq} \sup\limits_{\{z_j\geq 0,\sum\limits_{j\in\mathcal{S}}z_j=1\}}\sum\limits_{j\in\mathcal{S}}(\mu_1(i)P_1(j|i)+\epsilon')\log z_j-\frac{\epsilon'}{|\mathcal{S}|}\nonumber\\
    	&\hspace{4cm}=\sum\limits_{j\in\mathcal{S}}(\mu_1(i)P_1(j|i)+\epsilon')\log\frac{\mu_1(i)P_1(j|i)+\epsilon'}{\mu_1(i)+\epsilon'|\mathcal{S}|}-\frac{\epsilon'}{|\mathcal{S}|},\label{rested_arms_eq:liminf_t_2(n)/n_5}
    \end{align}\endgroup
    where the supremum on the right-hand side of $(a)$ above is computed over all vectors $(z_j)_{j\in\mathcal{S}}$ such that $z_j\geq 0$ for all $j\in\mathcal{S}$, and $\sum\limits_{j\in\mathcal{S}}z_j=1$. Plugging \eqref{rested_arms_eq:liminf_t_2(n)/n_5} into \eqref{rested_arms_eq:liminf_t_2(n)/n_1}, we get
    \begingroup\allowdisplaybreaks\begin{align}
    	\frac{T_{2}(n)}{n}
    	\geq \frac{N_h(n)}{n}\bigg\lbrace\bigg[\sum\limits_{i\in\mathcal{S}}\sum\limits_{j\in\mathcal{S}}(\mu_1(i)P_1(j|i)+\epsilon')
    	\log\frac{\mu_1(i)P_1(j|i)+\epsilon'}{\mu_1(i)+\epsilon'|\mathcal{S}|}\bigg]-\epsilon'\bigg\rbrace\label{rested_arms_eq:liminf_t_2(n)/n_final}
    \end{align}\endgroup
    for all $n\geq M_2$.
    \item From \eqref{rested_arms_eq:t_3(n)}, we have
    \begingroup\allowdisplaybreaks\begin{align}
    	\frac{T_3(n)}{n}=\frac{1}{n}\sum\limits_{i\in\mathcal{S}}\log B\left(\left(\sum\limits_{a\neq h}N_a(n,i,j)+1\right)_{j\in\mathcal{S}}\right).\label{rested_arms_eq:liminf_t_3(n)/n_1}
    \end{align}\endgroup
    Using the same arguments as those used to simplify \eqref{rested_arms_eq:liminf_t_2(n)/n_1}, we obtain the following: there exists a positive integer $M_3=M_3(\epsilon')$ such that for all $n\geq M_3$, we have
    \begingroup\allowdisplaybreaks\begin{align}
    	\frac{T_{3}(n)}{n}
    	\geq \frac{\sum\limits_{a\neq h}N_a(n)}{n}\bigg\lbrace\bigg[\sum\limits_{i\in\mathcal{S}}\sum\limits_{j\in\mathcal{S}}(\mu_2(i)P_2(j|i)+\epsilon')
    	\log\frac{\mu_2(i)P_2(j|i)+\epsilon'}{\mu_2(i)+\epsilon'|\mathcal{S}|}\bigg]-\epsilon'\bigg\rbrace.\label{rested_arms_eq:liminf_t_3(n)/n_final}
    \end{align}\endgroup
    \item From \eqref{rested_arms_eq:t_4(n)}, we have
    \begingroup\allowdisplaybreaks\begin{align}
    	\frac{T_4(n)}{n}=-\frac{1}{n}\sum\limits_{i,j\in\mathcal{S}}N_{h'}(n,i,j)\log\frac{N_{h'}(n,i,j)}{N_{h'}(n,i)}\label{rested_arms_eq:liminf_t_4(n)/n_1}.
    \end{align}\endgroup
If $N_h(n,i)=0$ for some state $i\in\mathcal{S}$ (in which case it follows that $N_h(n,i,j)=0$ for all $j\in\mathcal{S}$), or if $N_h(n,i,j)=0$\footnote{This may be the case if, for instance, $P_2(j|i)=0$ for some pair of states $i,j\in\mathcal{S}$.} for some pair of states $i,j\in\mathcal{S}$, then the corresponding terms in the summation in \eqref{rested_arms_eq:liminf_t_4(n)/n_1} will be of the form $0\log \frac{0}{0}$ or $0\log 0$ respectively, which we treat as zero by convention. Thus, without loss of generality, we assume that $N_h(n,i,j)>0$ for all $i,j\in\mathcal{S}$.




    Noting that $h'\neq h$, when the underlying configuration is $C=(h,P_1,P_2)$, from Lemma \ref{rested_arms_lemma:convergence_of_ML_estimates}, we have the following almost sure convergences (as $n\to\infty$):
    \begingroup\allowdisplaybreaks\begin{align}
    	\frac{N_{h'}(n,i,j)}{n}&\to \mu_2(i)P_2(j|i),\nonumber\\
    	\frac{N_{h'}(n,i,j)}{N_{h'}(n,i)}&\to P_2(j|i).\label{rested_arms_eq:liminf_t_4(n)/n_2}
    \end{align}\endgroup
    Using these in \eqref{rested_arms_eq:liminf_t_4(n)/n_1}, we get that there exists a positive integer $M_4=M_4(\epsilon')$ such that for all $n\geq M_4$, we have
    \begingroup\allowdisplaybreaks\begin{align}
    	\frac{T_4(n)}{n}\geq \sum\limits_{i,j\in\mathcal{S}}(\mu_2(i)P_2(j|i)-\epsilon')\log\frac{1}{P_2(j|i)+\epsilon'}.\label{rested_arms_eq:liminf_t_4(n)/n_final}
    \end{align}\endgroup
    \item Lastly, we present a simplification of the term $T_5(n)/n$. From \eqref{rested_arms_eq:t_5(n)}, we have
    \begingroup\allowdisplaybreaks\begin{align}
    	\frac{T_5(n)}{n}=-\frac{1}{n}\sum\limits_{i,j\in\mathcal{S}}\sum\limits_{a\neq h'}N_{a}(n,i,j)\log\frac{\sum\limits_{a\neq h'}N_{a}(n,i,j)}{\sum\limits_{a\neq h'}N_{a}(n,i)}.\label{rested_arms_eq:liminf_t_5(n)/n_1}
    \end{align}\endgroup
    For each $n$ and each $i,j\in\mathcal{S}$, we define $P_n(j|i)$ as the following quantity:
    \begin{equation}
    	P_n(j|i)=\frac{\sum\limits_{a\neq h'}N_a(n,i,j)}{\sum\limits_{a\neq h'}N_a(n,i)}.\label{rested_arms_eq:P_n(j|i)}
    \end{equation}
    Note that $P_n=(P_n(j|i))_{i,j\in\mathcal{S}}$ constitutes a valid probability transition matrix. From Lemma \ref{rested_arms_lemma:convergence_of_ML_estimates}, under the underlying configuration $C=(h,P_1,P_2)$, we note the following almost convergences as $n\to\infty$:
    \begingroup\allowdisplaybreaks\begin{align}
    	\frac{\sum\limits_{a\neq h,h'}N_{a}(n,i,j)}{\sum\limits_{a\neq h,h'}N_{a}(n,i)}\stackrel{n\to\infty}{\longrightarrow} P_2(j|i),\quad
  	\frac{\sum\limits_{a\neq h,h'}N_{a}(n,i)}{\sum\limits_{a\neq h,h'}N_{a}(n)}\stackrel{n\to\infty}{\longrightarrow} \mu_2(i).
    \end{align}\endgroup
    The above convergences then imply that there exists a positive integer $M_5=M_5(\epsilon')$ such that for all $n\geq M_5$, we have
    \begingroup\allowdisplaybreaks\begin{align}
    	\frac{T_5(n)}{n}&\geq \frac{N_h(n)}{n}\sum\limits_{i,j\in\mathcal{S}}(\mu_1(i)P_1(j|i)-\epsilon')\log\frac{1}{P_n(j|i)}\nonumber\\
    	&\hspace{3cm}+\frac{\sum\limits_{a\neq h,h'}N_a(n)}{n}\sum\limits_{i,j\in\mathcal{S}}(\mu_2(i)P_2(j|i)-\epsilon')\log\frac{1}{P_n(j|i)}.\label{rested_arms_eq:liminf_t_5(n)/n_final}
    \end{align}\endgroup
%    inside the summation in \eqref{rested_arms_eq:M_{hh'}(n)_pos_drift_1}. For each $n$, let $\alpha_n$ and $\beta_n$ denote the sequences
%    \begin{equation}
%    	\alpha_n=\frac{N_h(n)}{n},\quad \beta_n=\frac{\sum\limits_{a\neq h,h'}N_a(n)}{n}.\label{rested_arms_eq:alpha_n_beta_n}
%    \end{equation}
%    Then, we have
%    \begingroup\allowdisplaybreaks\begin{align}
%    	&\frac{\sum\limits_{a\neq h'}N_{a}(n,i,j)}{\sum\limits_{a\neq h'}N_{a}(n,i)}=\frac{N_h(n,i,j)+\sum\limits_{a\neq h,h'}N_{a}(n,i,j)}{N_h(n,i)+\sum\limits_{a\neq h,h'}N_{a}(n,i)}
%    	&=\frac{\frac{N_h(n,i,j)}{N_h(n,i)}\frac{N_h(n,i)}{N_h(n)}\alpha_n+\frac{\sum\limits_{a\neq h,h'}N_{a}(n,i,j)}{\sum\limits_{a\neq h,h'}N_{a}(n,i)}\frac{\sum\limits_{a\neq h,h'}N_{a}(n,i)}{\sum\limits_{a\neq h,h'}N_{a}(n)}\beta_n}{\frac{N_h(n,i)}{N_h(n)}\alpha_n+\frac{\sum\limits_{a\neq h,h'}N_{a}(n,i)}{\sum\limits_{a\neq h,h'}N_{a}(n)}\beta_n}.\label{rested_arms_eq:liminf_t_5(n)_1}
%    \end{align}\endgroup
%    By lemma \ref{rested_arms_lemma:convergence_of_ML_estimates}, we note that when $C=(h,P_1,P_2)$ denotes the underlying configuration of the arms, we have the following convergences \text{almost surely}:
%    \begingroup\allowdisplaybreaks\begin{align}
%    	\frac{\sum\limits_{a\neq h,h'}N_{a}(n,i,j)}{\sum\limits_{a\neq h,h'}N_{a}(n,i)}&\stackrel{n\to\infty}{\longrightarrow} P_2(j|i),
%    	\frac{\sum\limits_{a\neq h,h'}N_{a}(n,i)}{\sum\limits_{a\neq h,h'}N_{a}(n)}&\stackrel{n\to\infty}{\longrightarrow} \mu_2(i).
%    \end{align}\endgroup
%    We use the above convergences in \eqref{rested_arms_eq:liminf_t_5(n)_1} and, after simplification,  arrive at the following inequality:
%    \begingroup\allowdisplaybreaks\begin{align}
%    	&\liminf\limits_{n\to\infty}\frac{T_5(n)}{n}
%    	&\geq \liminf\limits_{n\to\infty}\sum\limits_{i,j\in\mathcal{S}}\alpha_n\mu_1(i)P_1(j|i)\log\frac{1}{P_n(j|i)}
%    	&\hspace{1cm}+\liminf\limits_{n\to\infty}\sum\limits_{i,j\in\mathcal{S}}\beta_n\mu_2(i)P_2(j|i)\log\frac{1}{P_n(j|i)},\label{rested_arms_eq:liminf_t_5(n)_2}
%    \end{align}\endgroup
%    where the term $P_n(j|i)$ is given by
%    \begin{equation}
%    	P_n(j|i)=\frac{\alpha_n\mu_1(i)P_1(j|i)+\beta_n\mu_2(i)P_2(j|i)}{\alpha_n\mu_1(i)+\beta_n\mu_2(i)}.\label{rested_arms_eq:P_n(j|i)}
%    \end{equation}
%    We now rewrite the right-hand side of \eqref{rested_arms_eq:liminf_t_5(n)_2} as follows:
%    \begingroup\allowdisplaybreaks\begin{align}
%    &\liminf\limits_{n\to\infty}\frac{T_5(n)}{n}
%    &\geq \liminf\limits_{n\to\infty}\alpha_n\bigg\lbrace\sum\limits_{i\in\mathcal{S}}\mu_1(i)P_1(j|i)\log\frac{P_1(j|i)}{P_n(j|i)}
%    &\hspace{3cm}+\sum\limits_{i\in\mathcal{S}}\mu_1(i)P_1(j|i)\log \frac{1}{P_1(j|i)}\bigg\rbrace
%    &+ \liminf\limits_{n\to\infty}\beta_n\bigg\lbrace\sum\limits_{i\in\mathcal{S}}\mu_2(i)P_2(j|i)\log\frac{P_2(j|i)}{P_n(j|i)}
%    &\hspace{3cm}+\sum\limits_{i\in\mathcal{S}}\mu_2(i)P_2(j|i)\log \frac{1}{P_2(j|i)}\bigg\rbrace
%    &\geq \liminf\limits_{n\to\infty}\alpha_n D(P_1||P_n|\mu_1)+\liminf\limits_{n\to\infty}\beta_n D(P_2||P_n|\mu_2)	
%    &+(\lim\limits_{n\to\infty}\alpha_n) \left(\sum\limits_{i\in\mathcal{S}}\mu_1(i)H(P_1(\cdot|i))\right)
%    &+(\lim\limits_{n\to\infty}\beta_n) \left(\sum\limits_{i\in\mathcal{S}}\mu_2(i)H(P_2(\cdot|i))\right),\label{rested_arms_eq:liminf_t_5(n)_final}
%    \end{align}\endgroup
%    where $D(P_1||P_n|\mu_1)$ denotes the quantity
%    \begingroup\allowdisplaybreaks\begin{align}
%    	D(P_1||P_n|\mu_1)=\sum\limits_{i,j\in\mathcal{S}}\mu_1(i)P_1(j|i)\log\frac{P_1(j|i)}{P_n(j|i)};\label{rested_arms_eq:D(P_1||P_n|mu_1)}
%    \end{align}\endgroup
%    similarly for $D(P_2||P_n|\mu_2)$.
\end{enumerate}

Combining the results in \eqref{rested_arms_eq:liminf_t_1(n)/n_final}, \eqref{rested_arms_eq:liminf_t_2(n)/n_final}, \eqref{rested_arms_eq:liminf_t_3(n)/n_final}, \eqref{rested_arms_eq:liminf_t_4(n)/n_final} and \eqref{rested_arms_eq:liminf_t_5(n)/n_final}, we get that for all $n\geq M(\epsilon')=\max\{M_1,\dots,M_5\}$, we have
\begingroup\allowdisplaybreaks\begin{align}
	\frac{M_{hh'}(n)}{n}\geq f_{n}(\epsilon'),\label{rested_arms_eq:liminf_M_{hh'}(n)/n_1}
\end{align}\endgroup
	%
%	&+(\liminf\limits_{n\to\infty}\alpha_n-\limsup\limits_{n\to\infty}\alpha_n)\left(\sum\limits_{i\in\mathcal{S}}\mu_1(i)H(P_1(\cdot|i))\right)
%	&+(\liminf\limits_{n\to\infty}\beta_n-\limsup\limits_{n\to\infty}\beta_n)\left(\sum\limits_{i\in\mathcal{S}}\mu_2(i)H(P_2(\cdot|i))\right)
%	&\geq \liminf\limits_{n\to\infty}\alpha_n D(P_1||P_n|\mu_1)+\liminf\limits_{n\to\infty}\beta_n D(P_2||P_n|\mu_2).
where $f_n(\epsilon')$ denotes the sum of the terms of the right-hand sides of \eqref{rested_arms_eq:liminf_t_1(n)/n_final}, \eqref{rested_arms_eq:liminf_t_2(n)/n_final}, \eqref{rested_arms_eq:liminf_t_3(n)/n_final}, \eqref{rested_arms_eq:liminf_t_4(n)/n_final} and \eqref{rested_arms_eq:liminf_t_5(n)/n_final}.

We now define $f_n(0)$ as the following quantity:
\begingroup\allowdisplaybreaks\begin{align}
	f_n(0)\coloneqq\frac{N_h(n)}{n}D(P_1||P_n|\mu_1)+\frac{\sum\limits_{a\neq h,h'}N_a(n)}{n}D(P_2||P_n||\mu_2).\label{rested_arms_eq:f_n(0)}
\end{align}\endgroup
Then, by continuity, we have that for any choice of $\epsilon>0$, there exists $\epsilon'>0$ such that $f_n(\epsilon')>f_n(0)-\epsilon$ for all sufficiently large values of $n$. From \eqref{rested_arms_eq:liminf_M_{hh'}(n)/n_1}, this implies that
\begin{equation}
	\frac{M_{hh'}(n)}{n}> f_{n}(0)-\epsilon
\end{equation}
for all sufficiently large values of $n$, from which it follows that
\begin{equation}
	\liminf\limits_{n\to\infty}\left[\frac{M_{hh'}(n)}{n}-f_n(0)\right]\geq  -\epsilon.
\end{equation}
Since the above equation is true for an arbitrary choice of $\epsilon$, letting $\epsilon\downarrow 0$, we get
\begin{equation}
	\liminf\limits_{n\to\infty} \frac{M_{hh'}(n)}{n}-\limsup\limits_{n\to\infty}f_n(0)\geq  0,
\end{equation}
from which it follows that
\begingroup\allowdisplaybreaks\begin{align}
	&\liminf\limits_{n\to\infty}\frac{M_{hh'}(n)}{n}\geq  \limsup\limits_{n\to\infty}f_n(0)\nonumber\\
	&\geq \liminf\limits_{n\to\infty}f_n(0)\nonumber\\
	&\geq \liminf\limits_{n\to\infty}\bigg\lbrace\frac{N_h(n)}{n}D(P_1||P_n|\mu_1)+\frac{\sum\limits_{a\neq h,h'}N_a(n)}{n}D(P_2||P_n|\mu_2)\bigg\rbrace\nonumber\\
	&\geq \liminf\limits_{n\to\infty}\bigg\lbrace\frac{N_h(n)}{n}\,D(P_1||P_n|\mu_1)\bigg\rbrace+\liminf\limits_{n\to\infty}\bigg\lbrace\frac{\sum\limits_{a\neq{h,h'}}N_a(n)}{n}\,D(P_2||P_n|\mu_2)\bigg\rbrace
	%	&\stackrel{(a)}{\geq} \left(\liminf\limits_{n\to\infty}\frac{N_h(n)}{n}\right)\left(\liminf\limits_{n\to\infty}D(P_1||P_n|\mu_1)\right)+\left(\liminf\limits_{n\to\infty}\frac{\sum\limits_{a\neq h,h'}N_a(n)}{n}\right)\left(\liminf\limits_{n\to\infty}D(P_2||P_n|\mu_2)\right)\nonumber\\
%	&\stackrel{(b)}{\geq}\left(\frac{\delta}{2K}\right)\left(\liminf\limits_{n\to\infty}D(P_1||P_n|\mu_1)\right)+(K-2)\left(\frac{\delta}{2K}\right)\left(\liminf\limits_{n\to\infty}D(P_2||P_n|\mu_2)\right),
	\label{rested_arms_eq:liminf_M_{hh'}(n)/n_strictly_positive}
\end{align}\endgroup
We now claim that $\sup\limits_{n\geq 0}D(P_1||P_n|\mu_1)<\infty$ \text{almost surely}. Indeed, we note that
\begingroup\allowdisplaybreaks\begin{align}
	&P_n(j|i)=\frac{\sum\limits_{a\neq h'}N_a(n,i,j)}{\sum\limits_{a\neq h'}N_a(n,i)}\nonumber\\
	&\geq \frac{\sum\limits_{a\neq h'}N_a(n,i,j)}{n}\nonumber\\
	&\geq \frac{N_h(n)}{n}\cdot \frac{N_h(n,i)}{N_h(n)} \cdot \frac{N_h(n,i,j)}{N_h(n,i)}+\frac{\sum\limits_{a\neq h,h'}N_a(n)}{n} \cdot \frac{\sum\limits_{a\neq h,h'}N_a(n,i)}{\sum\limits_{a\neq h,h'}N_a(n)} \cdot \frac{\sum\limits_{a\neq h,h'}N_a(n,i,j)}{\sum\limits_{a\neq h,h'}N_a(n,i)}\nonumber\\
	&\stackrel{(a)}{\geq} \left(\frac{\delta}{2K}\right)\left(\frac{\mu_1(i)\,P_1(j|i)}{2}\right)+(K-2)\left(\frac{\delta}{2K}\right)\left(\frac{\mu_2(i)\,P_2(j|i)}{2}\right)\nonumber\\
	&\stackrel{(b)}{\geq}\left(\frac{\delta}{2K}\right)\left(\frac{\mu_1(i)P_1(j|i)+\mu_2(i)P_2(j|i)}{2}\right)\nonumber\\
	&\geq \left(\frac{\delta}{2K}\right)\left(\min\bigg\lbrace\min\limits_{i\in\mathcal{S}}\mu_1(i),\,\min\limits_{i\in\mathcal{S}}\mu_2(i)\bigg\rbrace\right)\left(\frac{P_1(j|i)+P_2(j|i)}{2}\right)~\text{\text{almost surely}}\label{rested_arms_eq:liminf_M_{hh'}(n)/n_strictly_positive_1}
\end{align}\endgroup
for all sufficiently large values of $n$, where $(a)$ follows from \eqref{rested_arms_eq:N_a(n)_lies_between_two_quantities} and Lemma \ref{rested_arms_lemma:convergence_of_ML_estimates}, and $(b)$ follows by using the fact that the number of arms $K\geq 3$. It then follows that
\begingroup\allowdisplaybreaks\begin{align} D(P_1 & ||P_n|\mu_1) \\
&=\sum\limits_{i\in\mathcal{S}}\mu_1(i)\sum\limits_{j\in\mathcal{S}}P_1(j|i)\log\frac{P_1(j|i)}{P_n(j|i)}\nonumber\\
	&\leq \sum\limits_{i,j\in\mathcal{S}}\mu_1(i)\,P_1(j|i)\log\frac{P_1(j|i)}{\frac{P_1(j|i)+P_2(j|i)}{2}}+\sum\limits_{i,j\in\mathcal{S}}\mu_1(i)P_1(j|i)\log P_1(j|i)\nonumber\\
	&\hspace{7cm}+\log\frac{1}{\left(\frac{\delta}{2K}\right)\left(\min\bigg\lbrace\min\limits_{i\in\mathcal{S}}\mu_1(i),\,\min\limits_{i\in\mathcal{S}}\mu_2(i)\bigg\rbrace\right)}\nonumber\\
	&=D\left(P_1\bigg|\bigg|\frac{P_1+P_2}{2}\bigg\vert \mu_1\right)+\sum\limits_{i\in\mathcal{S}}\mu_1(i)(-H(P_1(\cdot|i))\nonumber\\
	&\hspace{7cm}+\log\frac{1}{\left(\frac{\delta}{2K}\right)\left(\min\bigg\lbrace\min\limits_{i\in\mathcal{S}}\mu_1(i),\,\min\limits_{i\in\mathcal{S}}\mu_2(i)\bigg\rbrace\right)}\nonumber\\
	&<\infty~\text{\text{almost surely}}.\label{rested_arms_eq:liminf_M_{hh'}(n)/n_strictly_positive_2}
\end{align}\endgroup
On similar lines, it can be shown that $D(P_2||P_n|\mu_1)$ is bounded uniformly \text{almost surely} for all $n\geq 0$. Using the uniform boundedness property just proved, we may express \eqref{rested_arms_eq:liminf_M_{hh'}(n)/n_strictly_positive} as
\begingroup\allowdisplaybreaks\begin{align}
	&\liminf\limits_{n\to\infty}\frac{M_{hh'}(n)}{n}\nonumber\\
	&\geq \bigg\lbrace\liminf\limits_{n\to\infty}\frac{N_h(n)}{n}\bigg\rbrace\bigg\lbrace \liminf\limits_{n\to\infty}D(P_1||P_n|\mu_1)\bigg\rbrace+\bigg\lbrace\liminf\limits_{n\to\infty}\frac{\sum\limits_{a\neq h,h'}N_a(n)}{n}\bigg\rbrace\bigg\lbrace \liminf\limits_{n\to\infty}D(P_2||P_n|\mu_2)\bigg\rbrace\nonumber\\
	&\geq \left(\frac{\delta}{2K}\right)\left(\liminf\limits_{n\to\infty}D(P_1||P_n|\mu_1)+(K-2)\,\liminf\limits_{n\to\infty}D(P_2||P_n|\mu_2)\right)~\text{\text{almost surely}},\label{rested_arms_eq:liminf_M_{hh'}(n)/n_strictly_positive_3}
\end{align}\endgroup
where the last line follows from \eqref{rested_arms_eq:N_a(n)_lies_between_two_quantities}.

Finally, we show that the first limit infimum term in \eqref{rested_arms_eq:liminf_M_{hh'}(n)/n_strictly_positive_3} is strictly positive, and note that an exactly parallel argument may be used to show that the second limit infimum term is also strictly positive. Suppose that $\liminf\limits_{n\to\infty}D(P_1||P_n|\mu_1)=0$ \text{almost surely}. By the property that KL divergence is zero if and only if the argument probability distributions are identical, it follows that there exists a subsequence $(n_k)_{k\geq 1}$ such that $P_{n_k}(j|i)\to P_1(j|i)$ as $k\to\infty$ \text{almost surely} for all $i,j\in\mathcal{S}$. We now fix attention to this subsequence, and note that by the property that the sequences $(N_h(n_k)/n_k)_{k\geq 1}$ and $(\sum\limits_{a\neq h,h'}N_a(n_k)/n_k)_{k\geq 1}$ are bounded, there exists a further subsequence $(n_{k_l})_{l\geq 1}$ of $(n_k)_{k\geq 1}$ such that the aforementioned bounded sequences admit limits, say $\alpha$ and $\beta$ respectively. From Lemma \ref{rested_arms_lemma:convergence_of_ML_estimates}, we then have the following convergence \text{almost surely} as $l\to\infty$:
\begingroup\allowdisplaybreaks\begin{align}
	P_{n_{k_l}}(j|i)\to \frac{\alpha\,\mu_1(i)\,P_1(j|i)+\beta \,\mu_2(i)\,P_2(j|i)}{\alpha\,\mu_1(i)+\beta\,\mu_2(i)}.\label{rested_arms_eq:P_{n_{k_l}}_convergence}
\end{align}\endgroup
However, we note that the right-hand side of \eqref{rested_arms_eq:P_{n_{k_l}}_convergence} is not equal to $P_1(j|i)$ whenever $P_2(j|i)>0$, thus resulting in a contradiction. This completes the proof of the proposition.
%However, we note from Lemma \ref{rested_arms_lemma:convergence_of_ML_estimates} and \eqref{rested_arms_eq:N_a(n)_lies_between_two_quantities} that for sufficiently large values of $k$,
%\begingroup\allowdisplaybreaks\begin{align}
%	|P_{n_k}(j|i)-P_1(j|i)|&\approx \left(\frac{\sum\limits_{a\neq h,h'}N_a(n_k)}{n_k}\right)\cdot \mu_2(i)\cdot |P_1(j|i)-P_2(j|i)|\nonumber\\
%	&\geq \left(\frac{\delta}{2K}\right)\cdot \mu_2(i)\cdot |P_1(j|i)-P_2(j|i)|~\text{almost surely},\label{rested_arms_eq:P_n-P_1}
%\end{align}\endgroup
%from which we note that since $P_1\neq P_2$, there exists a pair of states $i,j\in\mathcal{S}$ such that $P_1(j|i)\neq P_2(j|i)$. Furthermore, since the Markov process of every non-odd arm is irreducible and positive recurrent (due to the state space $\mathcal{S}$ being finite), it follows that $\mu_2(i)>0$ for all $i\in\mathcal{S}$. The above facts together imply that the right-hand side of \eqref{rested_arms_eq:P_n-P_1} bounded away from zero, thereby resulting in a contradiction.
%Since the sequences $(N_h(n)/n)_{n\geq 1}$ and $(\sum\limits_{a\neq h,h'}N_a(n)/n)_{n\geq 1}$ are bounded, we may find  subsequences along which the aforementioned sequences admit limits, say $\alpha$ and $\beta$ respectively. It then follows from Lemma \ref{rested_arms_lemma:convergence_of_ML_estimates} that, as $l\to\infty$, we have
%\begingroup\allowdisplaybreaks\begin{align}
%	P_{n_{k_l}}(j|i)&\to \frac{\alpha\,\mu_1(i)\,P_1(j|i)+\beta \,\mu_2(i)\,P_2(j|i)}{\alpha\,\mu_1(i)+\beta\,\mu_2(i)}\nonumber\\
%	&\neq P_1(j|i)\text{ if }P_2(j|i)>0,
%\end{align}\endgroup
%which is a contradiction.
\end{proof}

\subsection{Proof of Proposition \ref{rested_arms_prop:pi_{LRMB}(L,delta)_belongs_to_Pi(epsilon)}}\label{rested_arms_appndx:pi_{LRMB}(L,delta)_belongs_to_Pi(epsilon)}
 The policy $\pi^{\star}(L,\delta)$ commits error if one of the following events is true:
\begin{enumerate}
	\item The policy never stops in finite time.
	\item The policy stops in finite time and declares $h'\neq h$ as the true index of the odd arm.
%	\item There exists $n$ such that $\theta(n)=h'\neq h$ and $M_{\theta(n)}\geq \log((K-1)L)$, at which point the policy stops at time $n$ and declares $h'$ as the true index of the odd arm.
\end{enumerate}
The event in item $1$ above has zero probability as a consequence of Proposition \ref{rested_arms_prop:positive_drift_of_M_{hh'}(n)}.
Thus, the probability of error of policy $\pi=\pi^{\star}(L,\delta)$, which we denote by $P^\pi_e$, may be evaluated as follows: suppose $C=(h,P_1,P_2)$ is the underlying configuration of the arms. Then,
\begingroup\allowdisplaybreaks\begin{align}
	P^\pi_e =P^\pi(I(\pi)\neq h|C)
	=P^\pi\bigg(\exists~ n\text{ and }~h'\neq h\text{ such that }
	I(\pi)=h'\text{ and } \tau(\pi)=n\bigg\vert C\bigg).\label{rested_arms_eq:P_e_partial_1}
\end{align}\endgroup
We now let
\begingroup\allowdisplaybreaks\begin{align}
	\mathcal{R}_{h'}(n)\coloneqq\{\omega:\tau(\pi)(\omega)=n,\,I(\pi)(\omega)=h'\}\label{rested_arms_eq:R_{h'}(n)}
\end{align}\endgroup
denote the set of all sample paths for which the policy stops at time $n$ and declares $h'$ as the true index of the odd arm. Clearly, the collection $\{\mathcal{R}_{h'}(n):h'\neq h,\,n\geq 0\}$ is a collection of mutually disjoint sets. Therefore, we have
\begingroup\allowdisplaybreaks\begin{align}
&  P^\pi_e =P^\pi\left(\bigcup\limits_{h'\neq h}\,\bigcup\limits_{n=0}^{\infty}\mathcal{R}_{h'}(n)\bigg\vert C\right)\nonumber\\
&= \sum\limits_{h'\neq h}\sum\limits_{n=0}^{\infty}P^\pi(\tau(\pi)=n,I(\pi)=h'|C)\nonumber\\
&= \sum\limits_{h'\neq h}\sum\limits_{n=0}^{\infty}~\int\limits_{\mathcal{R}_{h'}(n)}\,dP^\pi(\omega|C)\nonumber\\
&\stackrel{(a)}{=}\sum\limits_{h'\neq h}\sum\limits_{n=0}^{\infty}~\int\limits_{\mathcal{R}_{h'}(n)}f(A^n(\omega),\bar{X}^n(\omega)|H_h)\,\bigg[\prod\limits_{t=0}^n P_h(A_t|A^{t-1},\bar{X}^{t-1})\bigg]\,d(A^n(\omega),\bar{X}^n(\omega))\nonumber\\
&\stackrel{(b)}{\leq} \sum\limits_{h'\neq h}\sum\limits_{n=0}^{\infty}~\int\limits_{\mathcal{R}_{h'}(n)}\hat{f}(A^n(\omega),\bar{X}^n(\omega)|H_h)\bigg[\prod\limits_{t=0}^n P_h(A_t|A^{t-1},\bar{X}^{t-1})\bigg]\,d(A^n(\omega),\bar{X}^n(\omega))\nonumber\\
&\stackrel{(c)}{=}\sum\limits_{h'\neq h}\sum\limits_{n=0}^{\infty}~\bigg\lbrace\int\limits_{\mathcal{R}_{h'}(n)}e^{-M_{h'h}(n)}~{f}(A^n(\omega),\bar{X}^n(\omega)|H_{h'})\bigg[\prod\limits_{t=0}^n P_{h'}(A_t|A^{t-1},\bar{X}^{t-1})\bigg] d(A^n(\omega),\bar{X}^n(\omega))\bigg\rbrace\nonumber\\
&\leq \sum\limits_{h'\neq h}\sum\limits_{n=0}^{\infty}~\bigg\lbrace\int\limits_{\mathcal{R}_{h'}(n)}\frac{1}{(K-1)L}~dP^\pi(\omega|C')\bigg\rbrace\nonumber\\
&=\sum\limits_{h'\neq h}\frac{1}{(K-1)L}~P^\pi\left(\bigcup\limits_{n=0}^{\infty}\mathcal{R}_{h'}(n)\bigg|C'\right){\leq}~ \frac{1}{L},
\end{align}\endgroup
where in $(a)$ above, $P_h(A_t|A^{t-1},\bar{X}^{t-1})$ denotes the probability of selecting arm $A_t$ at time $t$ when the index of the odd arm is $h$, with the convention that at time $t=0$, this term represents $P_h(A_0)$; $(b)$ above follows by the definition of $\hat{f}$ in \eqref{rested_arms_eq:ml_likelihod_under_hyp_h}, and $(c)$ follows by using the fact that the probability of selecting an arm at any time $t$, based on the history of past arm selections and observations, is independent of the odd arm index, and is thus the same when the arm indexed by either $h$ or $h'$ is the odd arm. Setting $L=1/\epsilon$ gives $P_e^{\pi}\leq \epsilon$, thus proving that $\pi=\pi^{\star}(L,\delta)\in\Pi(\epsilon)$. This completes the proof of the proposition.
 \qed

 \subsection{Proof of Proposition \ref{rested_arms_prop:lim_M_h(n)/n_correct_drift}}\label{rested_arms_appndx:proof_of_prop_lim_M_h(n)/n_correct_drift}
Before we present the proof of Proposition \ref{rested_arms_prop:lim_M_h(n)/n_correct_drift}, we show that the odd arm chosen by the non-stopping version of policy $\pi^{\star}(L,\delta)$ is indeed the correct one. Further, we show that the arm selection frequencies under the same policy converge to the respective optimal values given in \eqref{rested_arms_eq:lambda^*(h,P_1,P_2)}.
\begin{prop}
	Let $C=(h,P_1,P_2)$ denote the underlying configuration of the arms. Fix $L\geq 1$ and $\delta\in(0,1)$, and consider the non-stopping version of policy $\pi^{\star}(L,\delta)$. For any $h'\neq h$ and $i,j\in\mathcal{S}$, let $P_n(j|i)$ be defined as in \eqref{rested_arms_eq:P_n(j|i)}, Then, the following convergences hold \text{almost surely} as $n\to\infty$.
	\begingroup\allowdisplaybreaks\begin{align}
	&\theta(n)\to h,\label{rested_arms_eq:h^*_converges_to_h}\\
	&\lambda_{opt}(\theta(n),\hat{P}^n_{\theta(n),1},\hat{P}^n_{\theta(n),2})\to\lambda_{opt}(h,P_1,P_2),\label{rested_arms_eq:lambda^*_converges_to_true_lambda^*_values}\\
	&\frac{N_a(n)}{n}\to \lambda^*_\delta(h,P_1,P_2)(a)\text{ for all }a\in\mathcal{A},\label{rested_arms_eq:arm_frequencies_converge}\\
	&P_{n}(j|i)\to P_\delta(j|i)\text{ for all }i,j\in\mathcal{S},\label{rested_arms_eq:P_n_converges_to_P}
	\end{align}\endgroup
where for each $a\in\mathcal{A}$ and each $i,j\in\mathcal{S}$, the quantity $\lambda_{\delta}^*(h,P_1,P_2)(a)$ and the term $P_\delta(j|i)$ in \eqref{rested_arms_eq:P_n_converges_to_P} are as defined in the statement of Proposition \ref{rested_arms_prop:lim_M_h(n)/n_correct_drift}.
\qed
\end{prop}
\begin{proof}
We already established that \eqref{rested_arms_eq:limsup_M_{h'}(n)_less_than_0} holds for all sufficiently large $n$. This establishes \eqref{rested_arms_eq:h^*_converges_to_h}, which in turn implies that
\begingroup\allowdisplaybreaks\begin{align}
\lambda_{opt}(\theta(n),\hat{P}^n_{\theta(n),1},\hat{P}^n_{\theta(n),2})\to \lambda_{opt}(h,P_1,P_2),
\end{align}\endgroup
because of the convergence of the maximum likelihood estimates shown in \eqref{rested_arms_eq:convergence_of_ml_estimates}, and the fact that $\lambda^*(h,P,Q)$ is jointly continuous in the pair $(P,Q)$, a fact that follows from Berge's Maximum Theorem \cite{Ausubel1993}. This establishes \eqref{rested_arms_eq:lambda^*_converges_to_true_lambda^*_values}.

We now proceed to show \eqref{rested_arms_eq:arm_frequencies_converge}. Towards this, we observe that from \eqref{rested_arms_eq:P(A_{n+1}=a|sigma(A^n,X^n)} and the convergence in \eqref{rested_arms_eq:lambda^*_converges_to_true_lambda^*_values}, we have
\begingroup\allowdisplaybreaks\begin{align}
	P(A_{n+1}=a|A^n,\bar{X}^n)
	&=\frac{\delta}{K}+(1-\delta)\,\lambda_{opt}(\theta(n),\hat{P}^n_{\theta(n),1},\hat{P}^n_{\theta(n),2})(a)\nonumber\\
	&\to \frac{\delta}{K}+(1-\delta)\lambda_{opt}(h,P_1,P_2)(a).
\end{align}\endgroup
We revisit the quantity $S_a(n)$ defined in \eqref{rested_arms_eq:S_a(n)}, and use the fact that $\frac{S_a(n)}{n}\to 0$ \text{almost surely} as $n\to\infty$ to obtain
\begingroup\allowdisplaybreaks\begin{align}
	\frac{N_a(n)}{n}&\to \frac{1}{n}\sum\limits_{t=0}^{n-1}P(A_{t+1}=a|A^t,\bar{X}^t)\nonumber\\
	&\to \frac{\delta}{K}+(1-\delta)\lambda_{opt}(h,P_1,P_2)(a).
\end{align}\endgroup

This establishes \eqref{rested_arms_eq:arm_frequencies_converge}.

Defining
\begin{equation}
	\alpha_n\coloneqq\frac{N_h(n)}{n},\quad \beta_n\coloneqq\frac{\sum\limits_{a\neq h,h'}N_a(n)}{n},\label{rested_arms_eq:alpha_n_beta_n}
\end{equation}
we note that the convergence in \eqref{rested_arms_eq:arm_frequencies_converge} implies in particular that
\begingroup\allowdisplaybreaks\begin{align}
	\alpha_n &\to \lambda^*_\delta(h,P_1,P_2)(h)=\frac{\delta}{K}+(1-\delta)\lambda^*=\lambda_\delta^*,\nonumber\\
	\beta_n &\to (K-2)\left(\frac{\delta}{K}+(1-\delta)\frac{1-\lambda^*}{K-1}\right)\nonumber\\
%	&=\frac{(K-2)}{(K-1)}\bigg\lbrace1-\left(\frac{\delta}{K}+(1-\delta)\lambda^*\right)\bigg\rbrace\nonumber\\
	&=\frac{(K-2)}{(K-1)}(1-\lambda_\delta^*).\label{rested_arms_eq:alpha_n_beta_n_convergence}
\end{align}\endgroup
Taking limits as $n\to\infty$ on both sides of \eqref{rested_arms_eq:P_n(j|i)}, and using the above limits for $\alpha_n$ and $\beta_n$,  we get the convergence in \eqref{rested_arms_eq:P_n_converges_to_P}, hence completing the proof of the proposition.
\end{proof}

\begin{proof}[Proof of Proposition \ref{rested_arms_prop:lim_M_h(n)/n_correct_drift}]
We recall from \eqref{rested_arms_eq:liminf_M_{hh'}(n)/n_strictly_positive} and \eqref{rested_arms_eq:f_n(0)} that
\begingroup\allowdisplaybreaks\begin{align}
	\liminf_{n\to\infty}\frac{M_{hh'}(n)}{n}
	&\geq \liminf\limits_{n\to\infty}\alpha_n D(P_1||P_n|\mu_1)+\liminf\limits_{n\to\infty}\beta_n D(P_2||P_n|\mu_2)\nonumber\\
	&=\lambda_\delta^*D(P_1||P_\delta|\mu_1)
	+\frac{(K-2)}{(K-1)}(1-\lambda_\delta^*)D(P_2||P_\delta||\mu_2),\label{rested_arms_eq:liminf_M_{hh'}(n)_D_delta}
\end{align}\endgroup
where the terms $\alpha_n$ and $\beta_n$ are as given in \eqref{rested_arms_eq:alpha_n_beta_n}.
Using Varadhan's integral lemma \cite[Theorem 4.3.1]{AmirDembo2009} to write
\begingroup\allowdisplaybreaks\begin{align}
    	\limsup\limits_{n\to\infty}\frac{1}{n}\log B((N_h(n,i,j)+1)_{j\in\mathcal{S}})
    	&\leq \limsup\limits_{n\to\infty}\frac{N_h(n)}{n}\mu_1(i)\sup\limits_{\{z_j\geq 0,\,\sum\limits_{j\in\mathcal{S}}z_j=1\}}\sum\limits_{j\in\mathcal{S}}P_1(j|i)\log z_j\nonumber\\
    	&=\lim\limits_{n\to\infty}\frac{N_h(n)}{n}\mu_1(i)(-H(P_1(\cdot|i))),\label{rested_arms_eq:limsup_t_2(n)/n_3}
    \end{align}\endgroup
    and following similar steps leading to \eqref{rested_arms_eq:liminf_t_2(n)/n_final}, we obtain
    \begingroup\allowdisplaybreaks\begin{align}
    \limsup_{n\to\infty}\frac{M_{hh'}(n)}{n}
	&\leq \lim\limits_{n\to\infty}\alpha_n D(P_1||P_n|\mu_1)+\lim\limits_{n\to\infty}\beta_n D(P_2||P_n|\mu_2)\nonumber\\
	&=\lambda_\delta^*D(P_1||P_\delta|\mu_1)
	+\frac{(K-2)}{(K-1)}(1-\lambda_\delta^*)D(P_2||P_\delta||\mu_2).\label{rested_arms_eq:limsup_M_{hh'}(n)_D_delta}
    \end{align}\endgroup
Combining \eqref{rested_arms_eq:liminf_M_{hh'}(n)_D_delta} and \eqref{rested_arms_eq:limsup_M_{hh'}(n)_D_delta}, we get the desired result.
\end{proof}

\subsection{Proof of Proposition \ref{rested_arms_prop:upper_bound}}\label{rested_arms_appndx:proof_of_upper_bound}
This section is organised as follows. We first show in Lemma \ref{rested_arms_lemma:stopping_time_of_policy_goes_to_infinity} that the stopping time of policy $\pi^\star(L,\delta)$ goes to infinity as the error probability vanishes (or as $L\to\infty$). We then exploit this to show that under policy $\pi^\star(L,\delta)$, the modified GLR statistic has the correct drift (see Lemma \ref{rested_arms_lemma:M_h(N(pi))/N(pi)_has_almost_correct_drift}). That is, we build on the result of Proposition \ref{rested_arms_prop:positive_drift_of_M_{hh'}(n)} and obtain the explicit limit for the modified GLR statistic for the regime of vanishing error probability. We then use the result of Lemma \ref{rested_arms_lemma:M_h(N(pi))/N(pi)_has_almost_correct_drift} to show in Lemma \ref{rested_arms_lemma:almost_sure_upper_bound_for_policy_pi_star} that the stopping time of policy $\pi^*(L,\delta)$ satisfies an asymptotic almost sure upper bound that matches with the right-hand side of \eqref{rested_arms_eq:upper_bound}. Finally, we establish that for any fixed $\delta\in(0,1)$, the family $\{\tau(\pi^\star(L,\delta))/\log L:L\geq 1\}$ is uniformly integrable, and as an intermediate step towards this, we establish in Lemma \ref{rested_arms_lemma:exp_bound}  an exponential upper bound for a certain probability term. Combining the almost sure limit of Lemma \ref{rested_arms_lemma:almost_sure_upper_bound_for_policy_pi_star} along with the uniform integrability result then yields the desired upper bound in \eqref{rested_arms_eq:upper_bound}.


\begin{lemma}\label{rested_arms_lemma:stopping_time_of_policy_goes_to_infinity}
	Fix $\delta\in(0,1)$. Under the policy $\pi^{\star}(L,\delta)$ and under the arms configuration $C=(h,P_1,P_2)$, we have
	\begin{equation}
		\liminf\limits_{L\to\infty}\tau(\pi^{\star}(L,\delta))=\infty\text{ \text{almost surely}}.\label{rested_arms_eq:stopping_time_of_policy_goes_to_infinity}
	\end{equation}
\end{lemma}
\begin{proof}
	Since policy $\pi=\pi^{\star}(L,\delta)$ selects each of the $K$ arms in the first $K$ slots, in order to prove the lemma, we note that it suffices to prove the following statement:
\begin{equation}
	\text{for each $m\geq K$,}\quad \lim\limits_{L\to\infty}P^\pi(\tau(\pi)\leq m|C)=0.
\end{equation}
Fix $m\geq K$, and note that
\begingroup\allowdisplaybreaks\begin{align}
	&\limsup\limits_{L\to\infty}\,P^\pi(\tau(\pi)\leq m|C)\nonumber\\
	&=\limsup\limits_{L\to\infty}\,P^\pi\bigg(\exists~K\leq n\leq m \text{ and }\tilde{h}\in\mathcal{A}
	\text{ such that }M_{\tilde{h}}(n)>\log((K-1)L)\bigg|C\bigg)\nonumber\\
	&\leq \limsup\limits_{L\to\infty}\sum\limits_{\tilde{h}\in\mathcal{A}}\sum\limits_{n=K}^{m}P^\pi(M_{\tilde{h}}(n)>\log((K-1)L)|C)\nonumber\\
	&\leq \limsup\limits_{L\to\infty}\frac{1}{\log((K-1)L)}\sum\limits_{\tilde{h}\in\mathcal{A}}\sum\limits_{n=K}^{m}E^\pi[M_{\tilde{h}}(n)|C],\label{rested_arms_eq:stop_time_goes_to_infty_1}
\end{align}\endgroup
where the first inequality above follows from the union bound, and the second inequality follows from Markov's inequality.

We now show that for each $m\in\{K,\ldots,n\}$, the expectation term inside the summation in \eqref{rested_arms_eq:stop_time_goes_to_infty_1} is finite. Towards this, we have
\begingroup\allowdisplaybreaks\begin{align}
	M_{\tilde{h}}(n)&=\log\left(\frac{f(A^n,\bar{X}^n|H_{\tilde{h}})}{\max\limits_{h'\neq \tilde{h}}\hat{f}(A^n,\bar{X}^n|H_{h'})}\right)\nonumber\\
	&\leq \log\left(\frac{\hat{f}(A^n,\bar{X}^n|H_{\tilde{h}})}{\hat{f}(A^n,\bar{X}^n|H_{h'})}\right)\text{ for all }h'\neq \tilde{h}.\label{rested_arms_eq:mod_glr_upper_bounded_by_glr}
\end{align}\endgroup
Fix an arbitrary $h'\neq \tilde{h}$. We recognise that the logarithmic term in \eqref{rested_arms_eq:mod_glr_upper_bounded_by_glr} is the classical GLR test statistic of hypothesis $H_{\tilde{h}}$ with respect to hypothesis $H_{h'}$, given by
\begingroup\allowdisplaybreaks\begin{align}
	\log\left(\frac{\hat{f}(A^n,\bar{X}^n|H_{\tilde{h}})}{\hat{f}(A^n,\bar{X}^n|H_{h'})}\right)=S_1(n)+S_2(n)+S_3(n)+S_4(n),\label{rested_arms_eq:stop_time_goes_to_infty_2}
\end{align}\endgroup
where the terms $S_1(n),\ldots,S_4(n)$ appearing in \eqref{rested_arms_eq:stop_time_goes_to_infty_2} are as below.
\begin{enumerate}
	\item The term $S_1(n)$ is given by
	\begin{equation}
		S_1(n)=\sum\limits_{i,j\in\mathcal{S}}N_{\tilde{h}}(n,i,j)\log\frac{N_{\tilde{h}}(n,i,j)}{N_{\tilde{h}}(n,i)}.\label{rested_arms_eq:S_1(n)}
	\end{equation}
	\item The term $S_2(n)$ is given by
    \begin{equation}
		S_2(n)=\sum\limits_{i,j\in\mathcal{S}}\sum\limits_{a\neq \tilde{h}}N_a(n,i,j)\log\frac{\sum\limits_{a\neq \tilde{h}}N_a(n,i,j)}{\sum\limits_{a\neq \tilde{h}}N_a(n,i)}.\label{rested_arms_eq:S_2(n)}
		\end{equation}
	\item The term $S_3(n)$ is given by
    \begin{equation}
		S_3(n)=-\sum\limits_{i,j\in\mathcal{S}}N_{h'}(n,i,j)\log\frac{N_{h'}(n,i,j)}{N_{h'}(n,i)}.\label{rested_arms_eq:S_3(n)}
	\end{equation}
	\item The term $S_4(n)$ is given by
    \begin{equation}
		S_4(n)=-\sum\limits_{i,j\in\mathcal{S}}\sum\limits_{a\neq h'}N_a(n,i,j)\log\frac{\sum\limits_{a\neq h'}N_a(n,i,j)}{\sum\limits_{a\neq h'}N_a(n,i)}.\label{rested_arms_eq:S_4(n)}
		\end{equation}
\end{enumerate}
We now obtain an \text{almost surely} upper bound for \eqref{rested_arms_eq:stop_time_goes_to_infty_2}. We recognise that $S_1(n)$ and $S_2(n)$ are non-positive, and thus upper bound each of these terms by zero. Let $$A(i)=(N_{h'}(n,i,j)/N_{h'}(n,i))_{j\in\mathcal{S}}$$ denote the probability vector corresponding to state $i$. Then, denoting the Shannon entropy of $A(i)$ by $H(A(i))$, we may express $S_3(n)$ as
\begingroup\allowdisplaybreaks\begin{align}
	S_3(n)&=(N_{h'}(n)-1)\sum\limits_{i\in\mathcal{S}}\bigg[\frac{N_{h'}(n,i)}{N_{h'}(n)-1}\bigg]H(A(i))\nonumber\\
	&\leq (N_{h'}(n)-1)~H\left(\sum\limits_{i\in\mathcal{S}}\bigg[\frac{N_{h'}(n,i)}{N_{h'}(n)-1}\bigg]A(i)\right)\nonumber\\
	&\leq N_{h'}(n) \log|\mathcal{S}|,\label{rested_arms_eq:S_3(n)_upper_bound}
\end{align}\endgroup
where the first inequality above follows from the concavity of the entropy function $H(\cdot)$, and the second inequality follows by noting that the Shannon entropy of a probability distribution on an alphabet of size $R$ is upper bounded by $\log R$. On similar lines, we get
\begingroup\allowdisplaybreaks\begin{align}
	S_4(n)&\leq \left(\sum\limits_{a\neq h'}N_a(n)\right)\log|\mathcal{S}|.\label{rested_arms_eq:S_4(n)_upper_bound}
\end{align}\endgroup
Using in \eqref{rested_arms_eq:stop_time_goes_to_infty_2} the results of \eqref{rested_arms_eq:S_3(n)_upper_bound} and \eqref{rested_arms_eq:S_4(n)_upper_bound}, along with the zero upper bound for the non-positive terms in \eqref{rested_arms_eq:S_1(n)} and \eqref{rested_arms_eq:S_2(n)} and the relation \eqref{rested_arms_eq:sum_N_a}, we get
\begingroup\allowdisplaybreaks\begin{align}
	M_{\tilde{h}}(n)\leq (n+1)\log |\mathcal{S}|\text{ \text{almost surely}},\label{rested_arms_eq:upper_bound_on_M_h(n)}
\end{align}\endgroup
from which it follows that
\begingroup\allowdisplaybreaks\begin{align}
	\limsup\limits_{L\to\infty}P^\pi(\tau(\pi)\leq m|C)
	&\leq \limsup\limits_{L\to\infty}\frac{1}{\log((K-1)L)}\sum\limits_{\tilde{h}\in\mathcal{A}}\sum\limits_{n=K}^{m}(n+1)\log |\mathcal{S}|\nonumber\\
	&=0.
\end{align}\endgroup
This completes the proof of the lemma.
\end{proof}

\begin{lemma}\label{rested_arms_lemma:M_h(N(pi))/N(pi)_has_almost_correct_drift}
	Fix $\delta\in(0,1)$. Under the policy $\pi=\pi^{\star}(L,\delta)$ and under the  arms configuration $C=(h,P_1,P_2)$, for any $h'\neq h$, we have
	\begin{equation}
		\lim\limits_{L\to\infty}\frac{M_{hh'}(\tau(\pi))}{\tau(\pi)}=D_\delta^*(h,P_1,P_2)~\text{\text{almost surely}}\label{rested_arms_eq:M_h(N(pi))/N(pi)_has_almost_correct_drift}
	\end{equation}
\end{lemma}
\begin{proof}
The proof follows as a consequence of Proposition \ref{rested_arms_prop:lim_M_h(n)/n_correct_drift} and Lemma \ref{rested_arms_lemma:stopping_time_of_policy_goes_to_infinity}.
\end{proof}

\begin{lemma}\label{rested_arms_lemma:almost_sure_upper_bound_for_policy_pi_star}
Fix $\delta\in(0,1)$. Under the arms configuration $C=(h,P_1,P_2)$, the stopping time of the policy $\pi=\pi^*(L,\delta)$ satisfies 
\begingroup\allowdisplaybreaks\begin{align}
	\limsup\limits_{L\to\infty}\, \frac{\tau(\pi)}{\log L}\leq  \frac{1}{D_\delta^*(h,P_1,P_2)}\quad \text{\text{almost surely}}.\label{rested_arms_eq:almost_sure_upper_bound_for_policy_pi_star}
\end{align}\endgroup	
\end{lemma}
\begin{proof}
We first show that for any $h'\neq h$ and $n\geq 1$, the increment $M_{hh'}(n)-M_{hh'}(n-1)$ is bounded. Fix an arbitrary $h'\neq h$, and consider the following cases.
	\begin{enumerate}
		\item Case 1: Suppose that arm $h$ is selected at time $n$. Then, noting that in the expression for $M_{hh'}(n)$, the only terms that depend on the arm index $h$ are those in \eqref{rested_arms_eq:t_2(n)} and \eqref{rested_arms_eq:t_5(n)}, we have
		    \begin{equation}
		    	M_{hh'}(n)-M_{hh'}(n-1)=\bigg[T_2(n)-T_2(n-1)\bigg] + \bigg[T_5(n)-T_5(n-1)\bigg].\label{rested_arms_eq:mod_glr_bdd_incr_1}
		    \end{equation}
		    Suppose that at time $n$, the Markov process of arm $h$ undergoes a transition from state $i$ to state $j$, where $i,j\in\mathcal{S}$ are such that $\max\{P_1(j|i),P_2(j|i)\}>0$\footnote{Otherwise, a jump from $i$ to $j$ is not observed on arm $h$.}. Then, noting that
		    \begingroup\allowdisplaybreaks\begin{align}
		    	N_a(n,i',j')&=N_a(n-1,i',j')\quad \text{for all }a\in\mathcal{A},~i'\neq i,~j'\neq j,\nonumber\\
		    	N_h(n,i,j)&=N_h(n-1,i,j)+1,\nonumber\\
		    	N_a(n,i')& =N_a(n-1,i')\quad \text{for all }a\in\mathcal{A},~i'\neq i,\nonumber\\
		    	N_h(n,i)&=N_h(n-1,i)+1,
		    \end{align}\endgroup
		    it can be shown after some simplification that
		    \begingroup\allowdisplaybreaks\begin{align}
		    	T_2(n)-T_2(n-1)&=\log\frac{B(N_h(n-1,i,j)+2,(N_h(n-1,i,j')+1)_{j'\neq j})}{B(N_h(n-1,i,j')+1)_{j'\in\mathcal{S}}}\nonumber\\
		    	&\stackrel{(a)}{=}\frac{N_h(n-1,i,j)}{\sum\limits_{j'\in\mathcal{S}}N_h(n-1,i,j')}\nonumber\\
		    	&\leq 1\quad \text{\text{almost surely}},
		    \end{align}\endgroup
		    where $(a)$ above follows by using the relation
		    \begin{equation}
		    	B(\alpha_1,\ldots,\alpha_{|\mathcal{S}|})=\left({\prod\limits_{k=1}^{|\mathcal{S}|}\Gamma(\alpha_k)}\right)\bigg/{\Gamma\left(\sum\limits_{k=1}^{|\mathcal{S}|}\alpha_k\right)}.
		    \end{equation}
		    Also, we have
		    \begingroup\allowdisplaybreaks\begin{align}
		    	T_5(n)-T_5(n-1)&=\left(\sum\limits_{a\neq h'}N_a(n-1,i,j)\right)\log\frac{\sum\limits_{a\neq h'}N_a(n-1,i,j)}{\sum\limits_{a\neq h'}N_a(n-1,i)}\nonumber\\
		    	&\hspace{3cm}-\left(1+\sum\limits_{a\neq h'}N_a(n-1,i,j)\right)\log \frac{1+\sum\limits_{a\neq h'}N_a(n-1,i,j)}{1+\sum\limits_{a\neq h'}N_a(n-1,i)}\nonumber\\
		    	&\leq \log\frac{\sum\limits_{a\neq h'}N_a(n-1,i)}{\sum\limits_{a\neq h'}N_a(n,i,j)}\nonumber\\
		    	&\to\log \frac{1}{P_\delta(j|i)}\quad \text{\text{almost surely}},
		    \end{align}\endgroup
		    where the convergence in the last line follows from \eqref{rested_arms_eq:P_n_converges_to_P}. Thus, it follows that the increment $M_{hh'}(n)-M_{hh'}(n-1)$ is bounded for all $n\geq 1$.
		    \item Case 2: Suppose that arm $h'$ is sampled at time $n$. Noting that the only terms that depend on the arm index $h'$ are those in \eqref{rested_arms_eq:t_3(n)} and \eqref{rested_arms_eq:t_4(n)},
		      the analysis for this case proceeds on the exactly same lines as that of Case 1 presented above, and is omitted.
		    \item Case 3: Suppose that arm $a'$ is sampled at time $n$, where $a'\in\mathcal{A}\setminus\{h,h'\}$. Noting that the only terms that depend on the arm index $a'$ are those in \eqref{rested_arms_eq:t_3(n)} and \eqref{rested_arms_eq:t_5(n)},
		      the analysis for this case proceeds on the exactly same lines as that of Case 1 presented above, and is omitted.
	\end{enumerate}
This establishes that the increments of the modified GLR process are bounded at all times.

Fix an arbitrary $h'\neq h$. By the definition of stopping time $\tau(\pi)$, we have that $M_{hh'}(\tau(\pi)-1)<\log ((K-1)L)$. Using this, we have
\begingroup\allowdisplaybreaks\begin{align}
	\limsup\limits_{L\to\infty}\frac{M_{hh'}(\tau(\pi))}{\log L}&\stackrel{(a)}{=}\limsup\limits_{L\to\infty}\frac{M_{hh'}(\tau(\pi)-1)}{\log L}\nonumber\\
	&\leq \limsup\limits_{L\to\infty}\frac{\log((K-1)L)}{\log L}\nonumber\\
	&=1\quad \text{almost surely},\label{rested_arms_eq:limsup_M_hh'(tau)/log_L}
\end{align}\endgroup	
where $(a)$ above is due to boundedness of the increments of the modified GLR process established above. Then, using Lemma \ref{rested_arms_lemma:M_h(N(pi))/N(pi)_has_almost_correct_drift} along with the relation \eqref{rested_arms_eq:limsup_M_hh'(tau)/log_L} yields
\begingroup\allowdisplaybreaks\begin{align}
	\limsup\limits_{L\to\infty}\frac{\tau(\pi)}{\log L}&=\limsup\limits_{L\to\infty}\bigg\lbrace\left(\frac{\tau(\pi)}{M_{hh'}(\tau(\pi))}\right)\left(\frac{M_{hh'}(\tau(\pi))}{\log L}\right)\bigg\rbrace \nonumber\\
	&=\left(\lim\limits_{L\to\infty}\frac{\tau(\pi)}{M_{hh'}(\tau(\pi))}\right)\left(\limsup\limits_{L\to\infty}\frac{M_{hh'}(\tau(\pi))}{\log L}\right)\nonumber\\
	&\leq \frac{1}{D_\delta^*(h,P_1,P_2)}\quad \text{almost surely},
\end{align}\endgroup
thus completing the proof of the lemma.
\end{proof}


\begin{proof}[Proof of Proposition \ref{rested_arms_prop:upper_bound}]
For any fixed $\delta\in(0,1)$, we now establish that under policy $\pi=\pi^\star(L,\delta)$, the family $\{\tau(\pi)/\log L:L\geq 1\}$ is uniformly integrable. In order to do so, we note that it suffices to show that
\begin{equation}
	\limsup\limits_{L\to\infty}E^\pi\bigg[\exp\bigg(\frac{\tau(\pi)}{\log L}\bigg)\bigg|C\bigg]<\infty.
\end{equation}
Towards this, let $l(L,\delta)$ denote the quantity
\begingroup\allowdisplaybreaks\begin{align}
	l(L,\delta)\coloneqq\frac{3\log((K-1)L)}{\frac{\delta}{2K}\bigg(D(P_1||P_\delta|\mu_1)+D(P_2||P_\delta|\mu_2)\bigg)}.\label{rested_arms_eq:l(L,delta)}
\end{align}\endgroup
Let $C=(h,P_1,P_2)$ be the underlying configuration of the arms. Further, let $\pi^\star_h=\pi^\star_h(L,\delta)$ denote the version of policy $\pi^{\star}(L,\delta)$ that stops only upon declaring $h$ as the index of the odd arm. Let
\begin{equation}
	u(L)\coloneqq\exp\bigg(\frac{1+l(L,\delta)}{\log L}\bigg)
\end{equation}
Clearly, we have $\tau(\pi^\star_h)\geq \tau(\pi)$ \text{almost surely}. Then,
\begingroup\allowdisplaybreaks\begin{align}
	&\limsup\limits_{L\to\infty}E^\pi\bigg[\exp\bigg(\frac{\tau(\pi)}{\log L}\bigg)\bigg|C\bigg]
	=\limsup\limits_{L\to\infty}\int\limits_{0}^{\infty}P^\pi\bigg(\frac{\tau(\pi)}{\log L}>\log x\bigg|C\bigg)\,dx\nonumber\\
	&\leq \limsup\limits_{L\to\infty}\int\limits_{0}^{\infty}P^\pi\bigg({\tau(\pi^\star_h)}\geq \lceil(\log x)({\log L})\rceil\bigg|C\bigg)\,dx\nonumber\\
	&\stackrel{(a)}{\leq} \limsup\limits_{L\to\infty}\bigg\lbrace u(L)+\int\limits_{u(L)}^{\infty}P^\pi\bigg({\tau(\pi^\star_h)}\geq \lceil(\log x)({\log L})\rceil\bigg|C\bigg)\,dx\bigg\rbrace\nonumber\\
	&\leq \exp\bigg(\frac{3}{\frac{\delta}{2K}(D(P_1||P_\delta|\mu_1)+D(P_2||P_\delta|\mu_2))}\bigg)\nonumber\\
	&\hspace{3cm}+\limsup\limits_{L\to\infty}\sum\limits_{n\geq l(L,\delta)}\exp\bigg(\frac{n+1}{\log L}\bigg)\,P^\pi(M_h(n)<\log((K-1)L)|C),\label{rested_arms_eq:uniform_integrability_1}
%	&\stackrel{(b)}{\leq} \exp\bigg(\frac{3}{\frac{\delta}{2K}(D(P_1||P_\delta|\mu_1)+D(P_2||P_\delta|\mu_2))}\bigg)
%	+\limsup\limits_{L\to\infty}\sum\limits_{n\geq l(L,\delta)}B\exp\bigg(\frac{n+1}{\log L}-n \theta\bigg)\nonumber\\
%	&<\infty,
\end{align}\endgroup
where $(a)$ above follows by upper bounding the probability term by $1$ for all $x\leq u(L)$.

We now show that for all $n\geq l(L,\delta)$, the probability term in \eqref{rested_arms_eq:uniform_integrability_1} decays exponentially in $n$. This is a strengthening of the result in Proposition \ref{rested_arms_prop:positive_drift_of_M_{hh'}(n)} which only establishes that when $C=(h,P_1,P_2)$ is the underlying configuration of the arms, $M_h(n)\to\infty$ as $n\to\infty$.

\begin{lemma}\label{rested_arms_lemma:exp_bound}
	Let $C=(h,P_1,P_2)$ denote the underlying configuration of the arms. Fix $L\geq 1$, $\delta\in(0,1)$, and consider the policy $\pi=\pi^{\star}(L,\delta)$. There exist constants $\theta>0$ and $0<B<\infty$ independent of $L$ such that for all sufficiently large values of $n$, we have
	\begin{equation}
		P^\pi(M_h(n)<\log((K-1)L)|C)\leq Be^{-\theta n}.\label{rested_arms_eq:exponential_bound}
	\end{equation}
\end{lemma}
\begin{proof}
Since
\begingroup\allowdisplaybreaks\begin{align}
	P^\pi(M_h(n)<\log((K-1)L)|C)
	&=P^\pi\left(\min\limits_{h'\neq h}M_{hh'}(n)<\log((K-1)L)\bigg|C\right)\nonumber\\
	&\leq \sum\limits_{h'\neq h}P^\pi\left(M_{hh'}(n)<\log((K-1)L)\bigg|C\right),\label{rested_arms_eq:exp_bound_1}
\end{align}\endgroup
in order to prove the lemma, it suffices to show that each term inside the summation in \eqref{rested_arms_eq:exp_bound_1} is exponentially bounded. Going further, we drop the superscript $\pi$ and the conditioning on configuration $C$ in $P^\pi(\cdot|C)$ for ease of notation. For all $i,j\in\mathcal{S}$, let
\begingroup\allowdisplaybreaks\begin{align}
	\tilde{P}_{n}(j|i)\coloneqq \frac{\alpha_n \mu_1(i) P_1(j|i)+\beta_n \mu_2(i)P_2(j|i)}{\alpha_n \mu_1(i)+\beta_n \mu_2(i)},\label{rested_arms_eq:tilde_P_n}
\end{align}\endgroup
where $\alpha_n$ and $\beta_n$ are as in \eqref{rested_arms_eq:alpha_n_beta_n}. Fix $h'\neq h$ and $\epsilon>0$ arbitrarily. Then, using \eqref{rested_arms_eq:M_{hh'}(n)} and triangle inequality, we have
\begingroup\allowdisplaybreaks\begin{align}
	P(M_{hh'}(n)<\log((K-1)L))
	% &=P(T_1+T_2(n)+T_3(n)+T_4(n)+T_5(n)<\log((K-1)L))
	&\leq U_1+U_2+U_3+U_4+U_5+U_6+U_7,\label{rested_arms_eq:P(M_{hh'}(n)<log((K-1)L))}
\end{align}\endgroup
where the terms $U_1,\ldots,U_7$ in \eqref{rested_arms_eq:P(M_{hh'}(n)<log((K-1)L))} are as below.
\begin{enumerate}
	\item The term $U_1$ is given by
	\begin{equation}
		U_1=P\left(\frac{T_1(n)}{n}<-\epsilon\right)\label{rested_arms_eq:exp_bound_t_1/n},
	\end{equation}
	where $T_1$ is given by \eqref{rested_arms_eq:t_1}.
	\item The term $U_2$ is given by
	\begin{equation}
		U_2=P\left(\frac{T_2(n)}{n}-\frac{N_h(n)}{n}\sum\limits_{i\in\mathcal{S}}\mu_1(i)(-H(P_1(\cdot|i)))<-\epsilon\right)\label{rested_arms_eq:exp_bound_t_2(n)/n},
	\end{equation}
	where $T_2(n)$ is given by \eqref{rested_arms_eq:t_2(n)}.
    \item The term $U_3$ is given by
    \begin{equation}
    	U_3=P\left(\frac{T_3(n)}{n}-\frac{\sum\limits_{a\neq h}N_a(n)}{n}\sum\limits_{i\in\mathcal{S}}\mu_2(i)(-H(P_2(\cdot|i)))<-\epsilon\right)\label{rested_arms_eq:exp_bound_t_3(n)/n},
    \end{equation}
    where $T_3(n)$ is given by \eqref{rested_arms_eq:t_3(n)}.
    \item The term $U_4$ is given by
    \begin{equation}
    	U_4=P\left(\frac{T_4(n)}{n}-\frac{N_{h'}(n)}{n}\sum\limits_{i\in\mathcal{S}}\mu_2(i)H(P_2(\cdot|i))<-\epsilon\right)\label{rested_arms_eq:exp_bound_t_4(n)/n},
    \end{equation}
    where $T_4(n)$ is given by \eqref{rested_arms_eq:t_4(n)}.
    \item The term $U_5$ is given by
    \begin{equation}
    	U_5=P\left(\frac{T_5(n)}{n}-\sum\limits_{i\in\mathcal{S}}(\alpha_n\mu_1(i)+\beta_n\mu_2(i))H(\tilde{P}_n(\cdot|i))<-\epsilon\right)\label{rested_arms_eq:exp_bound_t_5(n)/n},
    \end{equation}
    where $T_5(n)$ is given by \eqref{rested_arms_eq:t_5(n)}.
    \item The term $U_6$ is given by
    \begin{equation}
    	U_6=P\bigg(\alpha_n \bigg[D(P_1||\tilde{P}_n|\mu_1)-D(P_1||P_\delta|\mu_1)\bigg]+\beta_n \bigg[D(P_2||\tilde{P}_n|\mu_2)-D(P_2||P_\delta|\mu_2)\bigg]<-\epsilon\bigg),\label{rested_arms_eq:exp_bound_t_6}
    \end{equation}
    where $P_\delta$ is the probability transition matrix described in the statement of Proposition \ref{rested_arms_prop:lim_M_h(n)/n_correct_drift}.
    \item The term $U_7$ is given by
    \begin{equation}
    	U_7=P\bigg(\alpha_n D(P_1||P_\delta|\mu_1)+\beta_n D(P_2||P_\delta|\mu_2)-6\epsilon
	<\frac{\log((K-1)L)}{n}\bigg).\label{rested_arms_eq:exp_bound_compensatory_term}
    \end{equation}
\end{enumerate}
In \eqref{rested_arms_eq:exp_bound_t_2(n)/n}, the term $H(P_1(\cdot|i))$ refers to the Shannon entropy of the probability distribution $(P_1(j|i))_{j\in\mathcal{S}}$ on set $\mathcal{S}$;
the terms $H(P_2(\cdot|i))$ and $H(\tilde{P}_n(\cdot|i))$ are defined similarly.

We now obtain a bound for the terms in \eqref{rested_arms_eq:exp_bound_t_1/n}-\eqref{rested_arms_eq:exp_bound_compensatory_term}.

\begin{enumerate}
\item We begin by showing an exponential upper bound for \eqref{rested_arms_eq:exp_bound_compensatory_term}. We choose $0<\epsilon'<\frac{2}{3}$, and then select $\epsilon>0$ such that the following holds:
\begingroup\allowdisplaybreaks\begin{align}
	\frac{\delta}{2K}(1-\epsilon')\bigg(D(P_1||P_\delta|\mu_1)+D(P_2||P_\delta|\mu_2)\bigg)-6\epsilon
	>\frac{1}{3}\cdot\frac{\delta}{2K}\bigg(D(P_1||P_\delta|\mu_1)+D(P_2||P_\delta|\mu_2)\bigg).\label{rested_arms_eq:select_epsilon'}
\end{align}\endgroup
Then, for all $n\geq l(L,\delta)$,
we have
\begingroup\allowdisplaybreaks\begin{align}
	&P\bigg(\alpha_n D(P_1||P_\delta|\mu_1)+\beta_n D(P_2||P_\delta|\mu_2)-6\epsilon
	<\frac{\log((K-1)L)}{n},\nonumber\\
	&\hspace{6cm}\frac{N_a(n)}{n}>\frac{\delta}{2K}(1-\epsilon')\text{ for all }a\in\mathcal{A}\bigg)=0.\label{rested_arms_eq:exp_bound_compensatory_term_first_part_equal_zero}
\end{align}\endgroup
Writing the probability term in \eqref{rested_arms_eq:exp_bound_compensatory_term} as a sum of the probability term in \eqref{rested_arms_eq:exp_bound_compensatory_term_first_part_equal_zero} and a second probability term given by
\begin{align}
	& P\bigg(\alpha_n D(P_1||P_\delta|\mu_1)+\beta_n D(P_2||P_\delta|\mu_2)-6\epsilon
	<\frac{\log((K-1)L)}{n},\nonumber\\
	&\hspace{6cm}\frac{N_a(n)}{n}\leq\frac{\delta}{2K}(1-\epsilon')\text{ for some }a\in\mathcal{A}\bigg),\label{rested_arms_eq:exp_bound_compensatory_term_second_part}
\end{align}
and upper bounding \eqref{rested_arms_eq:exp_bound_compensatory_term_second_part} by $P(N_a(n)/n~\leq (\delta/2K)(1-\epsilon')\text{ for some }a\in\mathcal{A})$, an application of the union bound yields
\begingroup\allowdisplaybreaks\begin{align}
	&P\bigg(\alpha_n D(P_1||P_\delta|\mu_1)+\beta_n D(P_2||P_\delta|\mu_2)-6\epsilon
	<\frac{\log((K-1)L)}{n}\bigg)\nonumber\\
	&\leq \sum\limits_{a=1}^{K} P\left(\frac{N_a(n)}{n}\leq\frac{\delta}{2K}(1-\epsilon')\right).\label{rested_arms_eq:exp_bound_correction_term_temp_1}
\end{align}\endgroup	
Noting that for each $a\in\mathcal{A}$, the sequence $\left(N_a(n)-n\frac{\delta}{2K}\right)_{n\geq 0}$ is a submartingale, with the absolute value of the difference between any two successive terms of the submartingale sequence being of value at most $1$, we use the Azuma-Hoeffding inequality to obtain
\begingroup\allowdisplaybreaks\begin{align}
	P\left(\frac{N_a(n)}{n}\leq\frac{\delta}{2K}(1-\epsilon')\right)&=P\bigg(N_a(n)-n\frac{\delta}{2K}\leq -n\epsilon'\frac{\delta}{2K}\bigg)\nonumber\\
	&= P\bigg(\bigg[N_a(n)-n\frac{\delta}{2K}\bigg]-N_a(0)\leq -n\epsilon'\frac{\delta}{2K}-N_a(0)\bigg)\nonumber\\
	& \leq P\bigg(\bigg[N_a(n)-n\frac{\delta}{2K}\bigg]-N_a(0)\leq -n\epsilon'\frac{\delta}{2K}\bigg)\nonumber\\
	&\leq \exp\left(-\frac{n(\epsilon')^2\delta^2}{8K^2}\right).\label{rested_arms_eq:azuma_bound_exp_bound_correction_term}
\end{align}\endgroup
Plugging \eqref{rested_arms_eq:azuma_bound_exp_bound_correction_term} back in \eqref{rested_arms_eq:exp_bound_correction_term_temp_1}, we arrive at
\begingroup\allowdisplaybreaks\begin{align}
	P\bigg(\alpha_n D(P_1||P_\delta|\mu_1)+\beta_n D(P_2||P_\delta|\mu_2)-6\epsilon
	<\frac{\log((K-1)L)}{n}\bigg)
	\leq K\exp\left(-\frac{n(\epsilon')^2\delta^2}{8K^2}\right).
\end{align}\endgroup

\item We now turn attention to \eqref{rested_arms_eq:exp_bound_t_4(n)/n}, which we upper bound as follows:
\begingroup\allowdisplaybreaks\begin{align}
	&P\left(\frac{T_4(n)}{n}-\frac{N_{h'}(n)}{n}\sum\limits_{i\in\mathcal{S}}\mu_2(i)H(P_2(\cdot|i))<-\epsilon\right)\nonumber\\
	&=P\bigg(\frac{N_{h'}(n)}{n}\bigg\lbrace\sum\limits_{i\in\mathcal{S}}\frac{N_{h'}(n,i)}{N_{h'}(n)}H\left(\frac{N_{h'}(n,i,\cdot)}{N_{h'}(n,i)}\right)
	-\mu_2(i)H(P_2(\cdot|i))\bigg\rbrace<-\epsilon\bigg)\nonumber\\
	&\leq P\bigg(\frac{N_{h'}(n)}{n}\bigg\lbrace\sum\limits_{i\in\mathcal{S}}\frac{N_{h'}(n,i)}{N_{h'}(n)}H\left(\frac{N_{h'}(n,i,\cdot)}{N_{h'}(n,i)}\right)
	-\sum\limits_{i\in\mathcal{S}}\mu_2(i)H(P_2(\cdot|i))\bigg\rbrace<-\epsilon,\nonumber\\
	&\hspace{8cm}\frac{N_a(n)}{n}>\frac{\delta}{2K}(1-\epsilon')\text{ for all }a\in\mathcal{A}\bigg)\nonumber\\
	&+\sum\limits_{a=1}^{K}P\left(\frac{N_a(n)}{n}\leq\frac{\delta}{2K}(1-\epsilon')\right).\label{rested_arms_eq:exp_bound_t_4(n)/n_1}
\end{align}\endgroup
From the analysis using the Azuma-Hoeffding inequality for bounded difference submartingales presented earlier, we know that each term inside the summation in \eqref{rested_arms_eq:exp_bound_t_4(n)/n_1} is exponentially bounded. The first term in \eqref{rested_arms_eq:exp_bound_t_4(n)/n_1} may be written as
\begingroup\allowdisplaybreaks\begin{align}
	&P\bigg(\frac{N_{h'}(n)}{n}\bigg\lbrace\sum\limits_{i\in\mathcal{S}}\frac{N_{h'}(n,i)}{N_{h'}(n)}H\left(\frac{N_{h'}(n,i,\cdot)}{N_{h'}(n,i)}\right)
	-\sum\limits_{i\in\mathcal{S}}\mu_2(i)H(P_2(\cdot|i))\bigg\rbrace<-\epsilon,\nonumber\\
	&\hspace{7cm}\frac{N_a(n)}{n}>\frac{\delta}{2K}(1-\epsilon')\text{ for all }a\in\mathcal{A}\bigg)\nonumber\\
	&\leq P\bigg(\bigg\lbrace\sum\limits_{i\in\mathcal{S}}\frac{N_{h'}(n,i)}{N_{h'}(n)}H\left(\frac{N_{h'}(n,i,\cdot)}{N_{h'}(n,i)}\right)
	-\sum\limits_{i\in\mathcal{S}}\mu_2(i)H(P_2(\cdot|i))\bigg\rbrace<-{\epsilon},\nonumber\\
	&\hspace{7cm}\frac{N_a(n)}{n}>\frac{\delta}{2K}(1-\epsilon')\text{ for all }a\in\mathcal{A}\bigg).\label{rested_arms_eq:exp_bound_t_4(n)/n_2}
\end{align}\endgroup
From Lemma \ref{rested_arms_lemma:convergence_of_ML_estimates}, we have the following almost sure convergences as $n\to\infty$:
\begingroup\allowdisplaybreaks\begin{align}
	\frac{N_{h'}(n,i,j)}{N_{h'}(n,i)}&\to P_2(j|i),\text{ for all }i,j\in\mathcal{S},\nonumber\\
	\frac{N_{h'}(n,i)}{N_{h'}(n)}&\to \mu_2(i),\text{ for all }i\in\mathcal{S}.
\end{align}\endgroup
%\begingroup\allowdisplaybreaks\begin{align}
%	\sum\limits_{i\in\mathcal{S}}\bigg[\frac{N_{h'}(n,i)}{N_{h'}(n)}H\left(\frac{N_{h'}(n,i,\cdot)}{N_{h'}(n,i)}\right)-\mu_2(i)H(P_2(\cdot|i))\bigg]
%\end{align}\endgroup
Using the above convergences and the continuity of the Shannon entropy functional $H(\cdot)$, we get that there exist constants $\delta_1=\delta_1(\epsilon)$ and $\delta_2=\delta_2(\epsilon)$ such that the probability in \eqref{rested_arms_eq:exp_bound_t_4(n)/n_2} may be upper bounded by the probability
\begingroup\allowdisplaybreaks\begin{align}
	&P\bigg(\exists~i,j\in\mathcal{S}\text{ such that }
	\bigg|\frac{N_{h'}(n,i,j)}{N_{h'}(n,i)}-P_2(j|i)\bigg|>\delta_1,\bigg|\frac{N_{h'}(n,i)}{N_{h'}(n)}-\mu_2(i)\bigg|>\delta_2,\nonumber\\
	&\hspace{7cm}
	\frac{N_a(n)}{n}>\frac{\delta}{2K}(1-\epsilon')\text{ for all }a\in\mathcal{A}\bigg).\label{rested_arms_eq:exp_bound_t_4(n)/n_3}
\end{align}\endgroup
Noting that $(N_{h'}(n,i,j)-N_{h'}(n,i)P_2(j|i))_{n\geq 0}$ and $(N_{h'}(n,i)-N_{h'}(n)\mu_2(j|i))_{n\geq 0}$ are martingale sequences for all $i,j\in\mathcal{S}$, we may then express \eqref{rested_arms_eq:exp_bound_t_4(n)/n_3} as a probability of deviation of martingale sequences from zero, which may be exponentially bounded by using results from \cite[Theorem 1.2A]{Victor1999}.


\item We now upper bound the term in \eqref{rested_arms_eq:exp_bound_t_2(n)/n}. Towards this, we first pick $\epsilon_1>0$ satisfying
\begin{equation}
	0<\epsilon_1\leq \frac{\epsilon}{1+2\sum\limits_{i\in\mathcal{S}}\mu_1(i)H(P_1(\cdot|i))}.\label{rested_arms_eq:epsilon_1}
\end{equation}
Then, the following almost sure convergences hold for all $i,j\in\mathcal{S}$:
\begingroup\allowdisplaybreaks\begin{align}
\frac{N_h(n)}{n}&\to \lambda_\delta^*,\nonumber\\
\frac{N_h(n,i,j)}{N_h(n)}&\to \mu_1(i)P_1(j|i).	
\end{align}\endgroup
Following the steps leading up to \eqref{rested_arms_eq:liminf_t_2(n)/n_final}, we note that for every choice of $\epsilon'>0$, there exists $M=M(\epsilon')$ such that \eqref{rested_arms_eq:liminf_t_2(n)/n_final} holds. We now choose $\epsilon'$ such that
\begingroup\allowdisplaybreaks\begin{align}
	\frac{T_{2}(n)}{n}
    	&\geq \frac{N_h(n)}{n}\bigg\lbrace\bigg[\sum\limits_{i\in\mathcal{S}}\sum\limits_{j\in\mathcal{S}}(\mu_1(i)P_1(j|i)+\epsilon')
    	\log\frac{\mu_1(i)P_1(j|i)+\epsilon'}{\mu_1(i)+\epsilon'|\mathcal{S}|}\bigg]-\epsilon'\bigg\rbrace\nonumber\\
    	&\geq \frac{N_h(n)}{n}\bigg(\sum\limits_{i\in\mathcal{S}}\mu_1(i)(-H(P_1(\cdot|i)))\bigg)-\epsilon_1
\end{align}\endgroup
holds for all sufficiently large values of $n$, where the last line above follows from the continuity of the term within braces as a function of $\epsilon'$. We then have
%we first pick $\epsilon_1>0$ and $\epsilon_2>0$ such that the following inequalities hold \text{almost surely} for all $i,j\in\mathcal{S}$:
%\begingroup\allowdisplaybreaks\begin{align}
%	\sup\limits_{n\geq l(L,\delta)}\bigg\vert\frac{N_h(n)}{n}-\lambda_\delta^*\bigg\vert\leq\epsilon_1,
%	\sup\limits_{n\geq l(L,\delta)}\bigg\vert\frac{N_h(n,i,j)}{N_h(n)}-\mu_1(i)P_1(j|i)\bigg\vert\leq\epsilon_2.
%\end{align}\endgroup
%Then, it follows that for all $n\geq l(L,\delta)$, we may express \eqref{rested_arms_eq:exp_bound_t_2(n)/n} as
\begingroup\allowdisplaybreaks\begin{align}
	&P\left(\frac{T_2(n)}{n}-\frac{N_h(n)}{n}\sum\limits_{i\in\mathcal{S}}\mu_1(i)(-H(P_1(\cdot|i)))<-\epsilon\right)\nonumber\\
	&\leq P\bigg(\frac{T_2(n)}{n}-\frac{N_h(n)}{n}\sum\limits_{i\in\mathcal{S}}\mu_1(i)(-H(P_1(\cdot|i)))<-\epsilon,
	\bigg\vert\frac{N_h(n)}{n}-\lambda_\delta^*\bigg\vert\leq\epsilon_1,\nonumber\\
	&\hspace{6cm}
	\bigg\vert\frac{N_h(n,i,j)}{N_h(n)}-\mu_1(i)P_1(j|i)\bigg\vert\leq\epsilon'\text{ for all }i,j\in\mathcal{S}\bigg)\nonumber\\
	&+P\bigg(\bigg\vert\frac{N_h(n)}{n}-\lambda_\delta^*\bigg\vert>\epsilon_1\bigg)+\sum\limits_{i,j\in\mathcal{S}}P\bigg(\bigg\vert\frac{N_h(n,i,j)}{N_h(n)}-\mu_1(i)P_1(j|i)\bigg\vert>\epsilon'\bigg).\label{rested_arms_eq:exp_bound_t_2(n)/n_1}
\end{align}\endgroup
We now focus on the first term in \eqref{rested_arms_eq:exp_bound_t_2(n)/n_1}, and notice that for all sufficiently large values of $n$, this term may be upper bounded as
\begingroup\allowdisplaybreaks\begin{align}
	&P\bigg((\lambda_\delta^*+\epsilon_1)\sum\limits_{i\in\mathcal{S}}\mu_1(i)(-H(P_1(\cdot|i)))-\epsilon_1
	<-\epsilon+(\lambda_\delta^*-\epsilon_1)\sum\limits_{i\in\mathcal{S}}\mu_1(i)(-H(P_1(\cdot|i)))\bigg)\nonumber\\
	&\leq P\bigg(\epsilon_1>\frac{\epsilon}{1+2\sum\limits_{i\in\mathcal{S}}\mu_1(i)H(P_1(\cdot|i))}\bigg)\nonumber\\
	&=0,
\end{align}\endgroup
where the last line follows from the choice of $\epsilon_1$ in \eqref{rested_arms_eq:epsilon_1}. Exponential bounds for the remaining terms in \eqref{rested_arms_eq:exp_bound_t_2(n)/n_1} can be obtained similarly as in the analysis of the first term in \eqref{rested_arms_eq:exp_bound_t_4(n)/n_1}.

Lastly, for the terms in \eqref{rested_arms_eq:exp_bound_t_1/n}, \eqref{rested_arms_eq:exp_bound_t_3(n)/n},  \eqref{rested_arms_eq:exp_bound_t_5(n)/n} and \eqref{rested_arms_eq:exp_bound_t_6}, noting that the left-hand sides of the inequality inside the probability expression in all the three terms converge to zero \text{almost surely}, similar procedures as used above for \eqref{rested_arms_eq:exp_bound_t_2(n)/n} and \eqref{rested_arms_eq:exp_bound_t_4(n)/n} may be used to obtain exponential upper bounds.

%From \eqref{rested_arms_eq:liminf_t_1(n)/n_final}, we note that there exists $\epsilon'>0$ such that
%\begingroup\allowdisplaybreaks\begin{align}
%    	&\frac{T_{2}(n)}{n}
%    	&\geq \frac{N_h(n)}{n}\bigg\lbrace\bigg[\sum\limits_{i\in\mathcal{S}}\sum\limits_{j\in\mathcal{S}}(\mu_1(i)P_1(j|i)+\epsilon')
%    	&\hspace{3.5cm}\log\frac{\mu_1(i)P_1(j|i)+\epsilon'}{\mu_1(i)+\epsilon'|\mathcal{S}|}\bigg]-\epsilon'\bigg\rbrace
%    	&\geq \frac{N_h(n)}{n}\sum\limits_{i,j\in\mathcal{S}}\mu_1(i)P_1(j|i)\log P_1(j|i)-\epsilon\label{rested_arms_eq:liminf_t_2(n)/n_final}
%    \end{align}\endgroup
\end{enumerate}
This completes the proof of the lemma.
\end{proof}

Using the result of Lemma \ref{rested_arms_lemma:exp_bound} in \eqref{rested_arms_eq:uniform_integrability_1}, we get that there exist constants $\theta>0$ and $0<B<\infty$ independent of $L$ such that the following holds:
\begingroup\allowdisplaybreaks\begin{align}
&\limsup\limits_{L\to\infty}E^\pi\bigg[\exp\bigg(\frac{\tau(\pi)}{\log L}\bigg)\bigg|C\bigg]	\nonumber\\
&\leq \exp\bigg(\frac{3}{\frac{\delta}{2K}(D(P_1||P_\delta|\mu_1)+D(P_2||P_\delta|\mu_2))}\bigg)+\limsup\limits_{L\to\infty}\sum\limits_{n\geq l(L,\delta)}B\exp\bigg(\frac{n+1}{\log L}-n \theta\bigg)\nonumber\\
	&<\infty,
\end{align}\endgroup
thus establishing that the family $\{\tau(\pi^\star(L,\delta))/\log L:L\geq 1\}$ is uniformly integrable.

Combining the above result on uniform integrability along with the asymptotic bound in \eqref{rested_arms_eq:almost_sure_upper_bound_for_policy_pi_star} yields the desired upper bound in \eqref{rested_arms_eq:upper_bound}.
\end{proof}



% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


%\section{Proofs of the Main Results}
%\section{Proof of Lemma \ref{rested_arms_lemma:ChangeOfMeasure_htoh'}}\label{rested_arms_appndx:proof_of_ChangeOfMeasure_htoh'}
%The proof follows the outline in \cite{kaufmann2016complexity}, with necessary modifications needed for the problem at hand. We use the shorthand notations $E_{h'}[\cdot]$ and $E_{h}[\cdot]$ to denote respectively the quantities $E^\pi[\cdot|C']$ and $E^\pi[\cdot|C]$; similarly for $P_{h'}(\cdot)$ and $P_h(\cdot)$. We begin by showing that for all $n\geq 0$, the following statement is true: for every measurable function $g:\mathcal{S}^{n+1}\to \mathbb{R}$, we have
%\begin{equation}
%E_{h'}[g(Z^{n})]=E_{h}[g(Z^{n})\,\exp(-Z_{hh'}(\tau))].\label{rested_arms_eq:for_all_g_mble_lemma}
%\end{equation}
%Assuming that the above statement is true, for any $E\in \mathcal{F}_{\tau}$, we have the following set of equations:
%\begingroup\allowdisplaybreaks\begin{align}
%P_{h'}(E)&=E_{h'}[\mathbb{I}_{E}]
%&\stackrel{(a)}{=}\sum\limits_{n=0}^{\infty} E_{h'}[\mathbb{I}_{E}\mathbb{I}_{\{\tau=n\}}]
%&\stackrel{(b)}{=}\sum\limits_{n=0}^{\infty} E_{h}[\mathbb{I}_{E}\mathbb{I}_{\{\tau=n\}}\,\exp(-Z_{hh'}(\tau))]
%&=E_{h}[\mathbb{I}_{E}\,\,\exp(-Z_{hh'}(\tau))],\label{rested_arms_eq:proof_lem_1}
%\end{align}\endgroup
%hence completing the proof of the lemma. In the above set of equations, $(a)$ is due to monotone convergence theorem, and $(b)$ follows from the application of \eqref{rested_arms_eq:for_all_g_mble_lemma} to the function $g(Z^{n})=\mathbb{I}_{E}\cdot\mathbb{I}_{\{\tau=n\}}$ by noting that $E\in \mathcal{F}_{\tau}$ and therefore $E\cap\{\tau=n\}\in \mathcal{F}_{n}$.
%
%We now proceed to prove \eqref{rested_arms_eq:for_all_g_mble_lemma} by induction on $n$.
%The result for the case $n=0$ follows by noting that $$P_{h'}(X_0^a=i)=P_h^a(X_0^a=i)=\nu(i)$$ for each $i\in\mathcal{S}$. Therefore, we have $E_h[g(Z_0)]=E_{h'}[g(Z_0)]$.
%%from the following set of equations:
%%\begingroup\allowdisplaybreaks\begin{align}
%%&E_{h'}[g(Z_{0})]=E_{h'}\left[\sum\limits_{a=1}^{K}\mathbb{I}_{\{A_{0}=a\}}g(X_{0}^{a})\right]
%%&\stackrel{(a)}{=}\sum\limits_{a=1}^{K}E_{h'}\left[\mathbb{I}_{\{A_{0}=a\}}E_{h'}\left[g(X_{0}^{a})\right|\mathcal{F}_{0}]\right]
%%&\stackrel{(b)}{=}\sum\limits_{a=1}^{K}P_{h'}(A_{0}=a)\cdot E_{h'}[g(X_{0}^{a})]
%%&\stackrel{(c)}{=}\sum\limits_{a=1}^{K}P_{h}(A_{0}=a)\cdot E_{h}\left[g(X_{0}^{a})\frac{P_{h'}^{a}\left(X_{0}^{a}\right)}{P_{h}^{a}\left(X_{0}^{a}\right)}\right]
%%&=E_{h}\left[\sum\limits_{a=1}^{K}\mathbb{I}_{\{A_{0}=a\}}g(X_{0}^{a})\frac{P_{h'}^{a}\left(X_{0}^{a}\right)}{P_{h}^{a}\left(X_{0}^{a}\right)}\right]
%%&=E_{h}\left[g(Z_{0})\sum\limits_{a=1}^{K}\mathbb{I}_{\{A_{0}=a\}}\,\exp\left(-\log \frac{P_{h}^{a}\left(Z_{0}\right)}{P_{h'}^{a}\left(Z_{0}\right)}\right)\right]
%%&=E_{h}\left[g(Z_{0})\,\exp\left(-\sum\limits_{a=1}^{K} \mathbb{I}_{\{A_{0}=a\}}\log\frac{P_{h}^{a}\left(Z_{0}\right)}{P_{h'}^{a}\left(Z_{0}\right)}\right)\right]
%%&=E_{h}\left[g(Z_{0})\,\exp\left(-\log\frac{P_{h}^{A_{0}}\left(Z_{0}\right)}{P_{h'}^{A_{0}}\left(Z_{0}\right)}\right)\right]
%%&\stackrel{(d)}{=}E_{h}\left[g(Z_{0})\,\,\exp(-Z_{hh'}(0))\right]\label{rested_arms_eq:lem_1_case_n_equal_to_0},
%%\end{align}\endgroup
%%where $(a)$ follows from the fact that $A_{0}$ is measurable with respect to $\mathcal{F}_{0}$, $(b)$ follows from the fact that $E_{h'}\left[g(X_{0}^{a})|\mathcal{F}_{0}\right]=E_{h'}[g(X_{0}^{a})]$, $(c)$ uses the assumption that $P_{h}(A_{0}=a)=P_{h'}(A_{0}=a)=\nu(a)$, and $(d)$ follows from \eqref{rested_arms_eq:Z_{hh'}(\tau)}.
%
%We now assume that \eqref{rested_arms_eq:for_all_g_mble_lemma} holds for some positive integer $n$, and show that it also holds for $n+1$. We have
%\begingroup\allowdisplaybreaks\begin{align}
%E_{h'}[g(Z^{n+1})]=E_{h'}\left[E_{h'}\left[g(Z^{n+1})|\mathcal{F}_{n+1}\right]\right].
%\end{align}\endgroup
%From \eqref{rested_arms_eq:filtration}, we note that $E_{h'}\left[g(Z^{n+1})|\mathcal{F}_{n+1}\right]$ is a function of $Z^{n}$, and thus by the induction hypothesis, we get
%\begingroup\allowdisplaybreaks\begin{align}
%E_{h'}[g(Z^{n+1})]&=E_{h}\left[E_{h'}\left[g(Z^{n+1})|\mathcal{F}_{n+1}\right]\,\exp(-Z_{hh'}(\tau))\right]
%=E_{h}\bigg[\sum\limits_{a=1}^{K}&\mathbb{I}_{\{A_{n+1}=a\}} &E_{h'}\left[g(Z^{n},X_{n+1}^{a})|\mathcal{F}_{n+1}\right]\exp(-Z_{hh'}(\tau))\bigg].\label{rested_arms_eq:lem_1_induc_partial_1}
%\end{align}\endgroup
%Also, we have
%\begingroup\allowdisplaybreaks\begin{align}
%&E_{h}[g(Z^{n},X_{n+1}^{a})|\mathcal{F}_{n+1}]
%&=\sum\limits_{z\in S} g(Z^{n},z)\cdot \frac{P_{h'}^{a}(X_{n+1}^{a}=z|Z^{n})}{P_{h}^{a}(X_{n+1}^{a}=z|Z^{n})}\cdot P_{h}^{a}(X_{n+1}^{a}=z|Z^{n}).\label{rested_arms_eq:lem_1_induc_partial_2}
%\end{align}\endgroup
%Plugging in \eqref{rested_arms_eq:lem_1_induc_partial_2} into \eqref{rested_arms_eq:lem_1_induc_partial_1}, we get
%\begingroup\allowdisplaybreaks\begin{align}
%&E_{h}[g(Z^{n},X_{n+1}^{a})|\mathcal{F}_{n+1}]
%=E_{h}&\bigg[\sum\limits_{a=1}^{K}\sum\limits_{z\in S}\mathbb{I}_{\{A_{n+1}=a\}}
%&g(Z^{n},z)\,P_{h}^{a}(Z_{n+1}=z|Z^{n})\,\,\exp(-Z_{hh'}(n+1,z))\bigg],\label{rested_arms_eq:lem_1_induc_partial_3}
%\end{align}\endgroup
%where
%\begingroup\allowdisplaybreaks\begin{align}
%&Z_{hh'}(n+1,z)
%&\coloneqq\sum\limits_{t=1}^{n}\log\left(\frac{P_{h}^{A_{t}}(Z_{t}|Z^{t-1})}{P_{h'}^{A_{t}}(Z_{t}|Z^{t-1})}\right)
%+\log\left(\frac{P_{h}^{a}(Z_{n+1}=z|Z^{n})}{P_{h'}^{a}(Z_{n+1}=z|Z^{n})}\right).
%\end{align}\endgroup
%Finally, the right-hand side of \eqref{rested_arms_eq:lem_1_induc_partial_3} simplifies to $ E_{h}[E_{h}\left[g(Z^{n+1})|\mathcal{F}_{n+1}\right]] = E_{h}[g(Z^{n+1})] $, hence completing the proof of the lemma.
%
%
%\section{Proof of Lemma \ref{rested_arms_lemma:RelBtwNijAndNi}}\label{rested_arms_appndx:ProofOfLemmaRelBtwnNijAndNi}
%We use the shorthand notation $E_h[\cdot]$ to denote $E^\pi[\cdot|C]$.
%We demonstrate that for each $i,j\in \mathcal{S}$ and $a\in \mathcal{A}$,
%\begingroup\allowdisplaybreaks\begin{align}
%&{E}_{h}[{E}_{h}[N_a(\tau,i,j)|X_0^a]|N_a(\tau)]
%&=E_{h}[E_{h}[N_a(\tau,i)|X_0^a]|N_a(\tau)]\cdot P_h^a(j|i).\label{rested_arms_eq:RelBtwNijAndNiWithIteratedExpec}
%\end{align}\endgroup
%Towards this, we note that
%\begingroup\allowdisplaybreaks\begin{align}
%&E_{h}[E_{h}[N_a(\tau,i,j)|X_0^a]|N_a(\tau)]
%&=E_{h}\left[\sum\limits_{m=1}^{N_a(\tau)-1}E_{h}[\mathbb{I}_{\{X_{m-1}^a=i,\,X_m^a=j\}}|X_0^a]\bigg\vert N_a(\tau)\right].\label{rested_arms_eq:lower_bound_partial_5_rested_arms}
%\end{align}\endgroup
%We now simplify the inner conditional expectation term in \eqref{rested_arms_eq:lower_bound_partial_5_rested_arms} by considering the cases $m=1$ and $m\geq 2$ separately.
%\begin{enumerate}
%	\item Case $m=1$:
%	In this case, we get
%	\begingroup\allowdisplaybreaks\begin{align}
%	&E_{h}[\mathbb{I}_{\{X_{0}^a=i,\,X_1^a=j\}}|X_0^a]
%	&=\mathbb{I}_{\{X_{0}^a=i\}}\cdot E_{h}[\mathbb{I}_{\{X_1^a=j\}}|X_0^a]
%	&=\mathbb{I}_{\{X_{0}^a=i\}}\cdot P_h^a(X_1^a=j|X_0^a=i)
%	&=\mathbb{I}_{\{X_{0}^a=i\}}\cdot P_h^a(j|i).\label{rested_arms_eq:Casem=1}
%	\end{align}\endgroup
%	\item Case $m\geq 2$: Here, we get
%	\begingroup\allowdisplaybreaks\begin{align}
%	&E_{h}[\mathbb{I}_{\{X_{m-1}^a=i,\,X_m^a=j\}}|X_0^a=k]
%	&=P_h^a(X_{m-1}^a=i,\,X_m^a=j|X_0^a=k)
%	&\stackrel{(a)}{=}P_h^a(X_{m-1}^a=i|X_0^a=k)\cdot P_h^a(X_1^a=j|X_0^a=i)
%	&=E_{h}[\mathbb{I}_{\{X_{m-1}^{a}=i\}}|X_0^a=k]\cdot P_h^a(j|i),
%	\end{align}\endgroup
%	from which it follows that $E_{h}[\mathbb{I}_{\{X_{m-1}^a=i,\,X_m^a=j\}}|X_0^a]=E_{h}[\mathbb{I}_{\{X_{m-1}^{a}=i\}}|X_0^a]\cdot P_h^a(j|i)$. In the above set of equations, $(a)$ follows from the fact that the Markov process of arm $a$ is time homogeneous.
%\end{enumerate}
%From the aforementioned cases, it follows that the relation
%\begingroup\allowdisplaybreaks\begin{align}
%E_{h}[\mathbb{I}_{\{X_{m-1}^a=i,\,X_m^a=j\}}|X_0^a]=E_{h}[\mathbb{I}_{\{X_{m-1}^{a}=i\}}|X_0^a]\cdot P_h^a(j|i)\label{rested_arms_eq:ConditioningOnX_0^a}
%\end{align}\endgroup
%holds for all $m\geq 1$. Substituting \eqref{rested_arms_eq:ConditioningOnX_0^a} in \eqref{rested_arms_eq:lower_bound_partial_5_rested_arms} and simplifying, we arrive at \eqref{rested_arms_eq:RelBtwNijAndNiWithIteratedExpec}. The lemma then follows by applying expectation $E_{h}[\cdot]$ to both sides of \eqref{rested_arms_eq:RelBtwNijAndNiWithIteratedExpec}.
%
%
%\section{Proof of Proposition \ref{rested_arms_prop:positive_drift_of_M_{hh'}(n)}}\label{rested_arms_appndx:proof_of_strictly_positive_drift_of_M_{hh'}(n)}
%Throughout, we assume that $C=(h,P_1,P_2)$ is the underlying configuration of the arms. We first show that under the non-stopping version of policy $\pi^\star(L,\delta)$, the maximum likelihood estimates $\hat{P}^n_{1,h}$ and $\hat{P}^n_{h,2}$ converge to their respective true values $P_1$ and $P_2$. This is presented in the following lemma.
%\begin{lemma}\label{rested_arms_lemma:convergence_of_ML_estimates}
%	Let $C=(h,P_1,P_2)$ denote the underlying configuration of the arms. Then, under the non-stopping version of policy $\pi^\star(L,\delta)$, as $n\to\infty$, the following convergences hold \text{almost surely} for all $i,j\in\mathcal{S}$:
%	\begingroup\allowdisplaybreaks\begin{align}
%		\frac{N_a(n,i,j)}{N_a(n,i)}&\longrightarrow\begin{cases}
%			P_1(j|i),&a=h,\\
%			P_2(j|i),&a\neq h,
%		\end{cases}
%		\frac{\sum\limits_{a\neq h}N_a(n,i,j)}{\sum\limits_{a\neq h}N_a(n,i)}&\longrightarrow P_2(j|i).\label{rested_arms_eq:convergence_of_ml_estimates}
%	\end{align}\endgroup
%\end{lemma}
%\begin{proof}
%Fix $i,j\in\mathcal{S}$ and $a\in\mathcal{A}$. Let $S_a(n)$ denote the quantity
%\begin{equation}
%	S_a(n)=\sum\limits_{t=0}^{n-1}\left(\mathbb{I}_{\{A_{t+1}=a\}}-P(A_{t+1}=a|A^t,\bar{X}^t)\right),\label{rested_arms_eq:S_a(n)}
%\end{equation}	
%where $P(A_{t+1}=a|A^t,\bar{X}^t)$ is as given in \eqref{rested_arms_eq:P(A_{n+1}=a|A^n,\bar{X}^n)}.
%Noting that the terms in the above summation constitute a bounded martingale difference sequence, it follows from \cite[Th. 1.2A]{Victor1999} that $\frac{S_a(n)}{n}\to 0$ \text{almost surely}. This implies that the following is true \text{almost surely} for sufficiently large values of $n$:
%\begingroup\allowdisplaybreaks\begin{align}
%	\frac{\delta}{2K}<\frac{N_a(n)-1}{n}<1+\frac{\delta}{2K}.\label{rested_arms_eq:N_a(n)_lies_between_two_quantities}
%\end{align}\endgroup
%Thus, we have $\liminf\limits_{n\to\infty}\frac{N_a(n)}{n}>\frac{\delta}{2K}>0$ \text{almost surely}. By the ergodic theorem, it then follows that as $n\to\infty$, the following convergences hold \text{almost surely}:
%\begingroup\allowdisplaybreaks\begin{align}
%    \frac{N_a(n,i)}{N_a(n)}&\longrightarrow \mu_h^a(i),
%	\frac{N_a(n,i,j)/N_a(n)}{N_a(n,i)/N_a(n)}&\longrightarrow P_h^a(j|i)\label{rested_arms_eq:first_part_convergence};
%\end{align}\endgroup
%here, $\mu_h^a(i)$ and $P_h^a(j|i)$ are as defined in \eqref{rested_arms_eq:mu_h^a} and \eqref{rested_arms_eq:P_h^a(j|i)} respectively.
%This establishes the convergence in the first line of \eqref{rested_arms_eq:convergence_of_ml_estimates} under the assumption that $C=(h,P_1,P_2)$ is the underlying configuration of the arms.
%
%We then note that \text{almost surely},
%\begingroup\allowdisplaybreaks\begin{align}
%	\frac{\sum\limits_{a\neq h}N_a(n,i,j)}{\sum\limits_{a\neq h}N_a(n,i)}&=\frac{\sum\limits_{a\neq h}\frac{N_a(n,i,j)}{N_h^a(n,i)}\frac{N_h^a(n,i)}{N_h^a(n)}\frac{N_h^a(n)}{n}}{\sum\limits_{a\neq h}\frac{N_a(n,i)}{N_h^a(n)}\frac{N_h^a(n)}{n}}
%	&\stackrel{n\to\infty}{\longrightarrow} P_2(j|i),
%\end{align}\endgroup
%where the convergence in the last line above follows from \eqref{rested_arms_eq:first_part_convergence} by noting that for $a\neq h$, when $C=(h,P_1,P_2)$ is the underlying configuration of the arms, $\mu_h^a(i)=\mu_2(i)$ and $P_h^a(j|i)=P_2(j|i)$. This establishes the convergence in the second line of \eqref{rested_arms_eq:convergence_of_ml_estimates}, thus completing the proof of the lemma.
%\end{proof}
%
%We now use the above lemma to show that $\liminf\limits_{n\to\infty}\frac{M_{hh'}(n)}{n}>0$ for any $h'\neq h$. Towards this, we note that from \eqref{rested_arms_eq:M_{hh'}(n)}, we have
%\begingroup\allowdisplaybreaks\begin{align}
%	&\liminf_{n\to\infty}\frac{M_{hh'}(n)}{n}\nonumber
%	\\
%	&=\liminf\limits_{n\to\infty}\left(\frac{T_1}{n}+\frac{T_2(n)}{n}+\frac{T_3(n)}{n}+\frac{T_4(n)}{n}+\frac{T_5(n)}{n}\right),\label{rested_arms_eq:M_{hh'}(n)_pos_drift_1}
%\end{align}\endgroup
%where $T_1$ and $T_2(n),\ldots,T_5(n)$ are given by \eqref{rested_arms_eq:t_1}-\eqref{rested_arms_eq:t_5(n)}.
%
%We now show that the quantity on the right-hand side of \eqref{rested_arms_eq:M_{hh'}(n)_pos_drift_1} is strictly positive. Towards this, we note that for any choice of $\epsilon'>0$, we have the following:
%\begin{enumerate}
%	\item Since $T_1$ is a constant that does not grow with $n$, we have
%	\begin{equation}
%		\lim\limits_{n\to\infty}\frac{T_1}{n}=0,\label{rested_arms_eq:liminf_t_1(n)/n_final}
%	\end{equation}
%	and therefore it follows that there exists a positive integer $M_1=M_1(\epsilon')$ such that $T_1/n\geq -\epsilon'$ for all $n\geq M_1$.
%	\item From \eqref{rested_arms_eq:t_2(n)}, we have
%    \begingroup\allowdisplaybreaks\begin{align}
%    	\frac{T_2(n)}{n}=\frac{1}{n}\sum\limits_{i\in\mathcal{S}}\log B((N_h(n,i,j)+1)_{j\in\mathcal{S}}).\label{rested_arms_eq:liminf_t_2(n)/n_1}
%    \end{align}\endgroup
%    Fix $i\in\mathcal{S}$. Then, we have
%    \begingroup\allowdisplaybreaks\begin{align}
%    	\log B((N_h(n,i,j)+1)_{j\in\mathcal{S}})=\log E\left[\prod\limits_{j\in\mathcal{S}}X_{ij}^{N_h(n,i,j)}\right],\label{rested_arms_eq:liminf_t_2(n)/n_2}
%    \end{align}\endgroup
%    where the random vector $(X_{ij})_{j\in\mathcal{S}}$ follows Dirichlet distribution with parameters $\alpha_j=1$ for all $j\in\mathcal{S}$. We now write \eqref{rested_arms_eq:liminf_t_2(n)/n_2} as follows:
%    \begingroup\allowdisplaybreaks\begin{align}
%    	&\frac{1}{N_h(n)}\log B((N_h(n,i,j)+1)_{j\in\mathcal{S}})
%    	&=\frac{\log E\left[\exp\left(N_h(n)\sum\limits_{j\in\mathcal{S}} \frac{N_h(n,i,j)}{N_h(n)}\log X_{ij}\right)\right]}{N_h(n)}.\label{rested_arms_eq:liminf_t_2(n)/n_3}
%    \end{align}\endgroup
%    We now note that when $C=(h,P_1,P_2)$ is the underlying configuration of the arms, then from Lemma \ref{rested_arms_lemma:convergence_of_ML_estimates}, we have that $N_h(n,i,j)/N_h(n)$ converges \text{almost surely} as $n\to\infty$ to $\mu_1(i)P_1(j|i)$. Thus, there exists a positive integer $M_{21}=M_{21}(\epsilon')$ such that for all $n\geq M_{21}$, we have
%    \begingroup\allowdisplaybreaks\begin{align}
%    	&\frac{1}{N_h(n)}\log B((N_h(n,i,j)+1)_{j\in\mathcal{S}})
%    	&\geq \frac{\log E\left[\exp\left(N_h(n)\sum\limits_{j\in\mathcal{S}} (\mu_1(i)P_1(j|i)+\epsilon')\log X_{ij}\right)\right]}{N_h(n)}.\label{rested_arms_eq:liminf_t_2(n)/n_4}
%    \end{align}\endgroup
%    Noting that $N_h(n)$ converges \text{almost surely} to $+\infty$ as $n\to\infty$, by Varadhan's integral lemma \cite[Th. 4.3.1]{AmirDembo2009}, there exists a positive integer $M_{22}=M_{22}(\epsilon')$ such that for all $n\geq M_2=\max\{M_{21},M_{22}\}$, we have
%    \begingroup\allowdisplaybreaks\begin{align}
%    	&\frac{1}{N_h(n)}\log B((N_h(n,i,j)+1)_{j\in\mathcal{S}})
%    	&\stackrel{(a)}{\geq} \sup\limits_{\{(z_j)_{j\in\mathcal{S}}\}}\sum\limits_{j\in\mathcal{S}}(\mu_1(i)P_1(j|i)+\epsilon')\log z_j-\frac{\epsilon'}{|\mathcal{S}|}
%    	&=\sum\limits_{j\in\mathcal{S}}(\mu_1(i)P_1(j|i)+\epsilon')\log\frac{\mu_1(i)P_1(j|i)+\epsilon'}{\mu_1(i)+\epsilon'|\mathcal{S}|}-\frac{\epsilon'}{|\mathcal{S}|},\label{rested_arms_eq:liminf_t_2(n)/n_5}
%    \end{align}\endgroup
%    where the supremum on the right-hand side of $(a)$ above is computed over all vectors $(z_j)_{j\in\mathcal{S}}$ such that $z_j\geq 0$ for all $j\in\mathcal{S}$, and $\sum\limits_{j\in\mathcal{S}}z_j=1$. Plugging \eqref{rested_arms_eq:liminf_t_2(n)/n_5} into \eqref{rested_arms_eq:liminf_t_2(n)/n_1}, we get
%    \begingroup\allowdisplaybreaks\begin{align}
%    	&\frac{T_{2}(n)}{n}
%    	&\geq \frac{N_h(n)}{n}\bigg\lbrace\bigg[\sum\limits_{i\in\mathcal{S}}\sum\limits_{j\in\mathcal{S}}(\mu_1(i)P_1(j|i)+\epsilon')
%    	&\hspace{3.5cm}\log\frac{\mu_1(i)P_1(j|i)+\epsilon'}{\mu_1(i)+\epsilon'|\mathcal{S}|}\bigg]-\epsilon'\bigg\rbrace\label{rested_arms_eq:liminf_t_2(n)/n_final}
%    \end{align}\endgroup
%    for all $n\geq M_2$.
%%    Fix an $i\in\mathcal{S}$. Then, from \eqref{rested_arms_eq:f(A^n,\bar{X}^n|H_h)}, we have
%%    \begingroup\allowdisplaybreaks\begin{align}
%%    	&\liminf\limits_{n\to\infty}\frac{1}{n}\log B((N_h(n,i,j)+1)_{j\in\mathcal{S}})
%%    	&=\liminf\limits_{n\to\infty}\frac{1}{n}\log E\left[\prod\limits_{j\in\mathcal{S}}X_{ij}^{N_h(n,i,j)}\right]
%%    	&=\liminf\limits_{n\to\infty}\frac{1}{n}\log E\left[\exp\left(\sum\limits_{j\in\mathcal{S}} N_h(n,i,j)\log X_{ij}\right)\right]
%%    	&= \liminf\limits_{n\to\infty}\frac{1}{n}E\left[\exp\left(N_h(n)\sum\limits_{j\in\mathcal{S}} \frac{N_h(n,i,j)}{N_h(n)}\log X_{ij}\right)\right],\label{rested_arms_eq:liminf_t_2(n)/n_2}
%%    \end{align}\endgroup
%%    where for each $i\in\mathcal{S}$, the collection $(X_{ij})_{j\in\mathcal{S}}$ follows Dirichlet distribution with parameters $\alpha_j=1$ for all $j\in\mathcal{S}$. Noting that
%%    \begin{equation}
%%    	\frac{N_h(n,i,j)}{N_h(n)}\stackrel{n\to\infty}{\longrightarrow} \mu_1(i)P_1(j|i)\text{ \text{almost surely}}
%%    \end{equation}
%%    by lemma \ref{rested_arms_lemma:convergence_of_ML_estimates},
%%    we use Varadhan's integral lemma \cite[Th. 4.3.1, pp. 137]{AmirDembo2009} to obtain
%%    \begingroup\allowdisplaybreaks\begin{align}
%%    	&\liminf\limits_{n\to\infty}\frac{1}{n}\log B((N_h(n,i,j)+1)_{j\in\mathcal{S}})
%%    	&\geq \liminf\limits_{n\to\infty}\frac{N_h(n)}{n}\mu_1(i)\sup\limits_{\{(z_j)_{j\in\mathcal{S}}\}}\sum\limits_{j\in\mathcal{S}}P_1(j|i)\log z_j,\label{rested_arms_eq:liminf_t_2(n)/n_3}
%%    \end{align}\endgroup
%%    where the supremum on the right-hand side is computed over all vectors $(z_j)_{j\in\mathcal{S}}$ such that $z_j\geq 0$ for all $j\in\mathcal{S}$, and $\sum\limits_{j\in\mathcal{S}}z_j=1$. Simplifying the right-hand side of \eqref{rested_arms_eq:liminf_t_2(n)/n_3}, we get
%%    \begingroup\allowdisplaybreaks\begin{align}
%%    	&\liminf\limits_{n\to\infty}\frac{1}{n}\log B((N_h(n,i,j)+1)_{j\in\mathcal{S}})
%%    	&\geq \liminf\limits_{n\to\infty}\frac{N_h(n)}{n}\mu_1(i)(-H(P_1(\cdot||i))),\label{rested_arms_eq:liminf_t_2(n)/n_4}
%%    \end{align}\endgroup
%%    where $H(P_1(\cdot|i))>0$ represents the Shannon entropy of the probability vector $(P_1(j|i))_{j\in\mathcal{S}}$, and is given by
%%    \begin{equation}
%%    	H(P_1(\cdot|i))=\sum\limits_{j\in\mathcal{S}}P_1(j|i)\log\frac{1}{P_1(j|i)}.\label{rested_arms_eq:Shannon_entropy}
%%    \end{equation}
%%    Plugging \eqref{rested_arms_eq:liminf_t_2(n)/n_4} into \eqref{rested_arms_eq:liminf_t_2(n)/n_1}, we get
%%    \begingroup\allowdisplaybreaks\begin{align}
%%    	&\liminf\limits_{n\to\infty}\frac{T_2(n)}{n}
%%    	&\geq \liminf\limits_{n\to\infty}\left(\frac{N_h(n)}{n}\left(\sum\limits_{i\in\mathcal{S}}\mu_1(i)(-H(P_1(\cdot|i)))\right)\right).\label{rested_arms_eq:liminf_t_2(n)/n_final}
%%    \end{align}\endgroup
%%    Similar arguments as above can be used to show that
%%    \begingroup\allowdisplaybreaks\begin{align}
%%    	&\limsup\limits_{n\to\infty}\frac{T_2(n)}{n}
%%    	&\leq \limsup\limits_{n\to\infty}\left(\frac{N_h(n)}{n}\left(\sum\limits_{i\in\mathcal{S}}\mu_1(i)(-H(P_1(\cdot|i)))\right)\right).\label{rested_arms_eq:limsup_t_2(n)/n}
%%    \end{align}\endgroup
%%    We note that the results in \eqref{rested_arms_eq:liminf_t_2(n)/n_5} and \eqref{rested_arms_eq:limsup_t_2(n)/n} imply that $\limsup\limits_{n\to\infty}N_h(n)/n\leq \liminf\limits_{n\to\infty}N_h(n)/n$. This in turn implies that $\lim\limits_{n\to\infty} N_h(n)/n$ exists. Putting together all the results obtained, we get
%%    \begingroup\allowdisplaybreaks\begin{align}
%%    	&\liminf\limits_{n\to\infty}\frac{T_2(n)}{n}    	&\geq\left(\lim\limits_{n\to\infty}\frac{N_h(n)}{n}\right)\left(\sum\limits_{i\in\mathcal{S}}\mu_1(i)(-H(P_1(\cdot|i))\right).\label{rested_arms_eq:liminf_t_2(n)/n_final}
%%    \end{align}\endgroup
%%    \item From \eqref{rested_arms_eq:t_3(n)}, we have
%%    \begingroup\allowdisplaybreaks\begin{align}
%%    	&\liminf_{n\to\infty}\frac{T_3(n)}{n}
%%    	&=\liminf_{n\to\infty}\frac{1}{n}\sum\limits_{i\in\mathcal{S}}\log B((\sum\limits_{a\neq h}N_a(n,i,j)+1)_{j\in\mathcal{S}})
%%    	&\geq \sum\limits_{i\in\mathcal{S}}\liminf\limits_{n\to\infty}\frac{1}{n}\log B((\sum\limits_{a\neq h}N_a(n,i,j)+1)_{j\in\mathcal{S}}).\label{rested_arms_eq:liminf_t_3(n)/n_1}
%%    \end{align}\endgroup
%%    Noting that
%%    \begin{equation}
%%    	\frac{\sum\limits_{a\neq h}N_a(n,i,j)}{\sum\limits_{a\neq h}N_a(n)}\stackrel{n\to\infty}{\longrightarrow} \mu_2(i)P_2(j|i)\text{ \text{almost surely}}
%%    \end{equation}
%%    by lemma \ref{rested_arms_lemma:convergence_of_ML_estimates}, and
%%    following the same approach used to simplify \eqref{rested_arms_eq:liminf_t_2(n)/n_1}, we get
%%    \begingroup\allowdisplaybreaks\begin{align}
%%    	&\liminf\limits_{n\to\infty}\frac{T_3(n)}{n}
%%    	&\geq \left(\lim\limits_{n\to\infty}\sum\limits_{a\neq h}\frac{N_a(n)}{n}\right)\left(\sum\limits_{i\in\mathcal{S}}\mu_2(i)(-H(P_2(\cdot|i)))\right).\label{rested_arms_eq:liminf_t_3(n)/n_final}
%%    \end{align}\endgroup
%    \item From \eqref{rested_arms_eq:t_3(n)}, we have
%    \begingroup\allowdisplaybreaks\begin{align}
%    	\frac{T_3(n)}{n}=\frac{1}{n}\sum\limits_{i\in\mathcal{S}}\log B((\sum\limits_{a\neq h}N_a(n,i,j)+1)_{j\in\mathcal{S}}).\label{rested_arms_eq:liminf_t_3(n)/n_1}
%    \end{align}\endgroup
%    Using the same arguments as those used to simplify \eqref{rested_arms_eq:liminf_t_2(n)/n_1}, we obtain the following: there exists a positive integer $M_3=M_3(\epsilon')$ such that for all $n\geq M_3$, we have
%    \begingroup\allowdisplaybreaks\begin{align}
%    	&\frac{T_{3}(n)}{n}
%    	&\geq \frac{\sum\limits_{a\neq h}N_a(n)}{n}\bigg\lbrace\bigg[\sum\limits_{i\in\mathcal{S}}\sum\limits_{j\in\mathcal{S}}(\mu_2(i)P_2(j|i)+\epsilon')
%    	&\hspace{3.5cm}\log\frac{\mu_2(i)P_2(j|i)+\epsilon'}{\mu_2(i)+\epsilon'|\mathcal{S}|}\bigg]-\epsilon'\bigg\rbrace.\label{rested_arms_eq:liminf_t_3(n)/n_final}
%    \end{align}\endgroup
%    \item From \eqref{rested_arms_eq:t_4(n)}, we have
%    \begingroup\allowdisplaybreaks\begin{align}
%    	\frac{T_4(n)}{n}=-\frac{1}{n}\sum\limits_{i,j\in\mathcal{S}}N_{h'}(n,i,j)\log\frac{N_{h'}(n,i,j)}{N_{h'}(n,i)}\label{rested_arms_eq:liminf_t_4(n)/n_1}.
%    \end{align}\endgroup
%    Noting that $h'\neq h$, when the underlying configuration is $C=(h,P_1,P_2)$, from Lemma \ref{rested_arms_lemma:convergence_of_ML_estimates}, we have the following almost sure convergences (as $n\to\infty$):
%    \begingroup\allowdisplaybreaks\begin{align}
%    	\frac{N_{h'}(n.i.j)}{n}&\to \mu_2(i)P_2(j|i),
%    	\frac{N_{h'}(n,i,j)}{N_{h'}(n,i)}&\to P_2(j|i).\label{rested_arms_eq:liminf_t_4(n)/n_2}
%    \end{align}\endgroup
%    Using these in \eqref{rested_arms_eq:liminf_t_4(n)/n_1}, we get that there exists a positive integer $M_4=M_4(\epsilon')$ such that for all $n\geq M_4$, we have
%    \begingroup\allowdisplaybreaks\begin{align}
%    	\frac{T_4(n)}{n}\geq \sum\limits_{i,j\in\mathcal{S}}(\mu_2(i)P_2(j|i)-\epsilon')\log\frac{1}{P_2(j|i)+\epsilon'}.\label{rested_arms_eq:liminf_t_4(n)/n_final}
%    \end{align}\endgroup
%    \item Lastly, we present a simplification of the term $T_5(n)/n$. From \eqref{rested_arms_eq:t_5(n)}, we have
%    \begingroup\allowdisplaybreaks\begin{align}
%    	\frac{T_5(n)}{n}=-\frac{1}{n}\sum\limits_{i,j\in\mathcal{S}}\sum\limits_{a\neq h'}N_{a}(n,i,j)\log\frac{\sum\limits_{a\neq h'}N_{a}(n,i,j)}{\sum\limits_{a\neq h'}N_{a}(n,i)}.\label{rested_arms_eq:liminf_t_5(n)/n_1}
%    \end{align}\endgroup
%    For each $n$ and each $i,j\in\mathcal{S}$, we define $P_n(j|i)$ as the following quantity:
%    \begin{equation}
%    	P_n(j|i)=\frac{\sum\limits_{a\neq h'}N_a(n,i,j)}{\sum\limits_{a\neq h'}N_a(n,i)}.\label{rested_arms_eq:P_n(j|i)}
%    \end{equation}
%    Note that $P_n=(P_n(j|i))_{i,j\in\mathcal{S}}$ constitutes a valid probability transition matrix. From Lemma \ref{rested_arms_lemma:convergence_of_ML_estimates}, under the underlying configuration $C=(h,P_1,P_2)$, we note the following almost convergences as $n\to\infty$:
%    \begingroup\allowdisplaybreaks\begin{align}
%    	\frac{\sum\limits_{a\neq h,h'}N_{a}(n,i,j)}{\sum\limits_{a\neq h,h'}N_{a}(n,i)}&\stackrel{n\to\infty}{\longrightarrow} P_2(j|i),
%  	\frac{\sum\limits_{a\neq h,h'}N_{a}(n,i)}{\sum\limits_{a\neq h,h'}N_{a}(n)}&\stackrel{n\to\infty}{\longrightarrow} \mu_2(i).
%    \end{align}\endgroup
%    The above convergences then imply that there exists a positive integer $M_5=M_5(\epsilon')$ such that for all $n\geq M_5$, we have
%    \begingroup\allowdisplaybreaks\begin{align}
%    	&\frac{T_5(n)}{n}\geq \frac{N_h(n)}{n}\sum\limits_{i,j\in\mathcal{S}}(\mu_1(i)P_1(j|i)-\epsilon')\log\frac{1}{P_n(j|i)}
%    	&+\frac{\sum\limits_{a\neq h,h'}N_a(n)}{n}\sum\limits_{i,j\in\mathcal{S}}(\mu_2(i)P_2(j|i)-\epsilon')\log\frac{1}{P_n(j|i)}.\label{rested_arms_eq:liminf_t_5(n)/n_final}
%    \end{align}\endgroup
%%    inside the summation in \eqref{rested_arms_eq:M_{hh'}(n)_pos_drift_1}. For each $n$, let $\alpha_n$ and $\beta_n$ denote the sequences
%%    \begin{equation}
%%    	\alpha_n=\frac{N_h(n)}{n},\quad \beta_n=\frac{\sum\limits_{a\neq h,h'}N_a(n)}{n}.\label{rested_arms_eq:alpha_n_beta_n}
%%    \end{equation}
%%    Then, we have
%%    \begingroup\allowdisplaybreaks\begin{align}
%%    	&\frac{\sum\limits_{a\neq h'}N_{a}(n,i,j)}{\sum\limits_{a\neq h'}N_{a}(n,i)}=\frac{N_h(n,i,j)+\sum\limits_{a\neq h,h'}N_{a}(n,i,j)}{N_h(n,i)+\sum\limits_{a\neq h,h'}N_{a}(n,i)}
%%    	&=\frac{\frac{N_h(n,i,j)}{N_h(n,i)}\frac{N_h(n,i)}{N_h(n)}\alpha_n+\frac{\sum\limits_{a\neq h,h'}N_{a}(n,i,j)}{\sum\limits_{a\neq h,h'}N_{a}(n,i)}\frac{\sum\limits_{a\neq h,h'}N_{a}(n,i)}{\sum\limits_{a\neq h,h'}N_{a}(n)}\beta_n}{\frac{N_h(n,i)}{N_h(n)}\alpha_n+\frac{\sum\limits_{a\neq h,h'}N_{a}(n,i)}{\sum\limits_{a\neq h,h'}N_{a}(n)}\beta_n}.\label{rested_arms_eq:liminf_t_5(n)_1}
%%    \end{align}\endgroup
%%    By lemma \ref{rested_arms_lemma:convergence_of_ML_estimates}, we note that when $C=(h,P_1,P_2)$ denotes the underlying configuration of the arms, we have the following convergences \text{almost surely}:
%%    \begingroup\allowdisplaybreaks\begin{align}
%%    	\frac{\sum\limits_{a\neq h,h'}N_{a}(n,i,j)}{\sum\limits_{a\neq h,h'}N_{a}(n,i)}&\stackrel{n\to\infty}{\longrightarrow} P_2(j|i),
%%    	\frac{\sum\limits_{a\neq h,h'}N_{a}(n,i)}{\sum\limits_{a\neq h,h'}N_{a}(n)}&\stackrel{n\to\infty}{\longrightarrow} \mu_2(i).
%%    \end{align}\endgroup
%%    We use the above convergences in \eqref{rested_arms_eq:liminf_t_5(n)_1} and, after simplification,  arrive at the following inequality:
%%    \begingroup\allowdisplaybreaks\begin{align}
%%    	&\liminf\limits_{n\to\infty}\frac{T_5(n)}{n}
%%    	&\geq \liminf\limits_{n\to\infty}\sum\limits_{i,j\in\mathcal{S}}\alpha_n\mu_1(i)P_1(j|i)\log\frac{1}{P_n(j|i)}
%%    	&\hspace{1cm}+\liminf\limits_{n\to\infty}\sum\limits_{i,j\in\mathcal{S}}\beta_n\mu_2(i)P_2(j|i)\log\frac{1}{P_n(j|i)},\label{rested_arms_eq:liminf_t_5(n)_2}
%%    \end{align}\endgroup
%%    where the term $P_n(j|i)$ is given by
%%    \begin{equation}
%%    	P_n(j|i)=\frac{\alpha_n\mu_1(i)P_1(j|i)+\beta_n\mu_2(i)P_2(j|i)}{\alpha_n\mu_1(i)+\beta_n\mu_2(i)}.\label{rested_arms_eq:P_n(j|i)}
%%    \end{equation}
%%    We now rewrite the right-hand side of \eqref{rested_arms_eq:liminf_t_5(n)_2} as follows:
%%    \begingroup\allowdisplaybreaks\begin{align}
%%    &\liminf\limits_{n\to\infty}\frac{T_5(n)}{n}
%%    &\geq \liminf\limits_{n\to\infty}\alpha_n\bigg\lbrace\sum\limits_{i\in\mathcal{S}}\mu_1(i)P_1(j|i)\log\frac{P_1(j|i)}{P_n(j|i)}
%%    &\hspace{3cm}+\sum\limits_{i\in\mathcal{S}}\mu_1(i)P_1(j|i)\log \frac{1}{P_1(j|i)}\bigg\rbrace
%%    &+ \liminf\limits_{n\to\infty}\beta_n\bigg\lbrace\sum\limits_{i\in\mathcal{S}}\mu_2(i)P_2(j|i)\log\frac{P_2(j|i)}{P_n(j|i)}
%%    &\hspace{3cm}+\sum\limits_{i\in\mathcal{S}}\mu_2(i)P_2(j|i)\log \frac{1}{P_2(j|i)}\bigg\rbrace
%%    &\geq \liminf\limits_{n\to\infty}\alpha_n D(P_1||P_n|\mu_1)+\liminf\limits_{n\to\infty}\beta_n D(P_2||P_n|\mu_2)	
%%    &+(\lim\limits_{n\to\infty}\alpha_n) \left(\sum\limits_{i\in\mathcal{S}}\mu_1(i)H(P_1(\cdot|i))\right)
%%    &+(\lim\limits_{n\to\infty}\beta_n) \left(\sum\limits_{i\in\mathcal{S}}\mu_2(i)H(P_2(\cdot|i))\right),\label{rested_arms_eq:liminf_t_5(n)_final}
%%    \end{align}\endgroup
%%    where $D(P_1||P_n|\mu_1)$ denotes the quantity
%%    \begingroup\allowdisplaybreaks\begin{align}
%%    	D(P_1||P_n|\mu_1)=\sum\limits_{i,j\in\mathcal{S}}\mu_1(i)P_1(j|i)\log\frac{P_1(j|i)}{P_n(j|i)};\label{rested_arms_eq:D(P_1||P_n|mu_1)}
%%    \end{align}\endgroup
%%    similarly for $D(P_2||P_n|\mu_2)$.
%\end{enumerate}
%
%Combining the results in \eqref{rested_arms_eq:liminf_t_1(n)/n_final}, \eqref{rested_arms_eq:liminf_t_2(n)/n_final}, \eqref{rested_arms_eq:liminf_t_3(n)/n_final}, \eqref{rested_arms_eq:liminf_t_4(n)/n_final} and \eqref{rested_arms_eq:liminf_t_5(n)/n_final}, we get that for all $n\geq M(\epsilon')=\max\{M_1,\dots,M_5\}$, we have
%\begingroup\allowdisplaybreaks\begin{align}
%	\frac{M_{hh'}(n)}{n}\geq f_{n}(\epsilon'),\label{rested_arms_eq:liminf_M_{hh'}(n)/n_1}
%\end{align}\endgroup
%	%
%%	&+(\liminf\limits_{n\to\infty}\alpha_n-\limsup\limits_{n\to\infty}\alpha_n)\left(\sum\limits_{i\in\mathcal{S}}\mu_1(i)H(P_1(\cdot|i))\right)
%%	&+(\liminf\limits_{n\to\infty}\beta_n-\limsup\limits_{n\to\infty}\beta_n)\left(\sum\limits_{i\in\mathcal{S}}\mu_2(i)H(P_2(\cdot|i))\right)
%%	&\geq \liminf\limits_{n\to\infty}\alpha_n D(P_1||P_n|\mu_1)+\liminf\limits_{n\to\infty}\beta_n D(P_2||P_n|\mu_2).
%where $f_n(\epsilon')$ denotes the sum of the terms of the right-hand sides of \eqref{rested_arms_eq:liminf_t_1(n)/n_final}, \eqref{rested_arms_eq:liminf_t_2(n)/n_final}, \eqref{rested_arms_eq:liminf_t_3(n)/n_final}, \eqref{rested_arms_eq:liminf_t_4(n)/n_final} and \eqref{rested_arms_eq:liminf_t_5(n)/n_final}.
%
%We now define $f_n(0)$ as the following quantity:
%\begingroup\allowdisplaybreaks\begin{align}
%	f_n(0)=\frac{N_h(n)}{n}D(P_1||P_n|\mu_1)+\frac{\sum\limits_{a\neq h,h'}N_a(n)}{n}D(P_2||P_n||\mu_2).\label{rested_arms_eq:f_n(0)}
%\end{align}\endgroup
%Then, we observe that $f_n(\epsilon')\to f_n(0)$ as $\epsilon'\to 0$.
%Thus, we have the following: for any choice of $\epsilon>0$, there exists $\epsilon'>0$ such that $f_n(\epsilon')>f_n(0)-\epsilon$ for all sufficiently large values of $n$. This implies that
%\begin{equation}
%	\frac{M_{hh'}(n)}{n}\geq f_{n}(0)-\epsilon
%\end{equation}
%for all sufficiently large values of $n$, from which it follows that
%\begin{equation}
%	\liminf\limits_{n\to\infty}\left[\frac{M_{hh'}(n)}{n}-f_n(0)\right]\geq -\epsilon.
%\end{equation}
%Since the above equation is true for arbitrary choice of $\epsilon$, letting $\epsilon\downarrow 0$, we get
%\begingroup\allowdisplaybreaks\begin{align}
%	\liminf\limits_{n\to\infty}\frac{M_{hh'}(n)}{n}\geq \liminf\limits_{n\to\infty}f_n(0)>0.\label{rested_arms_eq:liminf_M_{hh'}(n)/n_strictly_positive}
%\end{align}\endgroup
%This completes the proof of the proposition.
%%We now claim that each of the terms appearing on the right-hand side of the above equation is strictly positive. Indeed, since $P_2\neq P_1$, there exist states $i',j'\in\mathcal{S}$ such that $P_1(j'|i')\neq P_2(j'|i')$. Also, by \eqref{rested_arms_eq:N_a(n)_lies_between_two_quantities}, we have
%%\begin{equation}
%%	(K-2)\frac{\delta}{2K}<\beta_n<(K-2)\left(1+\frac{\delta}{2K}\right)
%%\end{equation}
%%for all values of $n$ sufficiently large.
%%Therefore, by Pinsker's inequality, for all sufficiently large values of $n$, we have
%%\begingroup\allowdisplaybreaks\begin{align}
%%	& D(P_1||P_n|\mu_1)
%%	&=\sum\limits_{i\in\mathcal{S}}\mu_1(i)\sum\limits_{j\in\mathcal{S}}P_1(j|i)\log\frac{P_1(j|i)}{P_n(j|i)}
%%	&\geq c\sum\limits_{i\in\mathcal{S}}\mu_1(i)\bigg(\sum\limits_{j\in\mathcal{S}}|P_1(j|i)-P_n(j|i)|\bigg)^2
%%	&=c\sum\limits_{i\in\mathcal{S}}\mu_1(i)\left(\sum\limits_{j\in\mathcal{S}}\frac{\beta_n\mu_2(i)}{\alpha_n\mu_1(i)+\beta_n\mu_2(i)}|P_1(j|i)-P_2(j|i)|\right)^2
%%	&\geq c\bigg\lbrace\mu_1(i')\left(\frac{(K-2)\frac{\delta}{2K}}{(K-1)\left(1+\frac{\delta}{2K}\right)}\right)^2
%%	&\hspace{4cm}\mu_2^2(i')|P_1(j'|i')-P_2(j'|i')|^2\bigg\rbrace
%%	&>0,
%%\end{align}\endgroup
%%where in the above set of inequalities, $c$ is a constant from Pinsker's inequality. The last line follows from the fact that $\mu_1(j)>0$ and $\mu_2(j)>0$ for all $j\in\mathcal{S}$ since the Markov process of each arm is irreducible and positive recurrent. This establishes the claim that the first term on the right-hand side of \eqref{rested_arms_eq:liminf_M_{hh'}(n)/n_pos_drift_2} is strictly positive. A similar argument may be followed to establish the positivity of the second term on the right-hand side of \eqref{rested_arms_eq:liminf_M_{hh'}(n)/n_pos_drift_2}. This completes the proof of the proposition.
%
%\section{Proof of Proposition \ref{rested_arms_prop:lim_M_h(n)/n_correct_drift}}\label{rested_arms_appndx:proof_of_prop_lim_M_h(n)/n_correct_drift}
%Before we present the proof of Proposition \ref{rested_arms_prop:lim_M_h(n)/n_correct_drift}, we show that the odd arm chosen by the non-stopping version of policy $\pi^\star(L,\delta)$ is indeed the correct one. Further, we show that the arm selection frequencies under the same policy converge to the respective optimal values given in \eqref{rested_arms_eq:lambda^*(h,P_1,P_2)}.
%\begin{prop}
%	Let $C=(h,P_1,P_2)$ denote the underlying configuration of the arms. Consider the non-stopping version of policy $\pi^\star(L,\delta)$. Then, the following convergences hold \text{almost surely} as $n\to\infty$.
%	\begingroup\allowdisplaybreaks\begin{align}
%	&\theta(n)\to h,\label{rested_arms_eq:h^*_converges_to_h}\\
%	&\lambda_{opt}(\theta(n),\hat{P}^n_{\theta(n),1},\hat{P}^n_{\theta(n),2})\to\lambda_{opt}(h,P_1,P_2),\label{rested_arms_eq:lambda^*_converges_to_true_lambda^*_values}\\
%	&\frac{N_a(n)}{n}\to \lambda^*_\delta(h,P_1,P_2)(a)\text{ for all }a\in\mathcal{A},\label{rested_arms_eq:arm_frequencies_converge}\\
%	&P_{n}(j|i)\to P_\delta(j|i)\text{ for all }i,j\in\mathcal{S},\label{rested_arms_eq:P_n_converges_to_P}
%	\end{align}\endgroup
%where for each $a\in\mathcal{A}$, the quantity $\lambda_{\delta}^*(h,P_1,P_2)(a)$ in \eqref{rested_arms_eq:arm_frequencies_converge} is given by
%	\begin{equation}
%	\lambda^*_\delta(h,P_1,P_2)(a)=\frac{\delta}{K}+(1-\delta)\lambda_{opt}(h,P_1,P_2)(a),\label{rested_arms_eq:lambda_delta(a)}
%\end{equation}
%the term $P_n(j|i)$ in \eqref{rested_arms_eq:P_n_converges_to_P} is as defined in \eqref{rested_arms_eq:P_n(j|i)}, and the term $P_\delta(j|i)$ in \eqref{rested_arms_eq:P_n_converges_to_P} is as defined in the statement of Proposition \ref{rested_arms_prop:lim_M_h(n)/n_correct_drift}.
%\end{prop}
%\begin{proof}
%From the sequence of inequalities in \eqref{rested_arms_eq:limsup_M_{h'}(n)_less_than_0}, it follows that $\theta(n)=h$ \text{almost surely} for all sufficiently large values of $n$. This establishes \eqref{rested_arms_eq:h^*_converges_to_h}, which in turn implies that
%\begingroup\allowdisplaybreaks\begin{align}
%\lambda_{opt}(\theta(n),\hat{P}^n_{\theta(n),1},\hat{P}^n_{\theta(n),2})&\to\lambda_{opt}(h,\hat{P}^n_{h,1},\hat{P}^n_{h,2})
%&\to \lambda_{opt}(h,P_1,P_2),
%\end{align}\endgroup
%where in writing the last line above, we use the convergence of the maximum likelihood estimates shown in \eqref{rested_arms_eq:convergence_of_ml_estimates}, and the fact that $\lambda_{opt}(h,P,Q)$ is jointly continuous in the pair $(P,Q)$, a fact that follows from Berge's Maximum Theorem \cite{Ausubel1993}. This establishes \eqref{rested_arms_eq:lambda^*_converges_to_true_lambda^*_values}.
%
%We now proceed to show \eqref{rested_arms_eq:arm_frequencies_converge}. Towards this, we observe that from the convergence in \eqref{rested_arms_eq:lambda^*_converges_to_true_lambda^*_values}, we have
%\begingroup\allowdisplaybreaks\begin{align}
%	&P(A_{n+1}=a|A^n,\bar{X}^n)
%	&\hspace{0.5cm}=\frac{\delta}{K}+(1-\delta)\,\lambda_{opt}(\theta(n),\hat{P}^n_{\theta(n),1},\hat{P}^n_{\theta(n),2})(a).
%	&\hspace{0.5cm}\to \frac{\delta}{K}+(1-\delta)\lambda_{opt}(h,P_1,P_2)(a).
%\end{align}\endgroup
%We revisit the quantity $S_a(n)$ defined in \eqref{rested_arms_eq:S_a(n)}, and use the fact that $\frac{S_a(n)}{n}\to 0$ \text{almost surely} as $n\to\infty$ to obtain
%\begingroup\allowdisplaybreaks\begin{align}
%	\frac{N_a(n)}{n}&\to \frac{1}{n}\sum\limits_{t=0}^{n-1}P(A_{t+1}=a|A^t,\bar{X}^t)
%	&\to \frac{\delta}{K}+(1-\delta)\lambda_{opt}(h,P_1,P_2)(a).
%\end{align}\endgroup
%
%This establishes \eqref{rested_arms_eq:arm_frequencies_converge}.
%
%Defining
%\begin{equation}
%	\alpha_n=\frac{N_h(n)}{n},\quad \beta_n=\frac{\sum\limits_{a\neq h,h'}N_a(n)}{n},\label{rested_arms_eq:alpha_n_beta_n}
%\end{equation}
%we note that the convergence in \eqref{rested_arms_eq:arm_frequencies_converge} implies in particular that
%\begingroup\allowdisplaybreaks\begin{align}
%	\alpha_n &\to \lambda^*_\delta(h,P_1,P_2)(h)=\frac{\delta}{K}+(1-\delta)\lambda_{opt}(h),
%	\beta_n &\to (K-2)\left(\frac{\delta}{K}+(1-\delta)\frac{1-\lambda_{opt}(h)}{K-1}\right)
%	&=\left(\frac{K-2}{K-1}\right)\bigg\lbrace1-\left(\frac{\delta}{K}+(1-\delta)\lambda_{opt}(h)\right)\bigg\rbrace
%	&=\left(\frac{K-2}{K-1}\right)(1-\lambda_\delta^*(h)).\label{rested_arms_eq:alpha_n_beta_n_convergence}
%\end{align}\endgroup
%Taking limits as $n\to\infty$ on both sides of \eqref{rested_arms_eq:P_n(j|i)}, and using the above limits for $\alpha_n$ and $\beta_n$,  we get the convergence in \eqref{rested_arms_eq:P_n_converges_to_P}, hence completing the proof of the proposition.
%\end{proof}
%
%We now proceed towards proving Proposition \ref{rested_arms_prop:lim_M_h(n)/n_correct_drift}. We recall from \eqref{rested_arms_eq:liminf_M_{hh'}(n)/n_strictly_positive} that
%\begingroup\allowdisplaybreaks\begin{align}
%	&\liminf_{n\to\infty}\frac{M_{hh'}(n)}{n}\nonumber
%	\\
%	&\geq \liminf\limits_{n\to\infty}\alpha_n D(P_1||P_n|\mu_1)+\liminf\limits_{n\to\infty}\beta_n D(P_2||P_n|\mu_2)
%	&=\lambda_\delta^*(h)D(P_1||P_\delta|\mu_1)
%	&\hspace{3cm}+\left(\frac{K-2}{K-1}\right)(1-\lambda_\delta^*(h))D(P_2||P_\delta||\mu_2),\label{rested_arms_eq:liminf_M_{hh'}(n)_D_delta}
%\end{align}\endgroup
%where the terms $\alpha_n$ and $\beta_n$ are as given in \eqref{rested_arms_eq:alpha_n_beta_n}.
%
%Using Varadhan's integral lemma to write
%\begingroup\allowdisplaybreaks\begin{align}
%    	&\limsup\limits_{n\to\infty}\frac{1}{n}\log B((N_h(n,i,j)+1)_{j\in\mathcal{S}})
%    	&\leq \limsup\limits_{n\to\infty}\frac{N_h(n)}{n}\mu_1(i)\sup\limits_{\{(z_j)_{j\in\mathcal{S}}\}}\sum\limits_{j\in\mathcal{S}}P_1(j|i)\log z_j,
%    	&=\lim\limits_{n\to\infty}\frac{N_h(n)}{n}\mu_1(i)(-H(P_1(\cdot|i))),\label{rested_arms_eq:limsup_t_2(n)/n_3}
%    \end{align}\endgroup
%    and following similar steps leading to \eqref{rested_arms_eq:liminf_t_2(n)/n_final}, we obtain
%    \begingroup\allowdisplaybreaks\begin{align}
%    &\limsup_{n\to\infty}\frac{M_{hh'}(n)}{n}\nonumber
%	\\
%	&\leq \lim\limits_{n\to\infty}\alpha_n D(P_1||P_n|\mu_1)+\lim\limits_{n\to\infty}\beta_n D(P_2||P_n|\mu_2)
%	&=\lambda_\delta^*(h)D(P_1||P_\delta|\mu_1)
%	&\hspace{3cm}+\left(\frac{K-2}{K-1}\right)(1-\lambda_\delta^*(h))D(P_2||P_\delta||\mu_2).\label{rested_arms_eq:limsup_M_{hh'}(n)_D_delta}
%    \end{align}\endgroup
%Combining \eqref{rested_arms_eq:liminf_M_{hh'}(n)_D_delta} and \eqref{rested_arms_eq:limsup_M_{hh'}(n)_D_delta}, we get the desired result.
%
%\section{Proof of Lemma \ref{rested_arms_lemma:stopping_time_of_policy_goes_to_infinity}}\label{rested_arms_appndx:stopping_time_of_policy_goes_to_infinity}
%Since policy $\pi^\star(L,\delta)$ selects each of the $K$ arms in the first $K$ slots, in order to prove the lemma, we note that it suffices to prove the following statement:
%\begin{equation}
%	\lim\limits_{L\to\infty}P(\tau(\pi^\star(L,\delta))\leq m)=0\text{ for all $m\geq K$}.
%\end{equation}
%Towards this, we fix $m\geq K$. Then, we have
%\begingroup\allowdisplaybreaks\begin{align}
%	&\limsup\limits_{L\to\infty}P(\tau(\pi^\star(L,\delta))\leq m)
%	&=\limsup\limits_{L\to\infty}P\bigg(\exists~0\leq n\leq m \text{ and }h\in\mathcal{A}
%	&\hspace{2cm}\text{ such that }M_h(n)>\log((K-1)L)\bigg)
%	&\leq \limsup\limits_{L\to\infty}\sum\limits_{h\in\mathcal{A}}\sum\limits_{n=0}^{m}P(M_h(n)>\log((K-1)L))
%	&\leq \limsup\limits_{L\to\infty}\frac{1}{\log((K-1)L)}\sum\limits_{h\in\mathcal{A}}\sum\limits_{n=0}^{m}E[M_h(n)],\label{rested_arms_eq:stop_time_goes_to_infty_1}
%\end{align}\endgroup
%where the first inequality above follows from the union bound, and the second inequality follows from Markov's inequality.
%
%We now show that for each $m\in\{0,1,\ldots,n\}$, the expectation term inside the summation in \eqref{rested_arms_eq:stop_time_goes_to_infty_1} is finite. Towards this, we have
%\begingroup\allowdisplaybreaks\begin{align}
%	M_h(n)&=\log\left(\frac{f(A^n,\bar{X}^n|H_h)}{\max\limits_{h'\neq h}\hat{f}(A^n,\bar{X}^n|H_{h'})}\right)
%	&\leq \log\left(\frac{f(A^n,\bar{X}^n|H_h)}{\hat{f}(A^n,\bar{X}^n|H_{h'})}\right)\text{ for all }h'\neq h.\label{rested_arms_eq:stop_time_goes_to_infty_1}
%\end{align}\endgroup
%Fix $h'\neq h$. Then, we have
%\begingroup\allowdisplaybreaks\begin{align}
%	\log\left(\frac{f(A^n,\bar{X}^n|H_h)}{\hat{f}(A^n,\bar{X}^n|H_{h'})}\right)=S_1(n)+S_2(n)+S_3(n)+S_4(n),\label{rested_arms_eq:stop_time_goes_to_infty_2}
%\end{align}\endgroup
%where:
%\begin{enumerate}
%	\item The term $S_1(n)$ is given by
%	\begin{equation}
%		S_1(n)=\sum\limits_{i,j\in\mathcal{S}}N_h(n,i,j)\log\frac{N_h(n,i,j)}{N_h(n,i)}.\label{rested_arms_eq:S_1(n)}
%	\end{equation}
%	\item The term $S_2(n)$ is given by
%    \begin{equation}
%		S_2(n)=\sum\limits_{i,j\in\mathcal{S}}\sum\limits_{a\neq h}N_a(n,i,j)\log\frac{\sum\limits_{a\neq h}N_a(n,i,j)}{\sum\limits_{a\neq h}N_a(n,i)}.\label{rested_arms_eq:S_2(n)}
%		\end{equation}
%	\item The term $S_3(n)$ is given by
%    \begin{equation}
%		S_3(n)=-\sum\limits_{i,j\in\mathcal{S}}N_{h'}(n,i,j)\log\frac{N_{h'}(n,i,j)}{N_{h'}(n,i)}.\label{rested_arms_eq:S_3(n)}
%	\end{equation}
%	\item The term $S_2(n)$ is given by
%    \begin{equation}
%		S_4(n)=-\sum\limits_{i,j\in\mathcal{S}}\sum\limits_{a\neq h'}N_a(n,i,j)\log\frac{\sum\limits_{a\neq h'}N_a(n,i,j)}{\sum\limits_{a\neq h'}N_a(n,i)}.\label{rested_arms_eq:S_4(n)}
%		\end{equation}
%\end{enumerate}
%We now obtain an almost sure upper bound for \eqref{rested_arms_eq:stop_time_goes_to_infty_2}. We recognise that $S_1(n)\leq 0$ and $S_2(n)\leq 0$, and therefore drop these terms from \eqref{rested_arms_eq:stop_time_goes_to_infty_2}. Also, we note that $S_3(n)$ may be written as follows: for each $i\in\mathcal{S}$, let $$A(i)=(N_{h'}(n,i,j)/N_{h'}(n,i))_{j\in\mathcal{S}}$$ denote the probability vector corresponding to state $i$. Then, denoting the Shannon entropy of $A(i)$ by $H(A(i))$, we have
%\begingroup\allowdisplaybreaks\begin{align}
%	S_3(n)&=(N_{h'}(n)-1)\sum\limits_{i\in\mathcal{S}}\frac{N_{h'}(n,i)}{N_{h'}(n)-1}H(A(i))
%	&\leq (N_{h'}(n)-1)H\left(\sum\limits_{i\in\mathcal{S}}\frac{N_{h'}(n,i)}{N_{h'}(n)-1}A(i)\right)
%	&\leq N_{h'}(n) \log|\mathcal{S}|,\label{rested_arms_eq:S_3(n)_upper_bound}
%\end{align}\endgroup
%where the first inequality above follows from the concavity of the entropy function $H(\cdot)$, and the second inequality follows by noting that the Shannon entropy of a probability distribution on an alphabet of size $R$ is at most $\log R$.
%
%Using similar arguments, we get
%\begingroup\allowdisplaybreaks\begin{align}
%	S_4(n)&\leq \left(\sum\limits_{a\neq h'}N_a(n)\right)\log|\mathcal{S}|.\label{rested_arms_eq:S_4(n)_upper_bound}
%\end{align}\endgroup
%
%Using these results in \eqref{rested_arms_eq:stop_time_goes_to_infty_2}, we get
%\begingroup\allowdisplaybreaks\begin{align}
%	M_h(n)\leq (n+1)\log |\mathcal{S}|\text{ \text{almost surely}},\label{rested_arms_eq:upper_bound_on_M_h(n)}
%\end{align}\endgroup
%from which it follows that
%\begingroup\allowdisplaybreaks\begin{align}
%	&\limsup\limits_{L\to\infty}P(\tau(\pi^\star(L,\delta))\leq m)
%	&\leq \limsup\limits_{L\to\infty}\frac{1}{\log((K-1)L)}\sum\limits_{h\in\mathcal{A}}\sum\limits_{n=0}^{m}(n+1)\log |\mathcal{S}|,\label{rested_arms_eq:stop_time_goes_to_infty_1}
%	&=0.
%\end{align}\endgroup
%This completes the proof of the lemma.
%
%\section{Proof of Proposition \ref{rested_arms_prop:upper_bound}}\label{rested_arms_appndx:proof_of_upper_bound}
%From Proposition \ref{rested_arms_prop:lim_M_h(n)/n_correct_drift}, when $C=(h,P_1,P_2)$ denotes the underlying configuration of the arms, we know that $M_h(n)\to\infty$ as $n\to\infty$. We now provide a strengthening of this result in the following lemma, as a first step towards showing the upper bound in \eqref{rested_arms_eq:upper_bound}.
%\begin{lemma}\label{rested_arms_lemma:exp_bound}
%	Let $C=(h,P_1,P_2)$ denote the underlying configuration of the arms. Fix $L>1$, and consider policy $\pi^\star(L,\delta)$. Then, there exist constants $\theta>0$ and $0<B<\infty$ independent of $L$ such that for all sufficiently large values of $n$, we have
%	\begin{equation}
%		P(M_h(n)<\log((K-1)L))\leq Be^{-\theta n}.\label{rested_arms_eq:exponential_bound}
%	\end{equation}
%\end{lemma}
%\begin{proof}
%Since
%\begingroup\allowdisplaybreaks\begin{align}
%	&P(M_h(n)<\log((K-1)L))
%	&=P\left(\min\limits_{h'\neq h}M_{hh'}(n)<\log((K-1)L)\right)
%	&\leq \sum\limits_{h'\neq h}P\left(M_{hh'}(n)<\log((K-1)L)\right),\label{rested_arms_eq:exp_bound_1}
%\end{align}\endgroup
%in order to prove the lemma, it suffices to show that each term inside the summation in \eqref{rested_arms_eq:exp_bound_1} is exponentially bounded. Towards this, we fix $h'\neq h$. Then, for any choice of $\epsilon>0$, using \eqref{rested_arms_eq:M_{hh'}(n)} and triangle inequality, we have
%\begingroup\allowdisplaybreaks\begin{align}
%	&P(M_{hh'}(n)<\log((K-1)L))
%	% &=P(T_1+T_2(n)+T_3(n)+T_4(n)+T_5(n)<\log((K-1)L))
%	&\leq P\left(\frac{T_1(n)}{n}<-\epsilon\right)\label{rested_arms_eq:exp_bound_t_1/n}\\
%	&+P\left(\frac{T_2(n)}{n}-\frac{N_h(n)}{n}\sum\limits_{i\in\mathcal{S}}\mu_1(i)(-H(P_1(\cdot|i)))<-\epsilon\right)\label{rested_arms_eq:exp_bound_t_2(n)/n}\\
%	&+P\left(\frac{T_3(n)}{n}-\frac{\sum\limits_{a\neq h}N_a(n)}{n}\sum\limits_{i\in\mathcal{S}}\mu_2(i)(-H(P_2(\cdot|i)))<-\epsilon\right)\label{rested_arms_eq:exp_bound_t_3(n)/n}\\
%	&+P\left(\frac{T_4(n)}{n}-\frac{N_{h'}(n)}{n}\sum\limits_{i\in\mathcal{S}}\mu_2(i)H(P_2(\cdot|i))<-\epsilon\right)\label{rested_arms_eq:exp_bound_t_4(n)/n}\\
%	&+P\left(\frac{T_5(n)}{n}-\sum\limits_{i\in\mathcal{S}}(\alpha_n\mu_1(i)+\beta_n\mu_2(i))H(P_\delta(\cdot|i))<-\epsilon\right)\label{rested_arms_eq:exp_bound_t_5(n)/n}\\
%	&+P\bigg(\alpha_n D(P_1||P_\delta|\mu_1)+\beta_n D(P_2||P_\delta|\mu_2)-5\epsilon
%	&\hspace{5.5cm}<\frac{\log((K-1)L)}{n}\bigg),\label{rested_arms_eq:exp_bound_compensatory_term}
%\end{align}\endgroup	
%where in \eqref{rested_arms_eq:exp_bound_t_2(n)/n}, the term $H(P_1(\cdot|i))$ refers to the Shannon entropy of the probability distribution $(P_1(j|i))_{j\in\mathcal{S}}$ on set $\mathcal{S}$, defined as
%\begingroup\allowdisplaybreaks\begin{align}
%H(P_1(\cdot|i))=\sum\limits_{j\in\mathcal{S}}P_1(j|i)\log\frac{1}{P_1(j|i)};\label{rested_arms_eq:H(P_1(cdot|i))}	
%\end{align}\endgroup
%the term $H(P_2(\cdot|i))$ in \eqref{rested_arms_eq:exp_bound_t_2(n)/n} is defined similarly. Also, in \eqref{rested_arms_eq:exp_bound_t_5(n)/n} and \eqref{rested_arms_eq:exp_bound_compensatory_term} above, the terms $\alpha_n$ and $\beta_n$ are as defined in \eqref{rested_arms_eq:alpha_n_beta_n}.
%
%We now obtain a bound for the terms in \eqref{rested_arms_eq:exp_bound_t_1/n}-\eqref{rested_arms_eq:exp_bound_compensatory_term}.
%
%\begin{enumerate}
%\item We begin by showing an exponential upper bound for \eqref{rested_arms_eq:exp_bound_compensatory_term}. We choose $0<\epsilon'<\frac{1}{3}\cdot \frac{\delta}{2K}$, and then select $\epsilon>0$ such that the following holds:
%\begingroup\allowdisplaybreaks\begin{align}
%	&\frac{\delta}{2K}(1-\epsilon')\bigg(D(P_1||P_\delta|\mu_1)+D(P_1||P_\delta|\mu_1)\bigg)-5\epsilon
%	&>\frac{1}{3}\cdot\frac{\delta}{2K}\bigg(D(P_1||P_\delta|\mu_1)+D(P_1||P_\delta|\mu_1)\bigg).\label{rested_arms_eq:select_epsilon'}
%\end{align}\endgroup
%Then, we observe that for all $n$ satisfying
%\begingroup\allowdisplaybreaks\begin{align}
%	n\geq \frac{3\log((K-1)L)}{\frac{\delta}{2K}\bigg(D(P_1||P_\delta|\mu_1)+D(P_1||P_\delta|\mu_1)\bigg)},\label{rested_arms_eq:n_geq_u(L)}
%\end{align}\endgroup
%we have
%\begingroup\allowdisplaybreaks\begin{align}
%	&P\bigg(\alpha_n D(P_1||P_\delta|\mu_1)+\beta_n D(P_2||P_\delta|\mu_2)-5\epsilon
%	&\hspace{4cm}<\frac{\log((K-1)L)}{n},
%	&\hspace{1cm}\frac{N_a(n)}{n}>\frac{\delta}{2K}(1-\epsilon')\text{ for all }a\in\mathcal{A}\bigg)=0.
%\end{align}\endgroup
%Denoting the term on the right-hand side of \eqref{rested_arms_eq:n_geq_u(L)} by $l(\delta,L)$, we then have the following upper bound for \eqref{rested_arms_eq:exp_bound_compensatory_term} for all $n\geq l(L,\delta)$:
%\begingroup\allowdisplaybreaks\begin{align}
%	&P\bigg(\alpha_n D(P_1||P_\delta|\mu_1)+\beta_n D(P_2||P_\delta|\mu_2)-5\epsilon
%	&\hspace{4cm}<\frac{\log((K-1)L)}{n}\bigg)
%	&\leq P\left(\frac{N_h(n)}{n}\leq\frac{\delta}{2K}(1-\epsilon')\right)
%	&\hspace{2cm}+\sum\limits_{a\neq h} P\left(\frac{N_a(n)}{n}\leq\frac{\delta}{2K}(1-\epsilon')\right).
%\end{align}\endgroup	
%Noting that for each $a\in\mathcal{A}$, the sequence $\left(N_a(n)-n\frac{\delta}{2K}\right)_{n\geq 0}$ is a bounded difference submartingale, we use the Azuma-Hoeffding inequality to obtain
%\begingroup\allowdisplaybreaks\begin{align}
%	&P\bigg(\alpha_n D(P_1||P_\delta|\mu_1)+\beta_n D(P_2||P_\delta|\mu_2)-5\epsilon
%	&\hspace{4cm}<\frac{\log((K-1)L)}{n}\bigg)
%	&\leq 2\exp\left(-\frac{n\epsilon'}{2}\right).
%\end{align}\endgroup
%
%\item We now turn attention to \eqref{rested_arms_eq:exp_bound_t_4(n)/n}, which we upper bound for all $n\geq l(L,\delta)$ as follows:
%\begingroup\allowdisplaybreaks\begin{align}
%	&P\left(\frac{T_4(n)}{n}-\frac{N_{h'}(n)}{n}\sum\limits_{i\in\mathcal{S}}\mu_2(i)H(P_2(\cdot|i))<-\epsilon\right)
%	&=P\bigg(\frac{N_{h'}(n)}{n}\bigg\lbrace\sum\limits_{i\in\mathcal{S}}\frac{N_{h'}(n,i)}{N_{h'}(n)}H\left(\frac{N_{h'}(n,i,\cdot)}{N_{h'}(n,i)}\right)
%	&\hspace{3cm}-\mu_2(i)H(P_2(\cdot|i))\bigg\rbrace<-\epsilon\bigg)
%	&\leq P\bigg(\frac{N_{h'}(n)}{n}\bigg\lbrace\sum\limits_{i\in\mathcal{S}}\frac{N_{h'}(n,i)}{N_{h'}(n)}H\left(\frac{N_{h'}(n,i,\cdot)}{N_{h'}(n,i)}\right)
%	&\hspace{3cm}-\sum\limits_{i\in\mathcal{S}}\mu_2(i)H(P_2(\cdot|i))\bigg\rbrace<-\epsilon,
%	&\hspace{1cm}\frac{N_a(n)}{n}>\frac{\delta}{2K}(1-\epsilon')\text{ for all }a\in\mathcal{A}\bigg)
%	&+\sum\limits_{a\in\mathcal{A}}P\left(\frac{N_a(n)}{n}\leq\frac{\delta}{2K}(1-\epsilon')\right),\label{rested_arms_eq:exp_bound_t_4(n)/n_1}
%\end{align}\endgroup
%where in the above set of equations, $0<\epsilon'<\frac{1}{3}\cdot\frac{\delta}{2K}$. From the validity of \eqref{rested_arms_eq:N_a(n)_lies_between_two_quantities} for all sufficiently large values of $n$, and Azuma-Hoeffding inequality for bounded difference submartingales, we know that each term inside the summation in \eqref{rested_arms_eq:exp_bound_t_4(n)/n_1} is exponentially bounded. The first term in \eqref{rested_arms_eq:exp_bound_t_4(n)/n_1} may be written as
%\begingroup\allowdisplaybreaks\begin{align}
%	&P\bigg(\frac{N_{h'}(n)}{n}\bigg\lbrace\sum\limits_{i\in\mathcal{S}}\frac{N_{h'}(n,i)}{N_{h'}(n)}H\left(\frac{N_{h'}(n,i,\cdot)}{N_{h'}(n,i)}\right)
%	&\hspace{3cm}-\sum\limits_{i\in\mathcal{S}}\mu_2(i)H(P_2(\cdot|i))\bigg\rbrace<-\epsilon,
%	&\hspace{1cm}\frac{N_a(n)}{n}>\frac{\delta}{2K}(1-\epsilon')\text{ for all }a\in\mathcal{A}\bigg)
%	&\leq P\bigg(\bigg\lbrace\sum\limits_{i\in\mathcal{S}}\frac{N_{h'}(n,i)}{N_{h'}(n)}H\left(\frac{N_{h'}(n,i,\cdot)}{N_{h'}(n,i)}\right)
%	&\hspace{1cm}-\sum\limits_{i\in\mathcal{S}}\mu_2(i)H(P_2(\cdot|i))\bigg\rbrace<-\frac{\epsilon}{\frac{\delta}{2K}(1-\epsilon')},
%	&\hspace{1cm}\frac{N_a(n)}{n}>\frac{\delta}{2K}(1-\epsilon')\text{ for all }a\in\mathcal{A}\bigg).\label{rested_arms_eq:exp_bound_t_4(n)/n_2}
%\end{align}\endgroup
%By Lemma \ref{rested_arms_lemma:convergence_of_ML_estimates}, we have the following almost sure convergences as $n\to\infty$:
%\begingroup\allowdisplaybreaks\begin{align}
%	\frac{N_{h'}(n,i,j)}{N_{h'}(n,i)}&\to P_2(j|i),\text{ for all }i,j\in\mathcal{S},
%	\frac{N_{h'}(n,i)}{N_{h'}(n)}&\to \mu_2(i),\text{ for all }i\in\mathcal{S}.
%\end{align}\endgroup
%%\begingroup\allowdisplaybreaks\begin{align}
%%	\sum\limits_{i\in\mathcal{S}}\bigg[\frac{N_{h'}(n,i)}{N_{h'}(n)}H\left(\frac{N_{h'}(n,i,\cdot)}{N_{h'}(n,i)}\right)-\mu_2(i)H(P_2(\cdot|i))\bigg]
%%\end{align}\endgroup
%Using above convergences and the continuity of Shannon entropy $H(\cdot)$, we get that there exist $\delta_1=\delta_1(\epsilon)$ and $\delta_2=\delta_2(\epsilon)$ such that the probability in \eqref{rested_arms_eq:exp_bound_t_4(n)/n_2} may be upper bounded by
%\begingroup\allowdisplaybreaks\begin{align}
%	&P\bigg(\exists~i,j\in\mathcal{S}\text{ such that }
%	&\hspace{2cm}\frac{N_{h'}(n,i,j)}{N_{h'}(n,i)}>\delta_1,\frac{N_{h'}(n,i)}{N_{h'}(n)}>\delta_2,
%	&\hspace{1cm}\frac{N_a(n)}{n}>\frac{\delta}{2K}(1-\epsilon')\text{ for all }a\in\mathcal{A}\bigg).\label{rested_arms_eq:exp_bound_t_4(n)/n_3}
%\end{align}\endgroup
%We may then express \eqref{rested_arms_eq:exp_bound_t_4(n)/n_3} as a probability of deviation of martingale difference sequences from zero, which may be exponentially bounded by using results from \cite[Theorem 1.2A]{Victor1999}.
%
%
%\item We now upper bound the term in \eqref{rested_arms_eq:exp_bound_t_2(n)/n}. Towards this, we first pick $\epsilon_1>0$ satisfying
%\begin{equation}
%	0<\epsilon_1\leq \frac{\epsilon}{1+2\sum\limits_{i\in\mathcal{S}}\mu_1(i)H(P_1(\cdot|i))}.\label{rested_arms_eq:epsilon_1}
%\end{equation}
%Then, noting that under the underlying configuration $C=(h,P_1,P_2)$, the almost sure convergences
%\begingroup\allowdisplaybreaks\begin{align}
%\frac{N_h(n)}{n}&\to \lambda_\delta^*(h)\text{ \text{almost surely}},
%\frac{N_h(n,i,j)}{N_h(n)}&\to \mu_1(i)P_1(j|i)\text{ \text{almost surely}},	
%\end{align}\endgroup
%hold for all $i,j\in\mathcal{S}$, following the steps leading up to \eqref{rested_arms_eq:liminf_t_2(n)/n_final}, we note that for every choice of $\epsilon'>0$, there exists $M=M(\epsilon')$ such that \eqref{rested_arms_eq:liminf_t_2(n)/n_final} holds. We now choose $\epsilon'$ such that
%\begingroup\allowdisplaybreaks\begin{align}
%	&\frac{T_{2}(n)}{n}
%    	&\geq \frac{N_h(n)}{n}\bigg\lbrace\bigg[\sum\limits_{i\in\mathcal{S}}\sum\limits_{j\in\mathcal{S}}(\mu_1(i)P_1(j|i)+\epsilon')
%    	&\hspace{3.5cm}\log\frac{\mu_1(i)P_1(j|i)+\epsilon'}{\mu_1(i)+\epsilon'|\mathcal{S}|}\bigg]-\epsilon'\bigg\rbrace
%    	&\geq \frac{N_h(n)}{n}\bigg(\sum\limits_{i\in\mathcal{S}}\mu_1(i)(-H(P_1(\cdot|i)))\bigg)-\epsilon_1,
%\end{align}\endgroup
%for all $n\geq M$, where the last line above follows by continuity of the term within braces as a function of $\epsilon'$ and using $N_h(n)/n\leq 1$. Using the above,
%we write \eqref{rested_arms_eq:exp_bound_t_2(n)/n} as
%%we first pick $\epsilon_1>0$ and $\epsilon_2>0$ such that the following inequalities hold \text{almost surely} for all $i,j\in\mathcal{S}$:
%%\begingroup\allowdisplaybreaks\begin{align}
%%	\sup\limits_{n\geq l(L,\delta)}\bigg\vert\frac{N_h(n)}{n}-\lambda_\delta^*(h)\bigg\vert\leq\epsilon_1,
%%	\sup\limits_{n\geq l(L,\delta)}\bigg\vert\frac{N_h(n,i,j)}{N_h(n)}-\mu_1(i)P_1(j|i)\bigg\vert\leq\epsilon_2.
%%\end{align}\endgroup
%%Then, it follows that for all $n\geq l(L,\delta)$, we may express \eqref{rested_arms_eq:exp_bound_t_2(n)/n} as
%\begingroup\allowdisplaybreaks\begin{align}
%	&P\left(\frac{T_2(n)}{n}-\frac{N_h(n)}{n}\sum\limits_{i\in\mathcal{S}}\mu_1(i)(-H(P_1(\cdot|i)))<-\epsilon\right)
%	&\leq P\bigg(\frac{T_2(n)}{n}-\frac{N_h(n)}{n}\sum\limits_{i\in\mathcal{S}}\mu_1(i)(-H(P_1(\cdot|i)))<-\epsilon,
%	&\hspace{3cm}\bigg\vert\frac{N_h(n)}{n}-\lambda_\delta^*(h)\bigg\vert\leq\epsilon_1,
%	&\hspace{0.5cm}\bigg\vert\frac{N_h(n,i,j)}{N_h(n)}-\mu_1(i)P_1(j|i)\bigg\vert\leq\epsilon'\text{ for all }i,j\in\mathcal{S}\bigg)
%	&+P\bigg(\bigg\vert\frac{N_h(n)}{n}-\lambda_\delta^*(h)\bigg\vert>\epsilon_1\bigg)
%	&+\sum\limits_{i,j\in\mathcal{S}}P\bigg(\bigg\vert\frac{N_h(n,i,j)}{N_h(n)}-\mu_1(i)P_1(j|i)\bigg\vert>\epsilon'\bigg).\label{rested_arms_eq:exp_bound_t_2(n)/n_1}
%\end{align}\endgroup
%We now focus on the first term in \eqref{rested_arms_eq:exp_bound_t_2(n)/n_1}, and notice that for all $n\geq M$, this term may be upper bounded as
%\begingroup\allowdisplaybreaks\begin{align}
%	&P\bigg((\lambda_\delta^*(h)+\epsilon_1)\sum\limits_{i\in\mathcal{S}}\mu_1(i)(-H(P_1(\cdot|i)))-\epsilon_1
%	&\hspace{1cm}<-\epsilon+(\lambda_\delta^*(h)-\epsilon_1)\sum\limits_{i\in\mathcal{S}}\mu_1(i)(-H(P_1(\cdot|i)))\bigg)
%	&\leq P\bigg(\epsilon_1>\frac{\epsilon}{1+2\sum\limits_{i\in\mathcal{S}}\mu_1(i)H(P_1(\cdot|i))}\bigg)
%	&=0,
%\end{align}\endgroup
%where the last line follows from the choice of $\epsilon_1$ in \eqref{rested_arms_eq:epsilon_1}. Exponential bounds for the remaining terms in \eqref{rested_arms_eq:exp_bound_t_2(n)/n_1} can be obtained similarly as in the analysis of \eqref{rested_arms_eq:exp_bound_t_4(n)/n_2}.
%
%Lastly, for the terms in \eqref{rested_arms_eq:exp_bound_t_1/n}, \eqref{rested_arms_eq:exp_bound_t_3(n)/n} and \eqref{rested_arms_eq:exp_bound_t_5(n)/n}, noting that the left-hand sides of the inequality in all the three terms converge to zero \text{almost surely}, similar procedures as used above for \eqref{rested_arms_eq:exp_bound_t_2(n)/n} and \eqref{rested_arms_eq:exp_bound_t_4(n)/n} may be used to obtain exponential upper bounds.
%%From \eqref{rested_arms_eq:liminf_t_1(n)/n_final}, we note that there exists $\epsilon'>0$ such that
%%\begingroup\allowdisplaybreaks\begin{align}
%%    	&\frac{T_{2}(n)}{n}
%%    	&\geq \frac{N_h(n)}{n}\bigg\lbrace\bigg[\sum\limits_{i\in\mathcal{S}}\sum\limits_{j\in\mathcal{S}}(\mu_1(i)P_1(j|i)+\epsilon')
%%    	&\hspace{3.5cm}\log\frac{\mu_1(i)P_1(j|i)+\epsilon'}{\mu_1(i)+\epsilon'|\mathcal{S}|}\bigg]-\epsilon'\bigg\rbrace
%%    	&\geq \frac{N_h(n)}{n}\sum\limits_{i,j\in\mathcal{S}}\mu_1(i)P_1(j|i)\log P_1(j|i)-\epsilon\label{rested_arms_eq:liminf_t_2(n)/n_final}
%%    \end{align}\endgroup
%\end{enumerate}
%This completes the proof of the lemma.
%\end{proof}
%
%We now use Lemma \ref{rested_arms_lemma:exp_bound} to show the upper bound in \eqref{rested_arms_eq:upper_bound}. We note that a sufficient condition to show the result in \eqref{rested_arms_eq:upper_bound} for $\pi=\pi^\star(L,\delta)$ is to show that
%\begin{equation}
%	\limsup\limits_{L\to\infty}E\bigg[\exp\bigg(\frac{\tau(\pi)}{\log L}\bigg)\bigg]<\infty.
%\end{equation}
%Towards this, let $l(L,\delta)$ be the quantity on the right-hand side of \eqref{rested_arms_eq:n_geq_u(L)}. Let $C=(h,P_1,P_2)$ be the underlying configuration of the arms. Further, let $\pi^\star_h(L,\delta)=\pi^\star_h(L,\delta)$ denote the version of policy $\pi^\star(L,\delta)$ that stops and declares $h$ as the true index of the odd arm. Let
%\begin{equation}
%	u(L)\coloneqq\exp\bigg(\frac{1+l(L,\delta)}{\log L}\bigg)
%\end{equation}
%Clearly, we have $\tau(\pi^\star_h(L,\delta))\geq \tau(\pi)$. Then,
%\begingroup\allowdisplaybreaks\begin{align}
%	&\limsup\limits_{L\to\infty}E\bigg[\exp\bigg(\frac{\tau(\pi)}{\log L}\bigg)\bigg]
%	&=\limsup\limits_{L\to\infty}\int\limits_{0}^{\infty}P\bigg(\frac{\tau(\pi)}{\log L}>\log x\bigg)\,dx
%	&\leq \limsup\limits_{L\to\infty}\int\limits_{0}^{\infty}P\bigg({\tau(\pi^\star_h(L,\delta))}\geq \lceil(\log x)({\log L})\rceil\bigg)\,dx
%	&\stackrel{(a)}{\leq} \limsup\limits_{L\to\infty}\bigg\lbrace u(L)+\int\limits_{u(L)}^{\infty}P\bigg({\tau(\pi^\star_h(L,\delta))}\geq \lceil(\log x)({\log L})\rceil\bigg)\,dx\bigg\rbrace
%	&\leq \exp\bigg(\frac{3}{\frac{\delta}{2K}(D(P_1||P_\delta|\mu_1)+D(P_2||P_\delta|\mu_2))}\bigg)
%	&+\limsup\limits_{L\to\infty}\sum\limits_{n\geq l(L,\delta)}\exp\bigg(\frac{n+1}{\log L}\bigg)\,P(M_h(n)<\log((K-1)L))
%	&\stackrel{(b)}{\leq} \exp\bigg(\frac{3}{\frac{\delta}{2K}(D(P_1||P_\delta|\mu_1)+D(P_2||P_\delta|\mu_2))}\bigg)
%	&\hspace{2cm}+\limsup\limits_{L\to\infty}\sum\limits_{n\geq l(L,\delta)}B\exp\bigg(\frac{n+1}{\log L}-n \theta\bigg)
%	&<\infty,
%\end{align}\endgroup
%where $(a)$ above follows by upper bounding the probability term by $1$ for all $x\leq u(L)$, and $(b)$ follows from Lemma \ref{rested_arms_lemma:exp_bound}. This completes the proof of the proposition.

\section{Summary}\label{rested_arms_sec:conclusions}
We analysed the asymptotic behaviour of policies for the problem of odd arm identification in a multi-armed bandit setting with rested arms. The asymptotics is in the regime of vanishing error probabilities. We focused on the particular case when the transition probability matrix of neither the odd arm nor the non-odd arm Markov processes is known beforehand. We derived an asymptotic lower bound on {\color{black} the growth rate of} the expected stopping time of any policy as a function of error probability. We identified an explicit configuration-dependent constant in the lower bound. Furthermore, we proposed a scheme that (a) is a modification of the classical GLRT, and (b) uses an idea of ``forced exploration'' from \cite{albert1961sequential}. This scheme takes as inputs two parameters: $L>1$ and $\delta\in(0,1)$. We showed that (a) for a suitable choice of $L$, the probability of error of our scheme can be controlled to any desired tolerance level, and (b) by tuning $\delta$, the performance of our scheme can be made arbitrarily close to the lower bound for vanishingly small error probabilities. In proving the above results, we highlighted how to overcome some of the key challenges that the Markov setting offers in the analysis. Our analysis of the rested Markov setting is a key first step in understanding the difficult case of restless arms.
