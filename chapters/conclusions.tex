\chapter{Conclusions and Future Directions}
\label{ch:conclusions_and_future_work}

The central problem we studied in this thesis was that of identifying an anomalous arm (odd arm) in a multi-armed bandit as quickly as possible subject to an upper bound on the probability of error, when each yields Markov observations. For this problem, we characterised the asymptotic growth rate of the expected time required to find the odd arm, where the asymptotics is as the error probability vanishes. Our analysis was broadly along the following lines. We first derived a problem-instance specific asymptotic lower bound on {\color{black} the growth rate of} the expected time required to find the odd arm. The constant in the lower bound captured the hardness of the problem, and suggested a natural arm selection policy as per whose frequencies the arms must be selected in the long run in order to meet the lower bound. Accordingly, we devised policies that select the arms at the correct frequencies in the long run, consequently leading to upper bounds that matched with the lower bounds. A key, common feature in all of our policies was the idea of forced exploration -- selecting each arm with a strictly positive probability at any given time. We carried out the analysis separately for the settings of rested arms and restless arms when the TPMs of the arms are known beforehand or unknown.

A key ingredient in our analysis of the setting of rested arms was the identification of the fact that for the Markov process of each arm, the long term fraction of entries into a state is equal to the long term fraction of exits from the state (global balance). Both these quantities are in turn equal to the probability of observing the state under the arm's stationary distribution. As for the setting of restless arms, the focal point of our analysis was the recognition of the fact that for each pair of arm delays and last observed states, say $(\underline{d}, \underline{i})$, the long term fraction of entries into $(\underline{d}, \underline{i})$ equals the long term fraction of exits from $(\underline{d}, \underline{i})$. The recognition of such `invariant' quantities lies at the heart of our analysis. We showed that the arm delays and the last observed states form a controlled Markov process. The ergodicity of this process played a crucial role in the analysis. Our `lift' approach of considering the delays and the last observed states of all the arms jointly offered a global perspective in contrast to the local perspective of dealing with the delay and the last observed state of each arm separately as suggested by the prior works.

When the TPMs of the arms are unknown, we used the principle of maximum likelihood estimation to estimate the TPMs. The key challenge here was to show that these estimates converge to their true values in the long-term, i.e., the system is identifiable. In the setting of rested arms, we proved system identifiability by a simple application of the ergodic theorem. This was possible because closed-form expressions for the ML estimates were available. In the setting of restless arms, closed-form expressions for the ML estimates were not available, and therefore the convergence of ML estimates to their true values could not be asserted directly. We proved system identifiability under two sufficient conditions -- a regularity condition on the unknown TPMs and the existence of a continuous selection of near-optimal solutions to the lower bound. 

\vspace{1cm}
\noindent \textbf{Computation of the Lower Bound}  

In the setting of rested arms, we showed that the lower bound can be computed in closed form. We also showed that for any arms configuration, the optimal (unconditional) probability distribution on the arms could be identified in closed-form. 

%Based on the structure of the optimal solution, we constructed a practically feasible policy which eventually samples the arms according to the optimal solution and is therefore asymptotically optimal. 

In the setting of restless arms, we argued that computing the lower bound in closed form is challenging because of the presence of the countably infinite-valued arm {\em delays}. In this case, one must deal with conditional probability distributions on the arms (conditioned on the arm {\em delays} and the {\em last observed states}) which are more complicated to deal with than their unconditional counterparts. An algorithm such as $Q$-learning for restless arms \cite{avrachenkov2020whittle} may be needed to compute the optimal conditional distribution. 
%Practically implementable arm selection schemes in the setting of restless arms may (a) repeatedly sample each arm in a round-robin manner in order to learn the TPM of the arm and in turn use this knowledge to find the odd arm, or (b) forcefully sample an arm if its delay exceeds a certain large constant $M<\infty$. 
%
%In Chapter 4, we argued that the scheme of repeatedly sampling may not sample the arms eventually according to the optimal or a near-optimal solution to the lower bound, and may therefore be suboptimal. Also, the scheme of forcefully sampling an arm with exceedingly large delay may lead to losing the ergodicity of the countable-state Markov decision problem (MDP) arising from the arm delays and the last observed states -- an important property that is crucial for the analysis of the setting of restless arms.


%{\color{black} \textbf{Computation of the Lower Bound} \hspace{0.1cm} In the setting of rested arms, we showed that the lower bound can be computed in closed form. We also showed that for any arms configuration, the optimal (unconditional) probability distribution on the arms could be identified in closed-form. Based on the structure of the optimal solution, we constructed a practically feasible policy which eventually samples the arms according to the optimal solution and is therefore asymptotically optimal. 
%
%In the setting of restless arms, we argued that computing the lower bound in closed form is challenging because of the presence of the countably infinite-valued arm {\em delays}. In this case, one must deal with conditional probability distributions on the arms (conditioned on the arm {\em delays} and the {\em last observed states}) which are more complicated to deal with than their unconditional counterparts. An algorithm such as $Q$-learning for restless arms \cite{avrachenkov2020whittle} may be needed to compute the optimal conditional distribution. Practically implementable arm selection schemes in the setting of restless arms may (a) repeatedly sample each arm in a round-robin manner in order to learn the TPM of the arm and in turn use this knowledge to find the odd arm, or (b) forcefully sample an arm if its delay exceeds a certain large constant $M<\infty$. 
%
%In Chapter \ref{ch:restless_with_unknown_TPMs}, we argued that the scheme of repeatedly sampling may not sample the arms eventually according to the optimal or a near-optimal solution to the lower bound, and may therefore be suboptimal. Also, the scheme of forcefully sampling an arm with exceedingly large delay may lead to losing the ergodicity of the countable-state Markov decision problem (MDP) arising from the arm delays and the last observed states -- an important property that is crucial for the analysis of the setting of restless arms.}

 

\vspace{1cm}

\noindent \textbf{Open questions and future directions} \hspace{0.1cm} We conclude the thesis with some open questions and some possible future directions to explore. 
\begin{itemize}
\item \textbf{Switching costs}: 
Our problem setup does not include any penalty for switching between the arms \cite{krishnasamy2018augmenting, vaidhiyan2015active}. This means that the learning agent is free to switch between the arms any number of times and at any rate, until stoppage. However, in practice, switching between the arms may incur a cost. For example, when a secondary user in a cognitive radio network attempts to find a free channel for transmission, switching between the various channels in the network may be expensive. In such cases, it is important to incorporate switching costs into the problem formulation. A possible formulation is to minimise the total cost, equal to the expected time to find the odd arm plus the overall switching cost, subject to an upper bound on the error probability. A detailed analysis of this problem for the case of independent observations from the arms is available in \cite{prabhu2017learning}. Extensions to the case of Markov observations may be interesting to study.
\item \textbf{The case of no trembling hand}: The focal point in our analysis of the setting of restless arms (Chapters \ref{ch:restless_with_known_TPMs} and \ref{ch:restless_with_unknown_TPMs}) was the ergodicity of the controlled Markov process of arm delays and last observed states  (Lemma \ref{restless_with_known_lem:pi_delta^lambda_is_an_SSRS} of Chapter \ref{ch:restless_with_known_TPMs}). Recall that this ergodicity property was established under the trembling hand model (i.e., the trembling hand parameter $\eta>0$). The trembling hand model may be viewed as a regularisation that gives ergodicity for free. It would be interesting to study the case when $\eta=0$. Our analysis in Section \ref{restless_with_known_sec:case_eta_=_0} shows that merely setting $\eta=0$ in the expressions for the lower and the upper bounds derived for the case $\eta>0$ may not yield the corresponding bounds for the case $\eta=0$. 

For fixed $\eta, \delta>0$ and arms configuration $C$, if $\lambda_{\eta, \delta,C}$ is a $\delta$-optimal solution to the lower bound when the trembling hand parameter is $\eta$ and the true arms configuration is $C$, and $\mu^{\lambda_{\eta, \delta, C}}$ is the stationary distribution associated with the SRS policy $\pi^{\lambda_{\eta, \delta,C}}$, then establishing the {\em tightness} of the family $\{\mu^{\lambda_{\eta, \delta,C}}:\eta>0\}$ of stationary distributions may be crucial in order to show that the lower and the upper bounds for the case $\eta=0$ match \cite[Theorem 3.1, pp. 61]{fleming2012stochastic}. It may be interesting to explore this further.

\item \textbf{Second order asymptotics}: In Chapters \ref{ch:rested_arms}, \ref{ch:restless_with_known_TPMs} and \ref{ch:restless_with_unknown_TPMs}, given an arms configuration $C$, we characterised the value of $$ \lim\limits_{\epsilon\downarrow 0}  \inf\limits_{\pi\in \Pi(\epsilon)} \frac{E^{\pi}[\tau(\pi)|C]}{\log (1/\epsilon)}.$$ Denoting the value of the above quantity as $\alpha=\alpha(C)$, our exposition shows that for all $\pi\in \Pi(\epsilon)$, we have $E^{\pi}[\tau(\pi)|C]\approx \alpha\cdot \log (1/\epsilon)$ for sufficiently small values of $\epsilon$. Alternatively, the first order term in the asymptotic expansion for $E^{\pi}[\tau(\pi)|C]$ is given by $\alpha$. It would be interesting to characterise the second order term of this expansion. By this, we mean the following: for what choice of function $g$ does the expansion $$ E^{\pi}[\tau(\pi)|C]\approx \alpha\cdot \log (1/\epsilon) + \beta\cdot g(\epsilon) + o(g(\epsilon)) $$ hold for all $\pi\in \Pi(\epsilon)$? In the above expansion, $\beta=\beta(C)$ is a strictly positive, arms configuration-dependent constant that captures the second order growth rate of the expected time to find the odd arm. It is instructive to note that in our derivation of the lower bounds, we showed along the lines of \cite{kaufmann2016complexity} that for each $\epsilon>0$, $$ \inf\limits_{\pi\in\Pi(\epsilon)}E^{\pi}[\tau(\pi)|C]\geq \alpha\cdot  d(\epsilon, 1-\epsilon), $$ where $d(\epsilon, 1-\epsilon)=\epsilon \log \frac{\epsilon}{1-\epsilon}+(1-\epsilon)\log \frac{1-\epsilon}{\epsilon}$. Deriving the second and higher order terms in the above lower bound could serve as a starting point towards inferring the function $g$ mentioned above.
  
\item \textbf{General sequential hypothesis testing with Markov observations}: While the focus of this thesis has been on the analysis of the problem of identifying an anomalous arm in a multi-armed bandit with Markov observations from each arm, it may interesting to study more general sequential hypothesis testing problems such as best\footnote{Best arm in the context of Markov observations from the arms may be defined as the arm with the largest stationary mean.} arm identification, multi-bandit best arm identification \cite{scarlett2019overlapping} where the arms are categorised into one or more groups with possible overlaps between the various groups and the goal is to identify the best arm in each group, $L$-anomalous arms identification where the goal is to identify a set of $L> 1$ anomalous arms in a multi-armed bandit, etc, all in the context of Markov observations from the arms. An analysis of best arm identification for the setting of rested arms appears in the recent work of Moulos \cite{moulos2019optimal}. Extensions to the setting of restless arms could be interesting to explore. An analysis of general sequential hypothesis testing problems for the case of independent observations from the arms appears in the recent works \cite{deshmukh2019sequential, prabhu2020sequential}. Extensions to the case of Markov observations from the arms could be interesting to study.

\item \textbf{Sophisticated visual search models}: As noted at the beginning of Chapter \ref{ch:restless_with_unknown_TPMs} and in \cite{naghshvar2013two}, it is likely that in visual search experiments, the human subject scans multiple images at once before narrowing down the search to the oddball image. In particular, the human eye has the flexibility to scan multiple images at once at the cost of not capturing the fine details of each image, or at the other extreme, to focus on one image in order to capture its fine details. It would be interesting to incorporate such nuances in the selection of arms. The trembling hand model, although rendering the problem amenable to analysis, does not capture the aforementioned nuances.

In addition, the human subject participating in the experiment has limited memory and may not remember the entire history of images observed in the past. However, recall that the policies of Chapters \ref{ch:rested_arms}, \ref{ch:restless_with_known_TPMs} and \ref{ch:restless_with_unknown_TPMs} used the entire history of arm selections and observations in order to estimate the odd arm at each time instant. It may be interesting to fix attention to policies whose memory is a certain fixed time unit, say $T$, into the past, and derive lower and upper bounds for the class of such memory-constrained policies.

\item  {\color{black} \textbf{Nonasymptotic regime}
 
 Throughout the thesis, we study the problem of fixing the error probability and determining the asymptotic growth rate of the expected time to find the odd arm subject to the error probability constraint, where the asymptotics is as the error probability vanishes. It will be interesting to study a practically more relevant version of the problem -- the nonasymptotic version wherein the error probability is fixed at some $\epsilon>0$. Our analyses of the lower bounds for the asymptotic regime reveal that for any given $\epsilon>0$ and arm configurations $C$, the expected value of the stopping time of any policy whose error probability is $\leq \epsilon$ is lower bounded by $$d(\epsilon, 1-\epsilon)\cdot \alpha(C),$$ where $\alpha(C)$ is an arms configuration dependent constant. Here, $d(x, y)$ is the KL divergence between the Bernoulli distributions with parameters $x$ and $y$. This lower bound on the expected time to find the odd arm clearly carries over to the nonasymptotic regime. 

The upper bound in our analyses is asymptotic. For the nonasymptotic regime, it is suggestive to look at the next order term in the expression for the expected time to find the odd arm as explained on pp. 207. It is worth noting that the first order term is $O(\log 1/\epsilon)$.}
%\item {\color{black} \textbf{Nonasymptotic regime}: Throughout the thesis, we study the problem of fixing the error probability and determining the asymptotic growth rate of the expected time to find the odd arm subject to the error probability constraint, where the asymptotics is as the error probability vanishes. It will be interesting to study a practically more relevant version of the problem -- the nonasymptotic version wherein the error probability is fixed at some $\epsilon>0$. Our analyses of the lower bounds for the asymptotic regime reveal that for any given $\epsilon>0$ and arm configurations $C$, the expected value of the stopping time of any policy whose error probability is $\leq \epsilon$ is lower bounded by $$d(\epsilon, 1-\epsilon)\cdot \alpha(C),$$ where $\alpha(C)$ is an arms configuration dependent constant. Here, $d(x, y)$ is the KL divergence between the Bernoulli distributions with parameters $x$ and $y$. This lower bound on the expected time to find the odd arm clearly carries over to the nonasymptotic regime. 
%
%The key to achieving the lower bound in the asymptotic regime is that the arms must {\em eventually} be sampled according to the optimal or near optimal solutions given by the lower bound. However, in the nonasymptotic regime, it is important to ensure that for any given $\epsilon>0$, the fraction of arm pulls matches exactly with that of the optimal or near-optimal solutions for $\epsilon$. It will be interesting to explore this in more detail.}

%Instead of fixing the error probability, a dual problem formulation is to fix the number of rounds of sampling the arms, say $n$, and to determine the asymptotic or the nonasymptotic value of the error probability in finding the odd arm at the end of $n$ rounds ({\em fixed budget} setting), where the asymptotics is as $n\to \infty$. 
\end{itemize}













